{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde1f67d",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 2025 Januari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86197b0b",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0639b18c",
   "metadata": {},
   "source": [
    "\n",
    "This problem is about SVD and anomaly detection. In all the problems where you are asked to produce a matrix or vector, they should be **numpy arrays**. \n",
    "\n",
    "1. [4p] Load the file `data/SVD.csv` as instructed in the code cell. Compute the Singular Value Decomposition, i.e. construct the three matrices $U$, $D$, $V$ such that if $X$ is the data matrix of shape `n_samples x n_dimensions` then $X = UDV^T$. Put the resulting matrices in their variables, check that the shapes align with the instructions in the code cell. Finally, extract the first right and left singular vectors and store those as 1-d arrays in the instructed variables.\n",
    "2. [3p] The first goal is to calculate the explained variance, check the lecture notes for definition. Calculate the explained variance of using $1$, $2$,... number of singular vectors and select how many singular vectors are needed in order to explain at least $95\\%$ of the variance.\n",
    "3. [3p] With the number of components chosen in part 2, construct the best approximating matrix with the rank as the number of components. Explain what each row represents in the approximating matrix in terms of the original data, write your answer as free text in the Markdown cell below as instructed in the cells.\n",
    "4. [4p] Create a vector which corresponds to the row-wise (Euclidean) distance between the original matrix `problem1_data`and the approximating matrix `problem1_approximation` and plot the empirical distribution function of that distance. Based on the empirical distribution function choose a threshold such that 10 samples are above it and the rest below. Store the 10 samples in the instructed variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb8706",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ad4d23e",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b737d91",
   "metadata": {},
   "source": [
    "\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "2. [4p] A page loads in $\\text{Exp}(1)$ (Exponentially distributed with mean $1$) seconds if not preloaded and loads with $\\text{Exp}(10)$ (Exponentially distributed with mean $1/10$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $1$ simulate $10000$ load times from page $1$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a310986a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b079550",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae493ba0",
   "metadata": {},
   "source": [
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "3. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c386449",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9fb51c",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 2025 June"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8d70f",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf261d",
   "metadata": {},
   "source": [
    "\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "\n",
    "1. [4p] Fill in the remaining part of the function `problem1_rejection` in order to produce samples from the below distribution using rejection sampling: (Hint: $F$ is the distribution function)\n",
    "\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        \\frac{e^{x^2}-x^2-1}{e-2}, & 0 < x < 1 \\\\\n",
    "        1, & x \\geq 1\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "2. [2p] Produce 100000 samples (**use fewer if it takes too long (more than 10 sec) and you cannot find a solution**) and put the answer in `problem1_samples` from the above distribution and plot the histogram together with the true density. \n",
    "3. [2p] Use the above 100000 samples (`problem1_samples`) to approximately compute the integral\n",
    "\n",
    "$$\n",
    "    \\int_0^{1} \\sin(x) \\frac{2(e^{x^2}-1) x}{e-2} dx\n",
    "$$\n",
    "and store the result in `problem1_integral`.\n",
    "\n",
    "4. [2p] Use Hoeffdings inequality to produce a 95\\% confidence interval of the integral above and store the result as a tuple in the variable `problem1_interval`\n",
    "\n",
    "5. [4p] Fill in the remaining part of the function `problem1_rejection_2` in order to produce samples from the below distribution using rejection sampling:\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        20xe^{20-1/x}, & 0 < x < \\frac{1}{20} \\\\\n",
    "        1, & x \\geq \\frac{1}{20}\n",
    "    \\end{cases}\n",
    "$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least 9 times out of 10. You will get points based on how long your code takes to create a certain number of samples, if you choose the correct sampling distribution you can easily create 100000 samples within 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b31f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de64961",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d5857",
   "metadata": {},
   "source": [
    "\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "2. [4p] A page loads in $\\text{Exp}(3)$ (Exponentially distributed with mean $1/3$) seconds if not preloaded and loads with $\\text{Exp}(20)$ (Exponentially distributed with mean $1/20$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $8$ simulate $10000$ load times from page $1$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to the stationary distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517c1b44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "326fcff4",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89857444",
   "metadata": {},
   "source": [
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "3. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f89097",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0e73428",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 230105 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb3b0f",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2bd8d",
   "metadata": {},
   "source": [
    "A courier company operates a fleet of delivery trucks that make deliveries to different parts of the city. The trucks are equipped with GPS tracking devices that record the location of each truck at regular intervals. The locations are divided into three regions: downtown, the suburbs, and the countryside. The following table shows the probabilities of a truck transitioning between these regions at each time step:\n",
    "\n",
    "| Current region | Probability of transitioning to downtown | Probability of transitioning to the suburbs | Probability of transitioning to the countryside |\n",
    "|----------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------|\n",
    "| Downtown       | 0.3                                      | 0.4                                           | 0.3                                            |\n",
    "| Suburbs        | 0.2                                      | 0.5                                           | 0.3                                            |\n",
    "| Countryside    | 0.4                                      | 0.3                                           | 0.3                                            |\n",
    "\n",
    "1. If a truck is currently in the suburbs, what is the probability that it will be in the downtown region after two time steps? [2p]\n",
    "2. If a truck is currently in the suburbs, what is the probability that it will be in the downtown region **the first time** after two time steps? [2p]\n",
    "3. Is this Markov chain irreducible? Explain your answer. [3p]\n",
    "4. What is the stationary distribution? [3p]\n",
    "5. Advanced question: What is the expected number of steps until the first time one enters the suburbs region having started in the downtown region. Hint: to get within 1 decimal point, it is enough to compute the probabilities for hitting times below 30. Motivate your answer in detail [4p]. You could also solve this question by simulation, but this gives you a maximum of [2p].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb88b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0a1b084",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb0787",
   "metadata": {},
   "source": [
    "\n",
    "You are given the \"Abalone\" dataset found in `data/abalone.csv`, which contains physical measurements of abalone (a type of sea shells) and the age of the abalone measured in **rings** (the number of rings in the shell) [https://en.wikipedia.org/wiki/Abalone](https://en.wikipedia.org/wiki/Abalone). Your task is to train a `linear regression` model to predict the age (Rings) of an abalone based on its physical measurements.\n",
    "\n",
    "To evaluate your model, you will split the dataset into a training set and a testing set. You will use the training set to train your model, and the testing set to evaluate its performance.\n",
    "\n",
    "1. Load the data into a pandas dataframe `problem2_df`. Based on the column names, figure out what are the features and the target and fill in the answer in the correct cell below. [2p]\n",
    "2. Split the data into train and test. [2p]\n",
    "3. Train the model. [1p]\n",
    "4. On the test set, evaluate the model by computing the mean absolute error and plot the empirical distribution function of the residual with confidence bands (i.e. using the DKW inequality and 95% confidence). Hint: you can use the function `plotEDF,makeEDF` combo from `Utils.py` that we have used numerous times, which also contains the option to have confidence bands. [3p]\n",
    "5. Provide a scatter plot where the x-axis corresponds to the predicted value and the y-axis is the true value, do this over the test set. [2p]\n",
    "6. Reason about the performance, for instance, is the value of the mean absolute error good/bad and what do you think about the scatter plot in point 5? [3p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70699a03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d8ea02",
   "metadata": {},
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29d1a3",
   "metadata": {},
   "source": [
    "\n",
    "A healthcare organization is interested in understanding the relationship between the number of visits to the doctors office and certain patient characteristics. \n",
    "They have collected data on the number of visits for a sample of patients and have included the following variables\n",
    "\n",
    "* ofp : number of physician office visits\n",
    "* ofnp : number of nonphysician office visits\n",
    "* opp : number of physician outpatient visits\n",
    "* opnp : number of nonphysician outpatient visits\n",
    "* emr : number of emergency room visits\n",
    "* hosp : number of hospitalizations\n",
    "* exclhlth : the person is of excellent health (self-perceived)\n",
    "* poorhealth : the person is of poor health (self-perceived)\n",
    "* numchron : number of chronic conditions\n",
    "* adldiff : the person has a condition that limits activities of daily living ?\n",
    "* noreast : the person is from the north east region\n",
    "* midwest : the person is from the midwest region\n",
    "* west : the person is from the west region\n",
    "* age : age in years (divided by 10)\n",
    "* male : is the person male ?\n",
    "* married : is the person married ?\n",
    "* school : number of years of education\n",
    "* faminc : family income in 10000$\n",
    "* employed : is the person employed ?\n",
    "* privins : is the person covered by private health insurance?\n",
    "* medicaid : is the person covered by medicaid ?\n",
    "\n",
    "Decide which patient features are resonable to use to predict the target \"number of physician office visits\". Hint: should we really use the \"ofnp\" etc variables?\n",
    "\n",
    "Since the target variable is counts, a reasonable loss function is to consider the target variable as Poisson distributed where the parameter follows $\\lambda = \\exp(\\alpha \\cdot x + \\beta)$ where $\\alpha$ is a vector (slope) and $\\beta$ is a number (intercept). That is, the parameter is the exponential of a linear function. The reason we chose this as our parameter, is that it is always positive which is when the Poisson distribution is defined. To be specific we make the following assumption about our conditional density of $Y \\mid X$,\n",
    "$$\n",
    "    f_{Y \\mid X} (y,x) = \\frac{\\lambda^{y} e^{-\\lambda}}{y !}, \\quad \\lambda(x) = \\exp(\\alpha \\cdot x + \\beta).\n",
    "$$\n",
    "\n",
    "Recall from the lecture notes, (4.2) that in this case we should consider the log-loss (entropy) and that according to (4.2.1 Maximum Likelihood and regression) we can consider the conditional log-likelihood. Follow the steps of Example 1 and Example 2 in section (4.2) to derive the loss that needs to be minimized.\n",
    "\n",
    "Hint: when taking the log of the conditional density you will find that the term that contains the $y!$ does not depend on $\\lambda$ and as such does not depend on $\\alpha,\\beta$, it can thus be discarded. This will be essential due to numerical issues with factorials.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Load the file `data/visits_clean.csv` into the pandas dataframe `problem3_df`. Decide what should be features and target, give motivations for your choices. [3p]\n",
    "2. Create the `problem3_X` and the `problem3_y` as numpy arrays with `problem3_X` being the features and `problem3_y` being the target. Do the standard train-test split with 80% training data and 20% testing data. Store these in the variables defined in the cells. [3p]\n",
    "3. Implement $loss$ inside the class `PoissonRegression` by writing down the loss to be minimized, I have provided a formula for the $\\lambda$ that you can use. [2p]\n",
    "4. Now use the `PoissonRegression` class to train a Poisson regression model on the training data. [2p]\n",
    "5. Come up with a reasonable metric to evaluate your model on the test data, compute it and write down a justification of this. Also, interpret your result and compare it to something naive. [3p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486f6a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8c93070",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 230614"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145d94c",
   "metadata": {},
   "source": [
    "# 1.6 Exam vB, PROBLEM 1  \n",
    "Maximum Points = 14\n",
    "\n",
    "A courier company operates a fleet of delivery trucks that make deliveries to different parts of the city.  \n",
    "The trucks are equipped with GPS tracking devices that record the location of each truck at regular intervals.  \n",
    "The locations are divided into three regions: downtown, the suburbs, and the countryside.\n",
    "\n",
    "The following table shows the probabilities of a truck transitioning between these regions at each time step:\n",
    "\n",
    "| Current region | Probability to downtown | Probability to suburbs | Probability to countryside |\n",
    "|----------------|--------------------------|--------------------------|-----------------------------|\n",
    "| Downtown       | 0.3                      | 0.7                      | 0                           |\n",
    "| Suburbs        | 0.2                      | 0.5                      | 0.3                         |\n",
    "| Countryside    | 0                        | 0.5                      | 0.5                         |\n",
    "\n",
    "\n",
    "1. If a truck is currently in the downtown, what is the probability that it will be in the countryside region after 10 time steps? [2p]\n",
    "\n",
    "2. If a truck is currently in the downtown, what is the probability that it will be in the countryside region the first time after three time steps or more? [2p]\n",
    "\n",
    "3. Is this Markov chain irreducible? Explain your answer. [3p]\n",
    "\n",
    "4. What is the stationary distribution? [3p]\n",
    "\n",
    "5. Advanced question: What is the expected number of steps it takes starting from the Downtown region to first reach the Countryside region and then returning to Downtown.  \n",
    "   Hint: to get within 1 decimal point, it is enough to compute the probabilities for hitting times below 120.  \n",
    "   Motivate your answer in detail [4p].  \n",
    "   You could also solve this question by simulation, but this gives you a maximum of [2p].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d0def",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d5f692",
   "metadata": {},
   "source": [
    "# 1.9 Exam vB, PROBLEM 2  \n",
    "Maximum Points = 13\n",
    "\n",
    "You are given a “Data Science Salaries” dataset found in data/salaries.csv, which contains employment information of data scientists up to 2023 and the salary obtained. Your task is to train a linear regression model to predict the salary of a data scientist based on the employment information.\n",
    "\n",
    "To evaluate your model, you will split the dataset into a training set and a testing set. You will use the training set to train your model, and the testing set to evaluate its performance.\n",
    "\n",
    "Experience level: 0 = Entry Level, 1 = Mid Level, 2 = Senior Level, 3 = Executive Level.  \n",
    "\n",
    "Employment type: 0 = Part Time, 1 = Full Time, 2 = Contractor, 3 = Freelancer\n",
    "\n",
    "1. Load the data into a pandas dataframe problem2_df. Based on the column names, figure out what are the features and the target and fill in the answer in the correct cell below. [1p]\n",
    "\n",
    "2. Split the data into train and test. [1p]\n",
    "\n",
    "3. Train the model. [1p]\n",
    "\n",
    "4. Come up with a reasonable metric and compute it. Provide plots that show the performance of the model. Reason about the performance. [4p]\n",
    "\n",
    "5. Predict the 2023 salary of a data scientist that works full time (1) at mid employment level (1) with 0 remote ratio. Then, looking at the output of problem2_model.coef_, which are the coefficients of the linear model, would a higher remote ratio result in a higher predicted salary or vice versa? [3p]\n",
    "\n",
    "6. Advanced question: On the test set, plot the empirical distribution function of the residual with confidence bands (i.e. using the DKW inequality and 95% confidence). What does the confidence band tell us? What can the confidence band be used for? [3p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce338",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fbddfcc",
   "metadata": {},
   "source": [
    "# 1.13 Exam vB, PROBLEM 3  \n",
    "Maximum Points = 13\n",
    "\n",
    "For this problem we have the Diabetes dataset, I have encoded the categorical features\n",
    "using One-Hot encoding, namely the following \n",
    "['smoking_No Info', 'smoking_current', 'smoking_ever', 'smoking_former', \n",
    " 'smoking_never', 'smoking_not current', 'sex_Female', 'sex_Male', 'sex_Other'].\n",
    "\n",
    "Treating this as a classification problem, we will train a logistic regression model to predict whether\n",
    "the patient has diabetes or not. Then the task is to evaluate the model and using it to make some\n",
    "conclusions.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Load the file data/diabetes.csv into the pandas dataframe problem3_df.  \n",
    "   Decide what should be features and target, give motivations for your choices. [3p]\n",
    "\n",
    "2. Create the problem3_X and the problem3_y as numpy arrays with problem3_X being the\n",
    "   features and problem3_y being the target.  \n",
    "   Do the standard train-test split with 80% training data and 20% testing data.  \n",
    "   Store these in the variables defined in the cells. [2p]\n",
    "\n",
    "3. Now train a Logistic regression model on the training data using  \n",
    "   sklearn.linear_model.LogisticRegression.  \n",
    "   Hint: If you use many of the One-Hot encoded features you will probably see a warning about  \n",
    "   max iterations reached. Adjust the hyperparameter C (this is the penalization) when you create \n",
    "   your LogisticRegression. [2p]\n",
    "\n",
    "4. Evaluation: Calculate the precision and recall for class 0 and 1 with 95% confidence bounds.  \n",
    "   Explain their meaning. [3p]\n",
    "\n",
    "5. Advanced question: Come up with a way to define the one-hot encoded feature that is most\n",
    "   important for the prediction. Motivate your choice. [3p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0267b8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6f11703",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 230815"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f80f7",
   "metadata": {},
   "source": [
    "## 1.6 Exam vB, PROBLEM 1  \n",
    "Maximum Points = 14\n",
    "\n",
    "A courier company operates a fleet of delivery trucks that make deliveries to different parts of the city.  \n",
    "The trucks are equipped with GPS tracking devices that record the location of each truck at regular intervals.  \n",
    "The locations are divided into three regions: **downtown**, **the suburbs**, and **the countryside**,  \n",
    "however there is always the possibility the truck breaks down and it goes to the **workshop**.\n",
    "\n",
    "The following table shows the probabilities of a truck transitioning between these regions at each time step:\n",
    "\n",
    "Current region | Probability of transitioning to downtown | Probability of transitioning to the suburbs | Probability of transitioning to the countryside | Probability of transitioning to the Workshop\n",
    "---|---|---|---|---\n",
    "Downtown | 0.3 | 0.7 | 0 | 0\n",
    "Suburbs | 0.2 | 0.5 | 0.3 | 0\n",
    "Countryside | 0 | 0 | 0.5 | 0.5\n",
    "Workshop | 0 | 0 | 0 | 1\n",
    "\n",
    "1. If a truck is currently in the downtown, what is the probability that it will be in the countryside region after 10 time steps? **[2p]**\n",
    "\n",
    "2. If a truck is currently in the downtown, what is the probability that it will be in the countryside region the first time after three time steps or more? **[2p]**\n",
    "\n",
    "3. Is this Markov chain irreducible? Explain your answer. **[3p]**\n",
    "\n",
    "4. What is the stationary distribution? Furthermore is it reversible? (Explain your answer) **[3p]**\n",
    "\n",
    "5. **Advanced question:** What is the expected number of steps it takes starting from the Downtown region to first reach the Workshop?\n",
    "\n",
    "   *Hint:* to get within 1 decimal point, it is enough to compute the probabilities for hitting times below 50.  \n",
    "   Motivate your answer in detail. **[4p]**\n",
    "\n",
    "   You could also solve this question by simulation, but this gives you a maximum of **[2p]**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c27d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3f63179",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "## 2.2 Exam vB, PROBLEM 2  \n",
    "Maximum Points = 13  \n",
    "\n",
    "You are given a “Data Science Salaries” dataset found in `data/salaries.csv`, which contains  \n",
    "employment information of data scientists up to 2023 and the salary obtained.  \n",
    "Your task is to train a linear regression model to predict the salary of a data scientist  \n",
    "based on the employment information.\n",
    "\n",
    "To evaluate your model, you will split the dataset into a training set and a testing set.  \n",
    "You will use the training set to train your model, and the testing set to evaluate its performance.\n",
    "\n",
    "Experience level:  \n",
    "- 0 = Entry Level  \n",
    "- 1 = Mid Level  \n",
    "- 2 = Senior Level  \n",
    "- 3 = Executive Level  \n",
    "\n",
    "Employment type:  \n",
    "- 0 = Part Time  \n",
    "- 1 = Full Time  \n",
    "- 2 = Contractor  \n",
    "- 3 = Freelancer  \n",
    "\n",
    "1. Load the data into a pandas dataframe `problem2_df`.  \n",
    "   Based on the column names, figure out what are the features and the target and fill in the answer in the correct cell below. **[1p]**\n",
    "\n",
    "2. Split the data into train and test. **[1p]**\n",
    "\n",
    "3. Train the model. **[1p]**\n",
    "\n",
    "4. Come up with a reasonable metric and compute it.  \n",
    "   Provide plots that show the performance of the model.  \n",
    "   Reason about the performance. **[4p]**\n",
    "\n",
    "5. Predict the 2023 salary of a data scientist that works full time (1) at mid employment level (1) with 0 remote ratio.  \n",
    "   Then, looking at the output of `problem2_model.coef_`, which are the coefficients of the linear model,  \n",
    "   would a higher remote ratio result in a higher predicted salary or vice versa? **[3p]**\n",
    "\n",
    "6. **Advanced question:** On the test set, plot the empirical distribution function of the residual  \n",
    "   with confidence bands (i.e. using the DKW inequality and 95% confidence).  \n",
    "   What does the confidence band tell us?  \n",
    "   What can the confidence band be used for? **[3p]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df207784",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcfc66aa",
   "metadata": {},
   "source": [
    "## 2.6 Exam vB, PROBLEM 3  \n",
    "Maximum Points = 13  \n",
    "\n",
    "### 2.7 Random variable generation\n",
    "\n",
    "1. **[4p]** Using inversion sampling, construct 1000 samples from the below distribution  \n",
    "\n",
    "$$\n",
    "F[x] =\n",
    "\\begin{cases}\n",
    "0, & x \\le 0 \\\\\n",
    "e^x - 1, & 0 < x < \\ln(2) \\\\\n",
    "1, & x \\ge \\ln(2)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "2. **[2p]** Use the above 1000 samples to estimate the mean and variance.\n",
    "\n",
    "3. **[4p]** Using the Accept–Reject sampler (Algorithm 1 in TFDS notes) construct 1000 samples  \n",
    "   from the same distribution.  \n",
    "   What proposal distribution did you choose and why?  \n",
    "   What proportion of samples were accepted?\n",
    "\n",
    "4. **[3p]** Explain if it is possible to sample from the density  \n",
    "\n",
    "$$\n",
    "f(x) = C e^{-(x^2 - 2)^2}\n",
    "$$\n",
    "\n",
    "using the Accept–Reject sampler (Algorithm 1 in TFDS notes) with sampling density  \n",
    "given by the Gaussian.  \n",
    "Here \\( C \\) is a constant to make sure that \\( f \\) is a density, and it is between roughly  \n",
    "1.34 and 1.35.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75b9af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d07cefd",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 240607"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e4244",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 1\n",
    "Maximum Points = 14\n",
    "\n",
    "In this problem you will do rejection sampling from complicated distributions, you will also be using your samples to compute certain integrals, a method known as Monte Carlo integration: (Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. [4p]\n",
    "Fill in the remaining part of the function `problem1_rejection` in order to produce samples from the below density using rejection sampling:\n",
    "\n",
    "$$f[x] = C x^{0.2} (1 - x)^{1.3}$$\n",
    "\n",
    "for $0 \\le x \\le 1$, where $C$ is a value such that $f$ above is a density (i.e. integrates to one).\n",
    "\n",
    "Hint: you do not need to know the value of $C$ to perform rejection sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. [2p]\n",
    "Produce 100000 samples (use fewer if it takes too long) and put the answer in `problem1_samples` from the above distribution and plot the histogram.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. [2p]\n",
    "Define $X$ as a random variable with the density given in part 1. Denote $Y = \\sin(10X)$ and use the above 100000 samples to estimate\n",
    "\n",
    "$$E[Y]$$\n",
    "\n",
    "and store the result in `problem1_expectation`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. [2p]\n",
    "Use Hoeffdings inequality to produce a 95% confidence interval of the expectation above and store the result as a tuple in the variable `problem1_interval`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. [4p]\n",
    "Can you calculate an approximation of the value of $C$ from part 1 using random samples? Provide a plot of the histogram from part 2 together with the true density as a curve, recall that this requires the value of $C$. Explain what method you used and what answer you got.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffcd14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e23fff0",
   "metadata": {},
   "source": [
    "## Exam vB, PROBLEM 2\n",
    "**Maximum Points = 13**\n",
    "\n",
    "Let us build a proportional model ($P(Y = 1 | X) = G(\\beta_0 + \\beta \\cdot X)$ where $G$ is the logistic function)\n",
    "for the spam vs not spam data. Here we assume that the features are presence vs not presence of a\n",
    "word, let $X_1, X_2, X_3$ denote the presence (1) or absence (0) of the words (“free”, “prize”, “win”).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. [2p]\n",
    "Load the file `data/spam.csv` and create two numpy arrays, `problem2_X` which has\n",
    "shape $(n_{\\text{emails}}, 3)$ where each feature in `problem2_X` corresponds to\n",
    "$X_1, X_2, X_3$ from above, `problem2_Y` which has shape $(n_{\\text{emails}},)$ and\n",
    "consists of a 1 if the email is spam and 0 if it is not.\n",
    "\n",
    "Split this data into a train–calibration–test set where we have the split 40%, 20%, 40%.\n",
    "Put this data in the designated variables in the code cell.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. [4p]\n",
    "Follow the calculation from the lecture notes where we derive the logistic regression\n",
    "and implement the final loss function inside the class `ProportionalSpam`.\n",
    "You can use the Test cell to check that it gives the correct value for a test-point.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. [4p]\n",
    "Train the model `problem2_ps` on the training data.  \n",
    "The goal is to calibrate the probabilities output from the model.\n",
    "\n",
    "Start by creating a new variable `problem2_X_pred` (shape $(n_{\\text{samples}}, 1)$)\n",
    "which consists of the predictions of `problem2_ps` on the calibration dataset.\n",
    "\n",
    "Then train a calibration model using `sklearn.tree.DecisionTreeRegressor`,\n",
    "store this trained model in `problem2_calibrator`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. [3p]\n",
    "Use the trained model `problem2_ps` and the calibrator `problem2_calibrator` to make\n",
    "final predictions on the testing data, store the prediction in\n",
    "`problem2_final_predictions`.\n",
    "\n",
    "Compute the $0 - 1$ test-loss and store it in `problem2_01_loss` and provide a\n",
    "99% confidence interval of it.  \n",
    "Store this interval in the variable `problem2_interval`\n",
    "(this should again be a tuple as in Problem 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ef224",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e42754b4",
   "metadata": {},
   "source": [
    "## Exam vB, PROBLEM 3\n",
    "**Maximum Points = 13**\n",
    "\n",
    "Consider the following four Markov chains, answer each question for all chains:\n",
    "![Markov chain diagrams](Exams/Not%20DONE/exam240607/exam240607-markovImages.png)\n",
    "\n",
    "### 1. [2p]\n",
    "What is the transition matrix?\n",
    "\n",
    "### 2. [2p]\n",
    "Is the Markov chain irreducible?\n",
    "\n",
    "### 3. [3p]\n",
    "Is the Markov chain aperiodic?  \n",
    "What is the period for each state?\n",
    "\n",
    "Hint: Recall our definition of period:  \n",
    "Let $$T := \\{ t \\in \\mathbb{N} : P^t(x, x) > 0 \\}$$  \n",
    "and the greatest common divisor of $T$ is the period.\n",
    "\n",
    "### 4. [3p]\n",
    "Does the Markov chain have a stationary distribution, and if so, what is it?\n",
    "\n",
    "### 5. [3p]\n",
    "Is the Markov chain reversible?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a5bee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "109443dc",
   "metadata": {},
   "source": [
    "# Below is the exercises from the Exam 240828"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42693946",
   "metadata": {},
   "source": [
    "### Exam vB, PROBLEM 1  \n",
    "**Maximum Points = 14**\n",
    "\n",
    "In this problem you will do rejection sampling from complicated distributions, and you will also use your samples to compute certain integrals, a method known as *Monte Carlo integration*.  \n",
    "(Keep in mind that choosing a good sampling distribution is often key to avoid too much rejection.)\n",
    "\n",
    "1. **[4p]** Fill in the remaining part of the function `problem1_rejection` in order to produce samples from the density, using rejection sampling:\n",
    "\n",
    "$$f(x) = C (\\sin x)^{10}, \\quad 0 \\le x \\le \\pi$$\n",
    "\n",
    "\n",
    "where \\(C\\) is a value such that \\(f\\) above is a density (i.e. integrates to one).  \n",
    "*Hint:* you do not need to know the value of \\(C\\) to perform rejection sampling.\n",
    "\n",
    "2. **[2p]** Produce 10 000 samples (use fewer if it takes too long) from the above distribution, put the answer in the variable `problem1_samples`, and plot the histogram.\n",
    "\n",
    "3. **[2p]** Define \\(X\\) as a random variable with the density given in part 1. Denote\n",
    "\n",
    "\n",
    "$$Y = \\left(X - \\frac{\\pi}{2}\\right)^2$$\n",
    "\n",
    "\n",
    "and use the 10 000 samples from part 2 to estimate \\(\\mathbb{E}[Y]\\). Store the result in `problem1_expectation`.\n",
    "\n",
    "4. **[2p]** Use Hoeffding’s inequality to produce a 95% confidence interval of the expectation above and store the result as a tuple in the variable `problem1_interval`.\n",
    "\n",
    "5. **[4p]** Can you calculate an approximation of the value of \\(C\\) from part 1 using random samples?  \n",
    "Provide a plot of the histogram from part 2 together with the true density as a curve (this requires the value of \\(C\\)).  \n",
    "Explain what method you used and what answer you got.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f3508d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842ae2cc",
   "metadata": {},
   "source": [
    "## 2.1 Exam vB, PROBLEM 2  \n",
    "**Maximum Points: 13**\n",
    "\n",
    "Consider the dataset `CORIS.csv` in the `data` folder. The dataset contains cases of coronary heart disease (CHD) and variables associated with the patient’s condition:\n",
    "\n",
    "- systolic blood pressure (`sbp`)\n",
    "- yearly tobacco use in kg (`tobacco`)\n",
    "- low density lipoprotein (`ldl`)\n",
    "- adiposity\n",
    "- family history (0 or 1) (`famhist`)\n",
    "- type A personality score (`typea`)\n",
    "- obesity (body mass index)\n",
    "- alcohol use\n",
    "- age\n",
    "- diagnosis of CHD (0 or 1) (`chd`)\n",
    "\n",
    "Here:\n",
    "- **X** corresponds to the measurements,\n",
    "- **Y** is a 0–1 label where 1 represents CHD and 0 represents no CHD.\n",
    "\n",
    "The code to load the data, perform a train–test–validation split, and train a model is already prepared for you.  \n",
    "The trained model is stored in `problem2_pipe`, which is an `sklearn` `Pipeline`.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. [3p]**\n",
    "\n",
    "Use **Hoeffding’s inequality** and compute the **95% confidence intervals** for **precision and recall** (etc.) on the **test set**.  \n",
    "Store your intervals for each class in the variables:\n",
    "\n",
    "- `problem2_precision0`\n",
    "- `problem2_recall0`\n",
    "- `problem2_precision1`\n",
    "- `problem2_recall1`\n",
    "\n",
    "Each of these should be a **tuple** `(lower, upper)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [3p]**\n",
    "\n",
    "You are interested in minimizing the **average cost** of your classifier.  \n",
    "The hospital will use the model as a screening tool:\n",
    "\n",
    "- If the model predicts **CHD = 1**, the patient is sent for further investigation.\n",
    "- If the model predicts **CHD = 0**, nothing is done.\n",
    "\n",
    "You decide to use the following costs:\n",
    "\n",
    "- True positive (CHD = 1, predicted 1): cost = 0  \n",
    "- True negative (CHD = 0, predicted 0): cost = 0  \n",
    "- False positive (CHD = 0, predicted 1): cost = 10  \n",
    "- False negative (CHD = 1, predicted 0): cost = 300  *(worst case)*\n",
    "\n",
    "Complete the function `problem2_cost(model, threshold, X, Y)` to compute the **average cost per person** for a given prediction threshold, using `model.predict_proba`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. [4p]**\n",
    "\n",
    "Select the **threshold** between 0 and 1 that minimizes the **average cost** on the **test set**.  \n",
    "Check, for example, **100 evenly spaced thresholds** between 0 and 1.\n",
    "\n",
    "Store:\n",
    "\n",
    "- the optimal threshold in `problem2_optimal_threshold`\n",
    "- the cost at this threshold (on the test set) in `problem2_cost_at_optimal_threshold`\n",
    "\n",
    "---\n",
    "\n",
    "### **4. [3p]**\n",
    "\n",
    "With your newly computed threshold, compute the **cost of putting the model in production** by evaluating the cost on the **validation set**.\n",
    "\n",
    "Also compute a **99% confidence interval** for this cost using **Hoeffding’s inequality**, and store it as:\n",
    "\n",
    "- `problem2_cost_at_optimal_threshold_validation`\n",
    "- `problem2_cost_interval = (lower, upper)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0e6c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b73c1c7",
   "metadata": {},
   "source": [
    "# 2.2 Exam vB, PROBLEM 3  \n",
    "**Maximum Points: 13**\n",
    "\n",
    "![Markov Chains](Exams/Not%20DONE/exam240828/exam240828-markovImage.png)\n",
    "\n",
    "Consider the following two Markov chains:\n",
    "\n",
    "**Markov chain A**  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Markov chain B**\n",
    "\n",
    "\n",
    "Answer each question for **both chains**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. [2p]**  \n",
    "What is the **transition matrix**?  \n",
    "Your answer for each chain should be a NumPy array of shape `(n_states, n_states)`  \n",
    "where states `(A, B, …)` correspond to indices `(0, 1, …)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. [1p]**  \n",
    "Is the Markov chain **irreducible**?  \n",
    "Answer with `True` or `False` for each chain.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. [4p]**  \n",
    "Is the Markov chain **aperiodic**?  \n",
    "What is the **period of each state**?\n",
    "\n",
    "Provide:\n",
    "\n",
    "- a boolean (`True`/`False`) indicating if the chain is aperiodic\n",
    "- a NumPy array with the **period of each state**, shape `(n_states,)`\n",
    "\n",
    "*Hint:* Recall the definition of period:  \n",
    "\n",
    "$$\\text{period}(i) = \\gcd\\{\\, t \\ge 1 : P(X_t = i \\mid X_0 = i) > 0 \\,\\}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4. [2p]**  \n",
    "If the chain starts in state A at time 0, what is the probability of being in state B at time 5?  \n",
    "\n",
    "Store this in:\n",
    "\n",
    "- `problem3_A_PB5`\n",
    "- `problem3_B_PB5`\n",
    "\n",
    "---\n",
    "\n",
    "### **5. [4p]**  \n",
    "Let \\(T\\) be the **first hitting time of state D**, starting from state A:\n",
    "\n",
    "\n",
    "$$T(\\omega) = \\inf \\{\\, t \\in \\mathbb{N} : X_t(\\omega) = D \\,\\}$$\n",
    "\n",
    "\n",
    "where the infimum over an empty set is $\\infty$.\n",
    "\n",
    "Compute:\n",
    "\n",
    "- $P(T = 1)$  \n",
    "- $P(T = 2)$  \n",
    "- $P(T = 3)$  \n",
    "- $P(T = 4)$  \n",
    "- $P(T = 5)$  \n",
    "- $P(T = \\infty)$  \n",
    "\n",
    "\n",
    "for both chains A and B, and store them in the provided variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a2a3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94955949",
   "metadata": {},
   "source": [
    "# This is the AutoGraded 1 assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cb295",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1, PROBLEM 1\n",
    "Maximum Points = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1855f0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Given that you are being introduced to data science it is important to bear in mind the true costs of AI, a highly predictive family of algorithms used in data engineering sciences:\n",
    "\n",
    "Read the 16 pages of [ai-anatomy-publication.pdf](http://www.anatomyof.ai/img/ai-anatomy-publication.pdf) with the highly detailed [ai-anatomy-map.pdf](https://anatomyof.ai/img/ai-anatomy-map.pdf) of [https://anatomyof.ai/](https://anatomyof.ai/), \"Anatomy of an AI System\" By Kate Crawford and Vladan Joler (2018).  The first problem in ASSIGNMENT 1 is a trivial test of your reading comprehension.\n",
    "\n",
    "\n",
    "Answer whether each of the following statements is `True` or `False` *according to the authors* by appropriately replacing `Xxxxx` coresponding to `TruthValueOfStatement0a`, `TruthValueOfStatement0b` and `TruthValueOfStatement0c`, respectively, in the next cell to demonstrate your reading comprehension.\n",
    "\n",
    "1. `Statement0a =` *Each small moment of convenience (provided by Amazon's Echo) – be it answering a question, turning on a light, or playing a song – requires a vast planetary network, fueled by the extraction of non-renewable materials, labor, and data.*\n",
    "2. `Statement0b =` *The Echo user is simultaneously a consumer, a resource, a worker, and a product* \n",
    "3. `Statement0c =` *Many of the assumptions about human life made by machine learning systems are narrow, normative and laden with error. Yet they are inscribing and building those assumptions into a new world, and will increasingly play a role in how opportunities, wealth, and knowledge are distributed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb06de1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "189f5598",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1, PROBLEM 2\n",
    "Maximum Points = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ae552",
   "metadata": {},
   "source": [
    "\n",
    "Evaluate the following cells by replacing `X` with the right command-line option to `head` in order to find the first four lines of the csv file `data/final.csv`\n",
    "\n",
    "```\n",
    "%%sh\n",
    "man head\n",
    "\n",
    "HEAD(1)                   BSD General Commands Manual                  HEAD(1)\n",
    "\n",
    "NAME\n",
    "     head -- display first lines of a file\n",
    "\n",
    "SYNOPSIS\n",
    "     head [-n count | -c bytes] [file ...]\n",
    "\n",
    "DESCRIPTION\n",
    "     This filter displays the first count lines or bytes of each of the speci-\n",
    "     fied files, or of the standard input if no files are specified.  If count\n",
    "     is omitted it defaults to 10.\n",
    "\n",
    "     If more than a single file is specified, each file is preceded by a\n",
    "     header consisting of the string ``==> XXX <=='' where ``XXX'' is the name\n",
    "     of the file.\n",
    "\n",
    "EXIT STATUS\n",
    "     The head utility exits 0 on success, and >0 if an error occurs.\n",
    "\n",
    "SEE ALSO\n",
    "     tail(1)\n",
    "\n",
    "HISTORY\n",
    "     The head command appeared in PWB UNIX.\n",
    "\n",
    "BSD                              June 6, 1993                              BSD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a112d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e6efca9",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1, PROBLEM 3\n",
    "Maximum Points = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ab7de",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment the goal is to parse the `final.csv` file from the previous problem.\n",
    "\n",
    "1. Read the file `data/final.csv` and parse it using the `csv` package and store the result as follows\n",
    "\n",
    "the `header` variable contains a list of names all as strings\n",
    "\n",
    "the `data` variable should be a list of lists containing all the rows of the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c99305f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c45b3182",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1, PROBLEM 4\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe7bad",
   "metadata": {},
   "source": [
    "\n",
    "## Students passing exam (Sample exam problem)\n",
    "Let's say we have an exam question which consists of $10$ yes/no questions. \n",
    "From past performance of similar students, a randomly chosen student will know the correct answer to $N \\sim \\text{binom}(10,6/10)$ questions. Furthermore, we assume that the student will guess the answer with equal probability to each question they don't know the answer to, i.e. given $N$ we define $Z \\sim \\text{binom}(10-N,1/2)$ as the number of correctly guessed answers. Define $Y = N + Z$, i.e., $Y$ represents the number of total correct answers.\n",
    "\n",
    "We are interested in setting a deterministic threshold $T$, i.e., we would pass a student at threshold $T$ if $Y \\geq T$. Here $T \\in \\{0,1,2,\\ldots,10\\}$.\n",
    "\n",
    "1. [5p] For each threshold $T$, compute the probability that the student *knows* less than $5$ correct answers given that the student passed, i.e., $N < 5$. Put the answer in `problem11_probabilities` as a list.\n",
    "2. [3p] What is the smallest value of $T$ such that if $Y \\geq T$ then we are 90\\% certain that $N \\geq 5$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2a0d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a017d632",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1, PROBLEM 5\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677fbe5c",
   "metadata": {},
   "source": [
    "\n",
    "## Concentration of measure (Sample exam problem)\n",
    "\n",
    "As you recall, we said that concentration of measure was simply the phenomenon where we expect that the probability of a large deviation of some quantity becoming smaller as we observe more samples: [0.4 points per correct answer]\n",
    "\n",
    "1. Which of the following will exponentially concentrate, i.e. for some $C_1,C_2,C_3,C_4 $ \n",
    "$$\n",
    "    P(Z - \\mathbb{E}[Z] \\geq \\epsilon) \\leq C_1 e^{-C_2 n \\epsilon^2} \\vee C_3 e^{-C_4 n (\\epsilon+1)} \\enspace .\n",
    "$$\n",
    "\n",
    "    1. The empirical variance of i.i.d. random variables with finite mean?\n",
    "    2. The empirical variance of i.i.d. sub-Gaussian random variables?\n",
    "    3. The empirical variance of i.i.d. sub-Exponential random variables?\n",
    "    4. The empirical mean of i.i.d. sub-Gaussian random variables?\n",
    "    5. The empirical mean of i.i.d. sub-Exponential random variables?\n",
    "    6. The empirical mean of i.i.d. random variables with finite variance?\n",
    "    7. The empirical third moment of i.i.d. random variables with finite sixth moment?\n",
    "    8. The empirical fourth moment of i.i.d. sub-Gaussian random variables?\n",
    "    9. The empirical mean of i.i.d. deterministic random variables?\n",
    "    10. The empirical tenth moment of i.i.d. Bernoulli random variables?\n",
    "\n",
    "2. Which of the above will concentrate in the weaker sense, that for some $C_1$\n",
    "$$\n",
    "    P(Z - \\mathbb{E}[Z] \\geq \\epsilon) \\leq \\frac{C_1}{n \\epsilon^2}?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8502af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d48b949a",
   "metadata": {},
   "source": [
    "# This is the AutoGraded 2 assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dadb0d",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 2, PROBLEM 1\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94152996",
   "metadata": {},
   "source": [
    "\n",
    "A courier company operates a fleet of delivery trucks that make deliveries to different parts of the city. The trucks are equipped with GPS tracking devices that record the location of each truck at regular intervals. The locations are divided into three regions: downtown, the suburbs, and the countryside. The following table shows the probabilities of a truck transitioning between these regions at each time step:\n",
    "\n",
    "| Current region | Probability of transitioning to downtown | Probability of transitioning to the suburbs | Probability of transitioning to the countryside |\n",
    "|----------------|--------------------------------------------|-----------------------------------------------|------------------------------------------------|\n",
    "| Downtown       | 0.3                                      | 0.4                                           | 0.3                                            |\n",
    "| Suburbs        | 0.2                                      | 0.5                                           | 0.3                                            |\n",
    "| Countryside    | 0.4                                      | 0.3                                           | 0.3                                            |\n",
    "\n",
    "1. If a truck is currently in the suburbs, what is the probability that it will be in the downtown region after two time steps? [1.5p]\n",
    "2. If a truck is currently in the suburbs, what is the probability that it will be in the downtown region **the first time** after two time steps? [1.5p]\n",
    "3. Is this Markov chain irreducible? [1.5p]\n",
    "4. What is the stationary distribution? [1.5p]\n",
    "5. Advanced question: What is the expected number of steps until the first time one enters the downtown region having started in the suburbs region. Hint: to get within 1 decimal point, it is enough to compute the probabilities for hitting times below 30. [2p]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5c544",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3667575",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 2, PROBLEM 2\n",
    "Maximum Points = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67001360",
   "metadata": {},
   "source": [
    "\n",
    "Use the **Multi-dimensional Constrained Optimisation** example (in `07-Optimization.ipynb`) to numerically find the MLe for the mean and variance parameter based on `normallySimulatedDataSamples`, an array obtained by a specific simulation of $30$ IID samples from the $Normal(10,2)$ random variable.\n",
    "\n",
    "Recall that $Normal(\\mu, \\sigma^2)$ RV has the probability density function given by:\n",
    "\n",
    "$$\n",
    "f(x ;\\mu, \\sigma) = \\displaystyle\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(\\frac{-1}{2\\sigma^2}(x-\\mu)^2\\right)\n",
    "$$\n",
    "\n",
    "The two parameters, $\\mu \\in \\mathbb{R} := (-\\infty,\\infty)$ and $\\sigma \\in (0,\\infty)$, are sometimes referred to as the location and scale parameters.\n",
    "\n",
    "You know that the log likelihood function for $n$ IID samples from a Normal RV with parameters $\\mu$ and $\\sigma$ simply follows from $\\sum_{i=1}^n \\log(f(x_i; \\mu,\\sigma))$, based on the IID assumption. \n",
    "\n",
    "NOTE: When setting bounding boxes for $\\mu$ and $\\sigma$ try to start with some guesses like $[-20,20]$ and $[0.1,5.0]$ and make it larger if the solution is at the boundary. Making the left bounding-point for $\\sigma$ too close to $0.0$ will cause division by zero Warnings. Other numerical instabilities can happen in such iterative numerical solutions to the MLe. You need to be patient and learn by trial-and-error. You will see the mathematical theory in more details in a future course in scientific computing/optimisation. So don't worry too much now except learning to use it for our problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd60fab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c800efdd",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 2, PROBLEM 3\n",
    "Maximum Points = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc104f0a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Derive the maximum likelihood estimate for $n$ IID samples from a random variable with the following probability density function:\n",
    "$$\n",
    "f(x; \\lambda) = \\frac{1}{24} \\lambda^5 x^4 \\exp(-\\lambda x), \\qquad \\text{ where, } \\lambda>0, x > 0\n",
    "$$\n",
    "\n",
    "You can solve the MLe by hand (using pencil paper or using key-strokes). Present your solution as the return value of a function called `def MLeForAssignment2Problem3(x)`, where `x` is a list of $n$ input data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a6b4f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c1f4c94",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 2, PROBLEM 4\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3369a0",
   "metadata": {},
   "source": [
    "\n",
    "## Random variable generation and transformation\n",
    "\n",
    "The purpose of this problem is to show that you can implement your own sampler, this will be built in the following three steps:\n",
    "\n",
    "1. [2p] Implement a Linear Congruential Generator where you tested out a good combination (a large $M$ with $a,b$ satisfying the Hull-Dobell (Thm 6.8)) of parameters. Follow the instructions in the code block.\n",
    "2. [2p] Using a generator construct random numbers from the uniform $[0,1]$ distribution.\n",
    "3. [4p] Using a uniform $[0,1]$ random generator, generate samples from \n",
    "\n",
    "$$p_0(x) = \\frac{\\pi}{2}|\\sin(2\\pi x)|, \\quad x \\in [0,1] \\enspace .$$\n",
    "\n",
    "Using the **Accept-Reject** sampler (**Algorithm 1** in TFDS notes) with sampling density given by the uniform $[0,1]$ distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af98bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f12b7d0",
   "metadata": {},
   "source": [
    "# This is the AutoGraded 3 assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df06caaf",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 1\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548427ec",
   "metadata": {},
   "source": [
    "\n",
    "Download the updated data folder from the course github website or just download directly the file [https://github.com/datascience-intro/1MS041-2025/blob/main/notebooks/data/smhi.csv](https://github.com/datascience-intro/1MS041-2025/blob/main/notebooks/data/smhi.csv) from the github website and put it inside your data folder, i.e. you want the path `data/smhi.csv`. The data was aquired from SMHI (Swedish Meteorological and Hydrological Institute) and constitutes per hour measurements of wind in the Uppsala Aut station. The data consists of windspeed and direction. Your goal is to load the data and work with it a bit. The code you produce should load the file as it is, please do not alter the file as the autograder will only have access to the original file.\n",
    "\n",
    "The file information is in Swedish so you need to use some translation service, for instance `Google translate` or ChatGPT.\n",
    "\n",
    "1. [2p] Load the file, for instance using the `csv` package. Put the wind-direction as a numpy array and the wind-speed as another numpy array.\n",
    "2. [2p] Use the wind-direction (see [Wikipedia](https://en.wikipedia.org/wiki/Wind_direction)) which is an angle in degrees and convert it into a point on the unit circle **which is the direction the wind is blowing to** (compare to definition of radians [Wikipedia](https://en.wikipedia.org/wiki/Radian)). Store the `x_coordinate` as one array and the `y_coordinate` as another. From these coordinates, construct the wind-velocity vector.\n",
    "3. [2p] Calculate the average wind velocity and convert it back to direction and compare it to just taking average of the wind direction as given in the data-file.\n",
    "4. [2p] The wind velocity is a $2$-dimensional random variable, calculate the empirical covariance matrix which should be a numpy array of shape (2,2).\n",
    "\n",
    "For you to wonder about, is it more likely for you to have headwind or not when going to the university in the morning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d338cb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "027e93d7",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 2\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160085a7",
   "metadata": {},
   "source": [
    "\n",
    "For this problem you will need the [pandas](https://pandas.pydata.org/) package and the [sklearn](https://scikit-learn.org/stable/) package. Inside the `data` folder from the course website you will find a file called `indoor_train.csv`, this file includes a bunch of positions in (X,Y,Z) and also a location number. The idea is to assign a room number (Location) to the coordinates (X,Y,Z).\n",
    "\n",
    "1. [2p] Take the data in the file `indoor_train.csv` and load it using pandas into a dataframe `df_train`\n",
    "2. [3p] From this dataframe `df_train`, create two numpy arrays, one `Xtrain` and `Ytrain`, they should have sizes `(1154,3)` and `(1154,)` respectively. Their `dtype` should be `float64` and `int64` respectively.\n",
    "3. [3p] Train a Support Vector Classifier, `sklearn.svc.SVC`, on `Xtrain, Ytrain` with `kernel='linear'` and name the trained model `svc_train`.\n",
    "\n",
    "To mimic how [kaggle](https://www.kaggle.com/) works, the Autograder has access to a hidden test-set and will test your fitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502352bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f8806cc",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 3\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407f8b9",
   "metadata": {},
   "source": [
    "\n",
    "Let us build a proportional model ($\\mathbb{P}(Y=1 \\mid X) = G(\\beta_0+\\beta \\cdot X)$ where $G$ is the logistic function) for the spam vs not spam data. Here we assume that the features are presence vs not presence of a word, let $X_1,X_2,X_3$ denote the presence (1) or absence (0) of the words $(\"free\", \"prize\", \"win\")$.\n",
    "\n",
    "1. [2p] Load the file `data/spam.csv` and create two numpy arrays, `problem3_X` which has shape **(n_texts,3)** where each feature in `problem3_X` corresponds to $X_1,X_2,X_3$ from above, `problem3_Y` which has shape **(n_texts,)** and consists of a $1$ if the email is spam and $0$ if it is not. Split this data into a train-calibration-test sets where we have the split $40\\%$, $20\\%$, $40\\%$, put this data in the designated variables in the code cell.\n",
    "\n",
    "2. [2p] Follow the calculation from the lecture notes where we derive the logistic regression and implement the final loss function inside the class `ProportionalSpam`. You can use the `Test` cell to check that it gives the correct value for a test-point.\n",
    "\n",
    "3. [2p] Train the model `problem3_ps` on the training data. The goal is to calibrate the probabilities output from the model. Start by creating a new variable `problem3_X_pred` (shape `(n_samples,1)`) which consists of the predictions of `problem3_ps` on the calibration dataset. Then train a calibration model using `sklearn.tree.DecisionTreeRegressor`, store this trained model in `problem3_calibrator`. Recall that calibration error is the following for a fixed function $f$\n",
    "$$\n",
    "    \\sqrt{\\mathbb{E}[|\\mathbb{E}[Y \\mid f(X)] - f(X)|^2]}.\n",
    "$$\n",
    "\n",
    "4. [2p] Use the trained model `problem3_ps` and the calibrator `problem3_calibrator` to make final predictions on the testing data, store the prediction in `problem3_final_predictions`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec26c33f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "708919fc",
   "metadata": {},
   "source": [
    "# This is the AutoGraded 4 assignments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44f7bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 4, PROBLEM 1\n",
    "Maximum Points = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231a0cf",
   "metadata": {},
   "source": [
    "\n",
    "    This time the assignment only consists of one problem, but we will do a more comprehensive analysis instead.\n",
    "\n",
    "Consider the dataset `Corona_NLP_train.csv` that you can get from the course website [git](https://github.com/datascience-intro/1MS041-2024/blob/main/notebooks/data/Corona_NLP_train.csv). The data is \"Coronavirus tweets NLP - Text Classification\" that can be found on [kaggle](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification). The data has several columns, but we will only be working with `OriginalTweet`and `Sentiment`.\n",
    "\n",
    "1. [3p] Load the data and filter out those tweets that have `Sentiment`=`Neutral`. Let $X$ represent the `OriginalTweet` and let \n",
    "    $$\n",
    "        Y = \n",
    "        \\begin{cases}\n",
    "        1 & \\text{if sentiment is towards positive}\n",
    "        \\\\\n",
    "        0 & \\text{if sentiment is towards negative}.\n",
    "        \\end{cases}\n",
    "    $$\n",
    "    Put the resulting arrays into the variables $X$ and $Y$. Split the data into three parts, train/test/validation where train is 60% of the data, test is 15% and validation is 25% of the data. Do not do this randomly, this is to make sure that we all did the same splits (we are in this case assuming the data is IID as presented in the dataset). That is [train,test,validation] is the splitting layout.\n",
    "\n",
    "2. [4p] There are many ways to solve this classification problem. The first main issue to resolve is to convert the $X$ variable to something that you can feed into a machine learning model. For instance, you can first use [`CountVectorizer`](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) as the first step. The step that comes after should be a `LogisticRegression` model, but for this to work you need to put together the `CountVectorizer` and the `LogisticRegression` model into a [`Pipeline`](https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline). Fill in the variable `model` such that it accepts the raw text as input and outputs a number $0$ or $1$, make sure that `model.predict_proba` works for this. **Hint: You might need to play with the parameters of LogisticRegression to get convergence, make sure that it doesn't take too long or the autograder might kill your code**\n",
    "3. [3p] Use your trained model and calculate the precision and recall on both classes. Fill in the corresponding variables with the answer.\n",
    "4. [3p] Let us now define a cost function\n",
    "    * A positive tweet that is classified as negative will have a cost of 1\n",
    "    * A negative tweet that is classified as positive will have a cost of 5\n",
    "    * Correct classifications cost 0\n",
    "    \n",
    "    complete filling the function `cost` to compute the cost of a prediction model under a certain prediction threshold (recall our precision recall lecture and the `predict_proba` function from trained models). \n",
    "\n",
    "5. [4p] Now, we wish to select the threshold of our classifier that minimizes the cost, fill in the selected threshold value in value `optimal_threshold`.\n",
    "6. [4p] With your newly computed threshold value, compute the cost of putting this model in production by computing the cost using the validation data. Also provide a confidence interval of the cost using Hoeffdings inequality with a 99% confidence.\n",
    "7. [3p] Let $t$ be the threshold you found and $f$ the model you fitted (one of the outputs of `predict_proba`), if we define the random variable\n",
    "    $$\n",
    "        C = (1-1_{f(X)\\geq t})Y+5(1-Y)1_{f(X) \\geq t}\n",
    "    $$\n",
    "    then $C$ denotes the cost of a randomly chosen tweet. In the previous step we estimated $\\mathbb{E}[C]$ using the empirical mean. However, since the threshold is chosen to minimize cost it is likely that $C=0$ or $C=1$ than $C=5$ as such it will have a low variance. Compute the empirical variance of $C$ on the validation set. What would be the confidence interval if we used Bennett's inequality instead of Hoeffding in point 6 but with the computed empirical variance as our guess for the variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6226367a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ca4c872",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
