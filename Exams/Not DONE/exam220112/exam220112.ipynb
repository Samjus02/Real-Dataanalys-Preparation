{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9576cb",
   "metadata": {},
   "source": [
    "# Exam 12th of January 2022 for the course 1MS041  \n",
    "(Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "1. Fill in your anonymous exam code in the cell below.\n",
    "2. Complete the Problems by following instructions.\n",
    "3. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "## Exam vB, PROBLEM 1  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### Probability warmup\n",
    "\n",
    "\n",
    "Let's say we have an exam question which consists of 20 yes/no questions. From past performance of similar students, a randomly chosen student will know the correct answer to $N \\sim \\text{binom}(20, 11/20)$ questions. Furthermore, we assume that the student will guess the answer with equal probability to each question they don't know the answer to, i.e. given $N$ we define $Z \\sim \\text{binom}(20 - N, 1/2)$ as the number of correctly guessed answers. Define $Y = N + Z$, i.e., $Y$ represents the number of total correct answers.\n",
    "\n",
    "We are interested in setting a deterministic threshold $T$, i.e., we would pass a student at threshold $T$ if $Y \\ge T$. \n",
    "Here $T = \\in {0, 1, 2, ..., 20}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe24b85",
   "metadata": {},
   "source": [
    "1. [5p] For each threshold $T$, compute the probability that the student knows less than $10$ correct answers given that the student passed, i.e.,  $N < 10$. Put the answer in `problem11_probabilities` as a list.\n",
    "\n",
    "2. [3p] What is the smallest value of $T$ such that if $Y \\ge T$ then we are 90% certain that $N \\ge 10$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e059a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint the PMF of N is p_N(k) where p_N is\n",
    "from scipy.special import binom as binomial\n",
    "\n",
    "\n",
    "p = 11/20\n",
    "p_N = lambda k: binomial(20, k) * ((1 - p) ** (20 - k)) * (p ** k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a28caf",
   "metadata": {},
   "source": [
    "1. [5p] For each threshold $T$, compute the probability that the student knows less than $10$ correct answers given that the student passed, i.e.,  $N < 10$. Put the answer in `problem11_probabilities` as a list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681b0ed",
   "metadata": {},
   "source": [
    "## Students passing an exam (20 yes/no questions)\n",
    "\n",
    "We consider an exam consisting of **20 yes/no questions**.\n",
    "\n",
    "- Let $N$ be the number of questions that a randomly chosen student **knows**.\n",
    "- Let $Z$ be the number of **correctly guessed** answers among the questions the student does *not* know.\n",
    "- Let\n",
    "  $$\n",
    "  Y = N + Z\n",
    "  $$\n",
    "  be the total number of correct answers.\n",
    "\n",
    "A student **passes** the exam if\n",
    "$$\n",
    "Y \\ge T,\n",
    "$$\n",
    "where the threshold\n",
    "$$\n",
    "T \\in \\{0,1,2,\\dots,20\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Distributional assumptions\n",
    "\n",
    "### Number of known answers\n",
    "\n",
    "From past performance, we assume\n",
    "$$\n",
    "N \\sim \\mathrm{Bin}(20, 11/20).\n",
    "$$\n",
    "\n",
    "Thus the probability mass function (pmf) of $N$ is\n",
    "$$\n",
    "p_N(k)\n",
    "=\n",
    "\\mathbb{P}(N = k)\n",
    "=\n",
    "\\binom{20}{k}\n",
    "\\left(\\frac{11}{20}\\right)^k\n",
    "\\left(\\frac{9}{20}\\right)^{20-k},\n",
    "\\quad k = 0,1,\\dots,20.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Number of guessed correct answers\n",
    "\n",
    "If the student knows $N = n$ answers, then they must **guess** the remaining\n",
    "$20 - n$ questions.  \n",
    "Each guess is correct with probability $1/2$, independently of the others.\n",
    "\n",
    "Therefore, **conditionally on $N = n$**,\n",
    "$$\n",
    "Z \\mid (N = n) \\sim \\mathrm{Bin}(20-n, 1/2).\n",
    "$$\n",
    "\n",
    "The total score is\n",
    "$$\n",
    "Y = n + Z.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "\n",
    "For **each threshold $T = 0,1,\\dots,20$**, we want to compute\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T).\n",
    "$$\n",
    "\n",
    "This is the probability that a student who **passes** the exam actually\n",
    "**knows fewer than 10 answers**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Use the definition of conditional probability\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T)\n",
    "=\n",
    "\\frac{\\mathbb{P}(N < 10,\\, Y \\ge T)}{\\mathbb{P}(Y \\ge T)}.\n",
    "$$\n",
    "\n",
    "So we need to compute:\n",
    "- the **denominator** $\\mathbb{P}(Y \\ge T)$,\n",
    "- the **numerator** $\\mathbb{P}(N < 10,\\, Y \\ge T)$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Compute the denominator $\\mathbb{P}(Y \\ge T)$\n",
    "\n",
    "We apply the **law of total probability** over all possible values of $N$:\n",
    "$$\n",
    "\\mathbb{P}(Y \\ge T)\n",
    "=\n",
    "\\sum_{n=0}^{20}\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\\,\\mathbb{P}(N = n).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2a: Compute $\\mathbb{P}(Y \\ge T \\mid N = n)$\n",
    "\n",
    "Given $N = n$, we have $Y = n + Z$, where\n",
    "$Z \\sim \\mathrm{Bin}(20-n, 1/2)$.\n",
    "\n",
    "Then\n",
    "$$\n",
    "Y \\ge T\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "n + Z \\ge T\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "Z \\ge T - n.\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\n",
    "=\n",
    "\\mathbb{P}(Z \\ge T-n).\n",
    "$$\n",
    "\n",
    "Since $Z$ is binomial,\n",
    "$$\n",
    "\\mathbb{P}(Z \\ge T-n)\n",
    "=\n",
    "\\sum_{z=T-n}^{20-n}\n",
    "\\binom{20-n}{z}\\left(\\frac12\\right)^{20-n}.\n",
    "$$\n",
    "\n",
    "We must also handle the edge cases:\n",
    "\n",
    "- If $T - n \\le 0$, then $Z \\ge T-n$ is always true, so  \n",
    "  $$\n",
    "  \\mathbb{P}(Y \\ge T \\mid N = n) = 1.\n",
    "  $$\n",
    "\n",
    "- If $T - n > 20 - n$, then it is impossible, so  \n",
    "  $$\n",
    "  \\mathbb{P}(Y \\ge T \\mid N = n) = 0.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2b: Final expression for the denominator\n",
    "\n",
    "Putting this together,\n",
    "$$\n",
    "\\mathbb{P}(Y \\ge T)\n",
    "=\n",
    "\\sum_{n=0}^{20}\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\\, p_N(n).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Compute the numerator $\\mathbb{P}(N < 10,\\, Y \\ge T)$\n",
    "\n",
    "For the numerator, we restrict the sum to values where $N < 10$:\n",
    "$$\n",
    "\\mathbb{P}(N < 10,\\, Y \\ge T)\n",
    "=\n",
    "\\sum_{n=0}^{9}\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\\,\\mathbb{P}(N = n).\n",
    "$$\n",
    "\n",
    "That is, we only include students who know fewer than 10 answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Combine numerator and denominator\n",
    "\n",
    "Combining the two expressions, we obtain\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T)\n",
    "=\n",
    "\\frac{\n",
    "\\displaystyle \\sum_{n=0}^{9}\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\\, p_N(n)\n",
    "}{\n",
    "\\displaystyle \\sum_{n=0}^{20}\n",
    "\\mathbb{P}(Y \\ge T \\mid N = n)\\, p_N(n)\n",
    "}.\n",
    "$$\n",
    "\n",
    "This formula is valid for **every threshold**\n",
    "$$\n",
    "T = 0,1,\\dots,20.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Interpretation and connection to the code\n",
    "\n",
    "For each fixed threshold $T$, the algorithm:\n",
    "\n",
    "1. Loops over all possible values $n = 0,\\dots,20$.\n",
    "2. Computes the pmf value $p_N(n)$.\n",
    "3. Computes the tail probability\n",
    "   $$\n",
    "   \\mathbb{P}(Z \\ge T-n),\n",
    "   $$\n",
    "   with $Z \\sim \\mathrm{Bin}(20-n, 1/2)$.\n",
    "4. Adds $p_N(n)\\,\\mathbb{P}(Z \\ge T-n)$ to the **denominator**.\n",
    "5. Adds the same term to the **numerator** only if $n < 10$.\n",
    "6. Divides numerator by denominator to obtain\n",
    "   $$\n",
    "   \\mathbb{P}(N < 10 \\mid Y \\ge T).\n",
    "   $$\n",
    "\n",
    "Repeating this for all $T = 0,1,\\dots,20$ produces a list of **21 values**,\n",
    "stored in `problem11_probabilities`.\n",
    "\n",
    "Each entry answers the question:\n",
    "\n",
    "> *Among students who pass with threshold $T$, how likely is it that they\n",
    "> actually know fewer than 10 answers?*\n",
    "\n",
    "---\n",
    "\n",
    "This derivation matches exactly what is implemented in the code and can be\n",
    "followed step by step by hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4755d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.24928935982841177),\n",
       " np.float64(0.24928935982832875),\n",
       " np.float64(0.24928935982261038),\n",
       " np.float64(0.2492893596354927),\n",
       " np.float64(0.24928935576839326),\n",
       " np.float64(0.24928929915834958),\n",
       " np.float64(0.2492886751893024),\n",
       " np.float64(0.24928330207958446),\n",
       " np.float64(0.24924628523366013),\n",
       " np.float64(0.24903902630299046),\n",
       " np.float64(0.2480856990043143),\n",
       " np.float64(0.24460820014975926),\n",
       " np.float64(0.2349439695781523),\n",
       " np.float64(0.21475641513175908),\n",
       " np.float64(0.18267139196620988),\n",
       " np.float64(0.1427252244707204),\n",
       " np.float64(0.10227042692681906),\n",
       " np.float64(0.06762809950564576),\n",
       " np.float64(0.041664724391227426),\n",
       " np.float64(0.024151134340423375),\n",
       " np.float64(0.013287462679601604)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1: \n",
    "# replace XXX to represent P(N < 10) for T = [0,1,2,...,20], i.e. your answer\n",
    "# should be a list\n",
    "# of length 21.\n",
    "\n",
    "def binom_tail_half(trials, k):\n",
    "    \"\"\"P(Binom(trials, 0.5) >= k)\"\"\"\n",
    "    if k <= 0:\n",
    "        return 1.0\n",
    "    if k > trials:\n",
    "        return 0.0\n",
    "    return sum(binomial(trials, z) for z in range(k, trials+1)) * (0.5 ** trials)\n",
    "\n",
    "problem11_probabilities = []\n",
    "\n",
    "# T = 0..20\n",
    "for T in range(21):\n",
    "    num = 0.0  # P(N < 10, Y >= T)\n",
    "    den = 0.0  # P(Y >= T)\n",
    "\n",
    "    # sum over all possible n = 0..20\n",
    "    for n in range(21):\n",
    "        # Given N=n, Z ~ Binom(20-n, 1/2), and Y=n+Z\n",
    "        py_ge_T_given_n = binom_tail_half(20 - n, T - n)\n",
    "        term = p_N(n) * py_ge_T_given_n\n",
    "\n",
    "        den += term\n",
    "        if n < 10:\n",
    "            num += term\n",
    "\n",
    "    problem11_probabilities.append(num / den)\n",
    "\n",
    "problem11_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a6a5e",
   "metadata": {},
   "source": [
    "2. [3p] What is the smallest value of $T$ such that if $Y \\ge T$ then we are 90% certain that $N \\ge 10$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c79352",
   "metadata": {},
   "source": [
    "## Task 2: Smallest threshold $T$ for 90% certainty that $N \\ge 10$\n",
    "\n",
    "From Task 1 we computed, for each threshold $T \\in \\{0,1,\\dots,20\\}$,\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T),\n",
    "$$\n",
    "and stored these values in the list `problem11_probabilities`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Translate the requirement into a probability statement\n",
    "\n",
    "The task asks:\n",
    "\n",
    "> Find the smallest $T$ such that if $Y \\ge T$ then we are 90% certain that $N \\ge 10$.\n",
    "\n",
    "This means we want\n",
    "$$\n",
    "\\mathbb{P}(N \\ge 10 \\mid Y \\ge T) \\ge 0.90.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Use complements\n",
    "\n",
    "Because $N \\ge 10$ is the complement of $N < 10$, we have:\n",
    "$$\n",
    "\\mathbb{P}(N \\ge 10 \\mid Y \\ge T)\n",
    "=\n",
    "1 - \\mathbb{P}(N < 10 \\mid Y \\ge T).\n",
    "$$\n",
    "\n",
    "So the condition becomes:\n",
    "$$\n",
    "1 - \\mathbb{P}(N < 10 \\mid Y \\ge T) \\ge 0.90.\n",
    "$$\n",
    "\n",
    "Rearranging:\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T) \\le 0.10.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Use the list from Part 1\n",
    "\n",
    "In `problem11_probabilities`, entry `T` equals\n",
    "$$\n",
    "\\mathbb{P}(N < 10 \\mid Y \\ge T).\n",
    "$$\n",
    "\n",
    "So to solve the task we simply scan the list from $T = 0$ upward and find the\n",
    "first index where the value is $\\le 0.10$.\n",
    "\n",
    "That first index is the *smallest threshold* $T$ satisfying the 90% certainty requirement.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Apply to the given output\n",
    "\n",
    "From your printed list, the values first drop below $0.10$ at:\n",
    "\n",
    "- $T = 17$: $0.0676\\ldots \\le 0.10$\n",
    "\n",
    "but at\n",
    "\n",
    "- $T = 16$: $0.1023\\ldots > 0.10$\n",
    "\n",
    "Therefore the smallest threshold is\n",
    "$$\n",
    "T^* = 17.\n",
    "$$\n",
    "\n",
    "This means: if we require $Y \\ge 17$ to pass, then among those who pass we have\n",
    "at least 90% certainty that they actually knew at least 10 answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346846c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the correct Threshold T:  17\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Give an integer between 0 and 20 which is the answer to 2.\n",
    "# Part 2:\n",
    "# Smallest threshold T such that P(N >= 10 | Y >= T) >= 0.90.\n",
    "# We already computed problem11_probabilities[T] = P(N < 10 | Y >= T).\n",
    "\n",
    "# We could also simply count the outputs from the previous exercise and count which T is required. \n",
    "# Starting from 0 and counting up.\n",
    "\n",
    "target = 0.10  # because P(N >= 10 | Y >= T) >= 0.90  <=>  P(N < 10 | Y >= T) <= 0.10\n",
    "\n",
    "problem11_T_star = None\n",
    "\n",
    "for T, prob in enumerate(problem11_probabilities):\n",
    "    if prob <= target:\n",
    "        problem11_T_star = T\n",
    "        break\n",
    "\n",
    "print(\"This is the correct Threshold T: \",problem11_T_star)\n",
    "\n",
    "problem12_T = problem11_T_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7627c",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 2  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### Random variable generation and transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbf50b",
   "metadata": {},
   "source": [
    "The purpose of this problem is to show that you can implement your own sampler, this will be built in the following three steps:\n",
    "\n",
    "1. [2p] Implement a Linear Congruential Generator where you tested out a good combination (a large $M$ with a, b satisfying the Hull-Dobell (Thm 6.8)) of parameters. Follow the instructions in the code block.\n",
    "\n",
    "2. [2p] Using a generator construct random numbers from the uniform [0, 1] distribution.\n",
    "\n",
    "3. [4p] Using a uniform [0, 1] random generator, generate samples from  \n",
    "$p_0(x) = | \\sin(2\\pi x) |$, $x \\in [0,1]$  \n",
    "\n",
    "Using the Accept-Reject sampler (Algorithm 1 in TFDS notes) with sampling density given by the uniform [0, 1] distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60b717",
   "metadata": {},
   "source": [
    "### Linear Congruential Generator (LCG)\n",
    "\n",
    "We implement a **Linear Congruential Generator (LCG)** to produce pseudo-random\n",
    "numbers. An LCG is defined by the recurrence relation\n",
    "\n",
    "$$\n",
    "u_{n+1} = (a u_n + b) \\bmod M\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $M$ is the modulus,\n",
    "- $a$ is the multiplier,\n",
    "- $b$ is the increment,\n",
    "- $u_0$ is the seed (initial value).\n",
    "\n",
    "---\n",
    "\n",
    "### Choice of Parameters\n",
    "\n",
    "We choose:\n",
    "- $M = 2^{32}$,\n",
    "- $a = 1664525$,\n",
    "- $b = 1013904223$.\n",
    "\n",
    "These parameters are a well-known combination (from *Numerical Recipes*) and\n",
    "satisfy the **Hull–Dobell theorem** (Lemma 6.8 in the lecture notes), which ensures\n",
    "that the generator has **full period $M$**.\n",
    "\n",
    "The conditions are:\n",
    "1. $\\gcd(b, M) = 1$  \n",
    "   (true since $b$ is odd and $M$ is a power of 2),\n",
    "2. Every prime factor of $M$ divides $(a - 1)$  \n",
    "   (the only prime factor of $M$ is 2, and $a - 1$ is even),\n",
    "3. If $M$ is divisible by 4, then $(a - 1)$ is divisible by 4  \n",
    "   (satisfied here).\n",
    "\n",
    "Therefore, the LCG cycles through all values in $\\{0, 1, \\dots, M-1\\}$ before\n",
    "repeating.\n",
    "\n",
    "---\n",
    "\n",
    "### Output of the Generator\n",
    "\n",
    "The function returns a sequence of integers in the set\n",
    "$$\n",
    "\\{0, 1, \\dots, M-1\\}.\n",
    "$$\n",
    "\n",
    "These values are **not yet uniformly distributed on $[0,1]$**.  \n",
    "To obtain samples from $\\text{Uniform}(0,1)$, the output must be scaled by\n",
    "dividing by $M$, which is done in the next step of the exercise.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose in the Sampling Pipeline\n",
    "\n",
    "This LCG serves as the foundational random number generator for:\n",
    "1. Constructing uniform $[0,1]$ samples,\n",
    "2. Implementing the Accept–Reject algorithm to sample from a target density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92107bcf",
   "metadata": {},
   "source": [
    "### What is a LCG:\n",
    "\n",
    "A Linear Congruential Generator (LCG) is one of the simplest and oldest algorithms for generating pseudo-random numbers on a computer.\n",
    "It produces a deterministic sequence of numbers that behave like random numbers if the parameters are chosen well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a55acb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1013904223, 1196435762, 3519870697, 2868466484, 1649599747, 2670642822, 1476291629, 2748932008, 2180890343, 2498801434]\n"
     ]
    }
   ],
   "source": [
    "def problem2_LCG(size=None, seed=0):\n",
    "    \"\"\"\n",
    "    A linear congruential generator that generates pseudo random numbers according\n",
    "    to size.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    size : an integer denoting how many samples should be produced\n",
    "    seed : the starting point of the LCG, i.e. u0 in the notes.\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    out : a list of the pseudo random numbers\n",
    "    \"\"\"\n",
    "\n",
    "    # Lemma 6.8 (Hull–Dobell Theorem) from the lecture notes:\n",
    "    # If the parameters (M, a, b) satisfy certain conditions, the LCG\n",
    "    # has full period M, meaning it cycles through all integers\n",
    "    # {0, 1, ..., M-1} before repeating.\n",
    "    \n",
    "    # Choose a large modulus M.\n",
    "    # Using M = 2^32 is standard and efficient for computers.\n",
    "    M = 2**32\n",
    "\n",
    "    # Multiplier a (Numerical Recipes choice).\n",
    "    # Chosen so that (a - 1) is divisible by all prime factors of M (here: 2).\n",
    "    a = 1664525  \n",
    "\n",
    "    # Increment b.\n",
    "    # Must be coprime with M; since M is a power of 2, choosing b odd is sufficient.\n",
    "    b = 1013904223\n",
    "\n",
    "    # List that will store the generated pseudo-random numbers\n",
    "    list_of_random_numbers = []\n",
    "\n",
    "    # Initialize the generator state with the seed (u_0 in the notes)\n",
    "    u = seed\n",
    "\n",
    "    # Generate 'size' pseudo-random numbers\n",
    "    for _ in range(size):\n",
    "        # Core LCG recurrence relation:\n",
    "        # u_{n+1} = (a * u_n + b) mod M\n",
    "        u = (a * u + b) % M\n",
    "\n",
    "        # Append the new state to the output list\n",
    "        list_of_random_numbers.append(u)\n",
    "\n",
    "    # Return the sequence of pseudo-random integers in {0, ..., M-1}\n",
    "    return list_of_random_numbers\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(problem2_LCG(size=10, seed=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb9e22",
   "metadata": {},
   "source": [
    "### Constructing Uniform(0,1) samples from an LCG\n",
    "\n",
    "The Linear Congruential Generator (LCG) produces a sequence of integers\n",
    "$$\n",
    "u_1, u_2, \\dots \\in \\{0,1,\\dots,M-1\\},\n",
    "$$\n",
    "where $M$ is the modulus (period).\n",
    "\n",
    "To obtain pseudo-random samples in the continuous interval $[0,1)$, we apply the standard scaling transformation\n",
    "$$\n",
    "U_i = \\frac{u_i}{M}.\n",
    "$$\n",
    "\n",
    "Because each $u_i$ lies in $\\{0,\\dots,M-1\\}$, dividing by $M$ ensures\n",
    "$$\n",
    "0 \\le U_i < 1,\n",
    "$$\n",
    "so the output is in $[0,1)$.\n",
    "\n",
    "---\n",
    "\n",
    "### What the code does\n",
    "\n",
    "1. It calls the provided integer generator `generator(size, seed)` to produce raw values.\n",
    "2. It maps each raw value into the range $\\{0,\\dots,M-1\\}$ using modulo `period`\n",
    "   (this is a robustness step).\n",
    "3. It divides each integer by `period` to convert it into a floating-point number in $[0,1)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose in the sampling pipeline\n",
    "\n",
    "This Uniform(0,1) generator is the basic source of randomness needed for later Monte Carlo methods.\n",
    "In particular, it will be used inside the Accept–Reject algorithm, which requires:\n",
    "- one uniform sample to propose an $x$ value, and\n",
    "- another uniform sample to decide acceptance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa37a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23645552527159452, 0.3692706737201661, 0.5042420323006809, 0.7048832636792213, 0.05054362863302231, 0.3695183543022722, 0.7747629624791443, 0.556188570568338, 0.0164932357147336, 0.6392460397910327]\n"
     ]
    }
   ],
   "source": [
    "def problem2_uniform(generator=None, period=1, size=None, seed=0):\n",
    "    \"\"\"\n",
    "    Takes a generator (like an LCG) that outputs integers in {0,...,period-1}\n",
    "    and transforms them into pseudo-random numbers in [0,1).\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    generator : callable\n",
    "        Function generator(size, seed) returning integers (e.g. LCG outputs).\n",
    "    period : int\n",
    "        The modulus / period M of the generator.\n",
    "    size : int\n",
    "        Number of uniform samples to produce.\n",
    "    seed : int\n",
    "        Seed passed into the generator.\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    out : list[float]\n",
    "        Uniform pseudo-random numbers in [0,1).\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate `size` raw integer pseudo-random values using the provided generator.\n",
    "    # For an LCG with modulus M, these should lie in {0, 1, ..., M-1}.\n",
    "    raw = generator(size=size, seed=seed)\n",
    "\n",
    "    # Safety step: ensure every output is mapped into {0, ..., period-1},\n",
    "    # even if the generator returns something unexpected (negative or too large).\n",
    "    # For a correct LCG, this does nothing harmful; it just makes it robust.\n",
    "    vals = [int(u) % int(period) for u in raw]\n",
    "\n",
    "    # Convert integers in {0, ..., period-1} to floats in [0,1)\n",
    "    # by dividing by period (M). This makes values < 1 always.\n",
    "    return [v / float(period) for v in vals]\n",
    "\n",
    "\n",
    "print(problem2_uniform(generator=problem2_LCG, period=2**32, size=10, seed=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8011a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def problem2_accept_reject(uniformGenerator=None, size=None, seed=0):\n",
    "    \"\"\"\n",
    "    Accept–Reject sampler to draw samples from:\n",
    "        p0(x) = (pi/2) * |sin(2*pi*x)|,  x in [0,1],\n",
    "    using q(x) = Uniform(0,1) as the proposal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    uniformGenerator : callable\n",
    "        Function uniformGenerator(size, seed) returning `size` samples in [0,1).\n",
    "    size : int\n",
    "        Number of accept/reject *attempts* (each attempt uses 2 uniforms).\n",
    "    seed : int\n",
    "        Seed passed to the uniformGenerator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        Accepted x-values. The number accepted is random (<= size).\n",
    "    \"\"\"\n",
    "\n",
    "    # Basic input validation: we need a uniform generator and a size.\n",
    "    if uniformGenerator is None or size is None:\n",
    "        raise ValueError(\"Provide a uniformGenerator(size, seed) and n_iterations.\")\n",
    "\n",
    "    # Each accept/reject attempt needs two uniforms:\n",
    "    #  - one for the proposal x\n",
    "    #  - one for the acceptance test u\n",
    "    uniforms = uniformGenerator(size=2 * size, seed=seed)\n",
    "\n",
    "    accepted = []\n",
    "\n",
    "    # Iterate in steps of 2: (x,u), (x,u), ...\n",
    "    for i in range(0, 2 * size, 2):\n",
    "        x = uniforms[i]       # proposal sample from q(x)=Uniform(0,1)\n",
    "        u = uniforms[i + 1]   # uniform used to decide accept/reject\n",
    "\n",
    "        # Robustness: ensure x and u lie in [0,1).\n",
    "        # If the generator returns something outside, mod 1 maps it back.\n",
    "        if not (0.0 <= x < 1.0):\n",
    "            x = x % 1.0\n",
    "        if not (0.0 <= u < 1.0):\n",
    "            u = u % 1.0\n",
    "\n",
    "        # Target density: p0(x) = (pi/2)*|sin(2*pi*x)|\n",
    "        # Proposal density: q(x)=1 on [0,1]\n",
    "        # Choose envelope constant M = max_x p0(x) = pi/2\n",
    "        #\n",
    "        # Acceptance probability:\n",
    "        #   p0(x)/(M*q(x)) = ((pi/2)|sin(2*pi*x)|) / ((pi/2)*1) = |sin(2*pi*x)|\n",
    "        #\n",
    "        # So accept if u <= |sin(2*pi*x)|.\n",
    "        if u <= abs(math.sin(2.0 * math.pi * x)):\n",
    "            accepted.append(x)\n",
    "\n",
    "    # Return the accepted samples (count is random)\n",
    "    return accepted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387ace2",
   "metadata": {},
   "source": [
    "Local Test for Exam vB, PROBLEM 2  \n",
    "\n",
    "Evaluate cell below to make sure your answer is valid.  \n",
    "You should not modify anything in the cell below when evaluating it to do a local test of your solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2aedf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCG output: [1015568748, 1586005467, 2165703038, 3027450565, 217083232, 1587069247, 3327581586, 2388811721, 70837908, 2745540835]\n",
      "Uniform sampler [0.23645552527159452, 0.3692706737201661, 0.5042420323006809, 0.7048832636792213, 0.05054362863302231, 0.3695183543022722, 0.7747629624791443, 0.556188570568338, 0.0164932357147336, 0.6392460397910327]\n",
      "Accept-Reject sampler [0.23645552527159452, 0.7747629624791443, 0.2504511415027082, 0.23507591942325234, 0.8608870944008231, 0.6826027217321098, 0.21594200655817986, 0.07054510014131665, 0.12981509510427713, 0.5828870958648622, 0.2629479970782995, 0.9307009153999388, 0.388584918808192]\n"
     ]
    }
   ],
   "source": [
    "# If you managed to solve all three parts you can test the following code to see\n",
    "# if it runs\n",
    "# you have to change the period to match your LCG though, this is marked as XXX.\n",
    "# It is a very good idea to check these things using the histogram function in\n",
    "# sagemath\n",
    "# try with a larger number of samples, up to 10000 should run\n",
    "\n",
    "print(\"LCG output: %s\" % problem2_LCG(size=10, seed = 1))\n",
    "\n",
    "period = 2**32\n",
    "\n",
    "print(\"Uniform sampler %s\" % problem2_uniform(generator=problem2_LCG, period =\n",
    "period, size=10, seed=1))\n",
    "\n",
    "uniform_sampler = lambda size,seed: problem2_uniform(generator=problem2_LCG,\n",
    "period = period, size=size, seed=seed)\n",
    "\n",
    "print(\"Accept-Reject sampler %s\" % problem2_accept_reject(uniformGenerator =\n",
    "uniform_sampler,size=20,seed=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d156588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accept-Reject sampler [0.763774618976614, 0.651592972722763, 0.0938595867742349, 0.8357651039198697, 0.762280082457942, 0.22876222127045265, 0.9014274576114836, 0.21659939713061338, 0.23308445025757263, 0.2187810373376886, 0.28978161459048557, 0.8375779756625729, 0.6422943629324456]\n"
     ]
    }
   ],
   "source": [
    "# If however you did not manage to implement either part 1 or part 2 but still\n",
    "# want to check part 3, you can run the code below\n",
    "import random\n",
    "\n",
    "def testUniformGenerator(size,seed):\n",
    "    random.seed(seed)\n",
    " \n",
    "    return [random.random() for s in range(size)]\n",
    "\n",
    "print(\"Accept-Reject sampler %s\" %\n",
    "problem2_accept_reject(uniformGenerator=testUniformGenerator, size=20,\n",
    "seed=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5027993",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 3  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### Concentration of measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521cd74",
   "metadata": {},
   "source": [
    "As you recall, we said that concentration of measure was simply the phenomenon where we expect that the probability of a large deviation of some quantity becoming smaller as we observe more samples:  [0.4 points per correct answer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38badb3",
   "metadata": {},
   "source": [
    "1. Which of the following will exponentially concentrate, i.e. for some $C_1, C_2, C_3, C_4$\n",
    "\n",
    "$$\n",
    "P(Z - E[Z] \\ge \\epsilon) \\le C_1 e^{-C_2 n \\epsilon^2} \\wedge C_3 e^{-C_4 n(\\epsilon + 1)}\n",
    "$$ \n",
    "\n",
    "\n",
    "1. The empirical mean of i.i.d. sub-Gaussian random variables?\n",
    "\n",
    "2. The empirical mean of i.i.d. sub-Exponential random variables?\n",
    "\n",
    "3. The empirical mean of i.i.d. random variables with finite variance?\n",
    "\n",
    "4. The empirical variance of i.i.d. random variables with finite variance?\n",
    "\n",
    "5. The empirical variance of i.i.d. sub-Gaussian random variables?\n",
    "\n",
    "6. The empirical variance of i.i.d. sub-Exponential random variables?\n",
    "\n",
    "7. The empirical third moment of i.i.d. sub-Gaussian random variables?\n",
    "\n",
    "8. The empirical fourth moment of i.i.d. sub-Gaussian random variables?\n",
    "\n",
    "9. The empirical mean of i.i.d. deterministic random variables?\n",
    "\n",
    "10. The empirical tenth moment of i.i.d. Bernoulli random variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0014296",
   "metadata": {},
   "source": [
    "2. Which of the above will concentrate in the weaker sense, that for some $C_1$\n",
    "\n",
    "$$\n",
    "P(Z - \\mathbb{E}[Z] \\geq \\epsilon) \\leq \\frac{C_1}{n\\epsilon^2}?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ce0e9",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Which alternatives exponentially concentrate (and why)\n",
    "\n",
    "We want an upper tail bound of the form\n",
    "$$\n",
    "P(Z-\\mathbb{E}[Z]\\ge \\epsilon)\\le C_1 e^{-C_2 n\\epsilon^2}\\ \\wedge\\ C_3 e^{-C_4 n(\\epsilon+1)},\n",
    "$$\n",
    "i.e. a **Bernstein / sub-exponential-type** concentration: Gaussian-like for small deviations\n",
    "($e^{-c n\\epsilon^2}$) and exponential-like for larger deviations ($e^{-c n\\epsilon}$).\n",
    "\n",
    "### 1) Empirical mean of i.i.d. sub-Gaussian random variables — **YES**\n",
    "If $X_i$ are sub-Gaussian, then $\\bar X_n$ satisfies a Hoeffding/sub-Gaussian bound\n",
    "$$\n",
    "P(\\bar X_n-\\mathbb{E}\\bar X_n\\ge \\epsilon)\\le e^{-c n\\epsilon^2}.\n",
    "$$\n",
    "This is even stronger than the required “wedge” form, so it qualifies.\n",
    "\n",
    "### 2) Empirical mean of i.i.d. sub-Exponential random variables — **YES**\n",
    "If $X_i$ are sub-exponential, Bernstein’s inequality gives\n",
    "$$\n",
    "P(\\bar X_n-\\mathbb{E}\\bar X_n\\ge \\epsilon)\\le \\exp(-c n\\epsilon^2)\\ \\text{for small }\\epsilon,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\le \\exp(-c n\\epsilon)\\ \\text{for large }\\epsilon,\n",
    "$$\n",
    "which matches the required structure.\n",
    "\n",
    "### 3) Empirical mean of i.i.d. random variables with finite variance — **NO**\n",
    "Finite variance alone only guarantees polynomial concentration (e.g. Chebyshev):\n",
    "$$\n",
    "P(\\bar X_n-\\mathbb{E}\\bar X_n\\ge \\epsilon)\\le \\frac{\\mathrm{Var}(X)}{n\\epsilon^2},\n",
    "$$\n",
    "not exponential.\n",
    "\n",
    "### 4) Empirical variance of i.i.d. random variables with finite variance — **NO**\n",
    "Finite variance alone is not enough for exponential concentration of the sample variance;\n",
    "you generally need stronger tail/moment assumptions (e.g. sub-Gaussian / sub-exponential\n",
    "or at least finite fourth moment plus additional control).\n",
    "\n",
    "### 5) Empirical variance of i.i.d. sub-Gaussian random variables — **YES**\n",
    "If $X$ is sub-Gaussian, then $(X-\\mathbb{E}X)^2$ is sub-exponential.\n",
    "The sample variance is (up to small centering effects) an empirical mean of a sub-exponential\n",
    "quantity, so it has Bernstein-type exponential concentration.\n",
    "\n",
    "### 6) Empirical variance of i.i.d. sub-Exponential random variables — **NO (in general)**\n",
    "Sub-exponential tails do not generally guarantee that $(X-\\mathbb{E}X)^2$ is sub-exponential;\n",
    "it can have heavier (sub-Weibull) tails. Without extra assumptions, the sample variance does\n",
    "not necessarily satisfy the required Bernstein-type bound.\n",
    "\n",
    "### 7) Empirical third moment of i.i.d. sub-Gaussian random variables — **NO**\n",
    "A sub-Gaussian variable $X$ has $X^3$ with heavier (sub-Weibull) tails, not sub-exponential.\n",
    "So the empirical third moment typically does not have the required $e^{-c n\\epsilon^2}\\wedge e^{-c n\\epsilon}$ form.\n",
    "\n",
    "### 8) Empirical fourth moment of i.i.d. sub-Gaussian random variables — **NO**\n",
    "Similarly, $X^4$ has even heavier (sub-Weibull) tails, so you do not generally get the required\n",
    "Bernstein-type concentration for the empirical fourth moment.\n",
    "\n",
    "### 9) Empirical mean of i.i.d. deterministic random variables — **YES**\n",
    "If $X_i$ are deterministic constants, then $Z=\\bar X_n$ is deterministic and\n",
    "$$\n",
    "P(Z-\\mathbb{E}[Z]\\ge \\epsilon)=0\n",
    "$$\n",
    "for all $\\epsilon>0$. This trivially satisfies the inequality (with any positive constants).\n",
    "\n",
    "### 10) Empirical tenth moment of i.i.d. Bernoulli random variables — **YES**\n",
    "If $X\\sim \\mathrm{Bernoulli}(p)$, then $X^{10}=X$ (since $0^{10}=0$ and $1^{10}=1$),\n",
    "so the empirical tenth moment equals the empirical mean of a bounded variable.\n",
    "By Hoeffding’s inequality it concentrates exponentially:\n",
    "$$\n",
    "P(\\bar X_n-\\mathbb{E}\\bar X_n\\ge \\epsilon)\\le e^{-c n\\epsilon^2}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final answer (indices that exponentially concentrate)\n",
    "$$\n",
    "[1,2,5,9,10].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers to part 1, which of the alternatives exponentially concentrate, answer\n",
    "# as a list\n",
    "# i.e. [1,4,5] that is example 1, 4, and 5 concentrate\n",
    "\n",
    "problem3_answer_1 = [1, 2, 5, 9, 10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac681fc",
   "metadata": {},
   "source": [
    "## Part 2: Which alternatives concentrate in the weaker (Chebyshev) sense?\n",
    "\n",
    "We want a bound of the form\n",
    "$$\n",
    "P(Z-\\mathbb{E}[Z]\\ge \\epsilon)\\le \\frac{C_1}{n\\epsilon^2}.\n",
    "$$\n",
    "\n",
    "A standard way to get this is **Chebyshev's inequality**:\n",
    "if $Z$ is an empirical mean (or behaves like one) and has finite variance scaling like\n",
    "$\\mathrm{Var}(Z)=O(1/n)$, then\n",
    "$$\n",
    "P(Z-\\mathbb{E}[Z]\\ge \\epsilon)\\le \\frac{\\mathrm{Var}(Z)}{\\epsilon^2}\n",
    "= O\\!\\left(\\frac{1}{n\\epsilon^2}\\right).\n",
    "$$\n",
    "\n",
    "### Apply this to each item\n",
    "\n",
    "1. Empirical mean of i.i.d. sub-Gaussian — **YES** (even exponential ⇒ also weak)\n",
    "2. Empirical mean of i.i.d. sub-Exponential — **YES** (even exponential ⇒ also weak)\n",
    "3. Empirical mean of i.i.d. finite variance — **YES** (Chebyshev applies directly)\n",
    "4. Empirical variance of i.i.d. finite variance — **NO (not guaranteed)**  \n",
    "   Finite variance of $X$ does not guarantee a Chebyshev-style bound for the *sample variance*,\n",
    "   because to use Chebyshev on the sample variance you typically need at least a finite **fourth moment**\n",
    "   (so that the variance of the variance-estimator is finite).\n",
    "5. Empirical variance of i.i.d. sub-Gaussian — **YES** (even exponential ⇒ also weak)\n",
    "6. Empirical variance of i.i.d. sub-Exponential — **YES (weakly)**  \n",
    "   Sub-exponential implies sufficiently many finite moments (in particular a finite fourth moment),\n",
    "   so a Chebyshev-type $1/(n\\epsilon^2)$ bound is available even if Bernstein-type exponential\n",
    "   concentration may fail.\n",
    "7. Empirical third moment of i.i.d. sub-Gaussian — **YES (weakly)**  \n",
    "   Sub-Gaussian implies all polynomial moments exist, so $\\mathrm{Var}(X^3)<\\infty$ and Chebyshev\n",
    "   gives $O(1/(n\\epsilon^2))$.\n",
    "8. Empirical fourth moment of i.i.d. sub-Gaussian — **YES (weakly)**  \n",
    "   Same reasoning: $\\mathrm{Var}(X^4)<\\infty$, so Chebyshev applies.\n",
    "9. Empirical mean of i.i.d. deterministic variables — **YES (trivial)**  \n",
    "   The deviation probability is 0 for all $\\epsilon>0$.\n",
    "10. Empirical tenth moment of i.i.d. Bernoulli — **YES**  \n",
    "   For Bernoulli, $X^{10}=X$, so it's just the empirical mean of a bounded variable (hence it even\n",
    "   concentrates exponentially).\n",
    "\n",
    "### Final answer\n",
    "$$\n",
    "[1,2,3,5,6,7,8,9,10].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6308878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers to part 2, which of the alternatives concentrate in the weaker sense,\n",
    "# answer as a list\n",
    "# i.e. [1,4,5] that is example 1, 4, and 5 concentrate\n",
    "# Answers to part 2 (weaker 1/(n ε^2) concentration)\n",
    "problem3_answer_2 = [1, 2, 3, 5, 6, 7, 8, 9, 10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ce5c6",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 4  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### SMS spam filtering [8p]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0be35e",
   "metadata": {},
   "source": [
    "In the following problem we will explore SMS spam texts. The dataset is the SMS Spam Collection Dataset and we have provided for you a way to load the data. If you run the appropriate cell below, the result will be in the spam_no_spam variable. The result is a list of tuples with the first position in the tuple being the SMS text and the second being a flag: 0 = not spam and 1 = spam.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10690d94",
   "metadata": {},
   "source": [
    "1. [3p] Let $X$ be the random variable that represents each SMS text (an entry in the list), and let $Y$ represent whether text is spam or not i.e. $Y \\in \\{0,1\\}$. Thus $P(Y = 1)$ is the probability that we get a spam. The goal is to estimate:\n",
    "\n",
    "$$\n",
    "P(Y = 1 \\mid \"free\" \\text{ or } \"prize\" \\text{ is in } X)\n",
    "$$.\n",
    "\n",
    "That is, the probability that the SMS is spam given that \"free\" or \"prize\" occurs in the SMS.\n",
    "\n",
    "Hint: it is good to remove the upper/lower case of words so that we can also find \"Free\" and \"Prize\"; this can be done with text.lower() if text a string. \n",
    "\n",
    "2. [3p] Provide a \"90%\" interval of confidence around the true probability. I.e. use the Hoeffding inequality to obtain for your estimate $\\hat{P}$ of the above quantity. Find $l > 0$\n",
    "such that the following holds:\n",
    "\n",
    "$$\n",
    "P(\\hat{P} - l \\le E[\\hat{P}] \\le \\hat{P} + l) \\ge 0.9\n",
    "$$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7674c2",
   "metadata": {},
   "source": [
    "3. [2p] Repeat the two exercises above for \"free\" appearing twice in the SMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3579b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Run this cell to get the SMS text data\n",
    "# from exam_extras import load_sms\n",
    "# spam_no_spam = load_sms()\n",
    "\n",
    "data = pd.read_csv(\"data/spam.csv\", usecols=[0, 1], encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b5ada",
   "metadata": {},
   "source": [
    "-----\n",
    "## Goal of the problem\n",
    "\n",
    "We have a dataset of SMS messages. For each SMS:\n",
    "\n",
    "- $X$ = the **text** of the SMS\n",
    "- $Y \\in \\{0,1\\}$ = whether the SMS is spam  \n",
    "  - $Y=1$ means **spam**\n",
    "  - $Y=0$ means **not spam** (often called “ham”)\n",
    "\n",
    "We want to estimate the conditional probability:\n",
    "\n",
    "$P(Y = 1 \\mid \\text{\"free\" or \"prize\" is in } X)$\n",
    "\n",
    "In words:\n",
    "\n",
    "> “Among all messages that contain the word **free** or **prize**, what fraction are spam?”\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step reasoning\n",
    "\n",
    "### Step 1: Identify the *conditioning event*\n",
    "The condition is:\n",
    "\n",
    "> \"free\" **OR** \"prize\" occurs in the SMS text.\n",
    "\n",
    "Call this event $B$:\n",
    "\n",
    "$B = \\{\\text{\"free\" is in } X \\ \\text{or}\\ \\text{\"prize\" is in } X\\}$\n",
    "\n",
    "So we first need to find all messages where this is true.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Identify the *event we care about*\n",
    "The event we want the probability of is:\n",
    "\n",
    "$A = \\{Y = 1\\}$\n",
    "\n",
    "Meaning: the SMS is spam.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Use the definition of conditional probability\n",
    "By definition:\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "\n",
    "Translated into counts from the dataset:\n",
    "\n",
    "- $P(B)$ is estimated by:  \n",
    "  “How many messages contain **free** or **prize**?”\n",
    "\n",
    "- $P(A \\cap B)$ is estimated by:  \n",
    "  “How many messages are **spam** AND contain **free** or **prize**?”\n",
    "\n",
    "So the estimate is:\n",
    "\n",
    "$\\widehat{P}(Y=1 \\mid B) = \\frac{\\#(\\text{spam AND contains free/prize})}{\\#(\\text{contains free/prize})}$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Why we use `.lower()` (or `case=False`)\n",
    "Words can appear with different casing:\n",
    "\n",
    "- \"Free\"\n",
    "- \"FREE\"\n",
    "- \"free\"\n",
    "\n",
    "We want to count them all the same, so we search case-insensitively.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Important detail about your CSV columns\n",
    "From your CSV:\n",
    "\n",
    "- `v1` contains the label: `\"ham\"` or `\"spam\"`\n",
    "- `v2` contains the SMS text\n",
    "\n",
    "So we must:\n",
    "- read text from `v2`\n",
    "- create $Y$ from whether `v1 == \"spam\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169eb3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages containing 'free' or 'prize': 345\n",
      "Spam among them: 279\n",
      "Estimated probability: 0.808695652173913\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Get the SMS text (X) and the spam label (Y)\n",
    "# ----------------------------\n",
    "\n",
    "# In your CSV:\n",
    "#   v1 = \"ham\" or \"spam\"   (the label)\n",
    "#   v2 = the SMS message   (the text)\n",
    "\n",
    "texts = data[\"v2\"].astype(str)              # X: SMS text as strings\n",
    "spam  = (data[\"v1\"] == \"spam\").astype(int)  # Y: 1 if spam, 0 if ham\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build the conditioning event B:\n",
    "#    \"free\" OR \"prize\" appears in the SMS text\n",
    "# ----------------------------\n",
    "\n",
    "# .str.contains(..., case=False) searches case-insensitively\n",
    "contains_free = texts.str.contains(\"free\",  case=False, regex=False)\n",
    "contains_prize = texts.str.contains(\"prize\", case=False, regex=False)\n",
    "\n",
    "# Event B is \"free OR prize\"\n",
    "contains_free_or_prize = contains_free | contains_prize\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Compute numerator and denominator for conditional probability\n",
    "# ----------------------------\n",
    "\n",
    "# Denominator: number of messages where B is true\n",
    "denominator = contains_free_or_prize.sum()\n",
    "\n",
    "# Numerator: number of messages that are spam (Y=1) AND satisfy B\n",
    "numerator = ((spam == 1) & contains_free_or_prize).sum()\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Conditional probability estimate\n",
    "# ----------------------------\n",
    "\n",
    "# This estimates: P(Y=1 | \"free\" or \"prize\" in X)\n",
    "problem4_hatP = numerator / denominator\n",
    "\n",
    "# Optional sanity prints (useful to verify you don't divide by 0)\n",
    "print(\"Messages containing 'free' or 'prize':\", denominator)\n",
    "print(\"Spam among them:\", numerator)\n",
    "print(\"Estimated probability:\", problem4_hatP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e899cb6",
   "metadata": {},
   "source": [
    "2. [3p] Provide a \"90%\" interval of confidence around the true probability. I.e. use the Hoeffding inequality to obtain for your estimate $\\hat{P}$ of the above quantity. Find $l > 0$\n",
    "such that the following holds:\n",
    "\n",
    "$$\n",
    "P(\\hat{P} - l \\le E[\\hat{P}] \\le \\hat{P} + l) \\ge 0.9\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2b0d9",
   "metadata": {},
   "source": [
    "-----\n",
    "## Goal of part 2\n",
    "\n",
    "In part 1 you computed an empirical estimate\n",
    "\n",
    "$\\hat{P} = \\widehat{P}(Y=1 \\mid \\text{\"free\" or \"prize\" in } X)$\n",
    "\n",
    "This $\\hat{P}$ is computed from a **finite sample**, so it is noisy.\n",
    "\n",
    "Now you want a **90% confidence interval** around the *true* conditional probability.\n",
    "\n",
    "The assignment asks you to use **Hoeffding’s inequality** to find a number $l > 0$ such that:\n",
    "\n",
    "$P(\\hat{P} - l \\le E[\\hat{P}] \\le \\hat{P} + l) \\ge 0.9$\n",
    "\n",
    "This is a two-sided concentration statement:\n",
    "- with probability at least $0.9$,\n",
    "- the true mean $E[\\hat{P}]$ lies within $\\hat{P} \\pm l$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-step reasoning\n",
    "\n",
    "### Step 1: Identify what random variable Hoeffding is applied to\n",
    "When we compute\n",
    "\n",
    "$\\hat{P} = \\frac{\\#(\\text{spam AND condition})}{\\#(\\text{condition})}$\n",
    "\n",
    "we are taking an **average of Bernoulli variables** over only the messages that satisfy the condition:\n",
    "\n",
    "$B = \\{\\text{\"free\" or \"prize\" appears}\\}$\n",
    "\n",
    "For every SMS that satisfies $B$, define:\n",
    "\n",
    "$Z_i =\n",
    "\\begin{cases}\n",
    "1 & \\text{if that SMS is spam} \\\\\n",
    "0 & \\text{if that SMS is ham}\n",
    "\\end{cases}$\n",
    "\n",
    "Then:\n",
    "\n",
    "$\\hat{P} = \\frac{1}{n}\\sum_{i=1}^n Z_i$\n",
    "\n",
    "where:\n",
    "\n",
    "$n = \\#(\\text{messages that satisfy } B)$\n",
    "\n",
    "So Hoeffding should use **$n = denominator$** from part 1, not the full dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Hoeffding’s inequality (two-sided)\n",
    "For variables bounded in $[0,1]$ (Bernoulli is in $[0,1]$), Hoeffding gives:\n",
    "\n",
    "$P(|\\hat{P} - E[\\hat{P}]| \\ge l) \\le 2\\exp(-2 n l^2)$\n",
    "\n",
    "We want:\n",
    "\n",
    "$P(|\\hat{P} - E[\\hat{P}]| \\le l) \\ge 0.9$\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "$P(|\\hat{P} - E[\\hat{P}]| \\ge l) \\le 0.1$\n",
    "\n",
    "So we set:\n",
    "\n",
    "$2\\exp(-2 n l^2) \\le 0.1$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Solve for $l$\n",
    "Start from:\n",
    "\n",
    "$2\\exp(-2 n l^2) \\le 0.1$\n",
    "\n",
    "Divide by 2:\n",
    "\n",
    "$\\exp(-2 n l^2) \\le 0.05$\n",
    "\n",
    "Take $\\ln$:\n",
    "\n",
    "$-2 n l^2 \\le \\ln(0.05)$\n",
    "\n",
    "Multiply by $-1$ (reverses inequality):\n",
    "\n",
    "$2 n l^2 \\ge -\\ln(0.05)$\n",
    "\n",
    "So:\n",
    "\n",
    "$l \\ge \\sqrt{\\frac{-\\ln(0.05)}{2n}}$\n",
    "\n",
    "We pick the smallest valid value:\n",
    "\n",
    "$l = \\sqrt{\\frac{\\ln(2/0.1)}{2n}} = \\sqrt{\\frac{\\ln(20)}{2n}}$\n",
    "\n",
    "because $-\\ln(0.05) = \\ln(20)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: The 90% confidence interval\n",
    "Once you compute $l$, the interval is:\n",
    "\n",
    "$[\\hat{P} - l,\\ \\hat{P} + l]$\n",
    "\n",
    "Since probabilities must lie in $[0,1]$, you typically clip:\n",
    "\n",
    "$[\\max(0,\\hat{P}-l),\\ \\min(1,\\hat{P}+l)]$\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546be232",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Yes, Hoeffding **does have** an interval \\([a,b]\\) in general.\n",
    "\n",
    "In *this* problem, we do not write \\([a,b]\\) explicitly because  \n",
    "**for Bernoulli random variables, the bounds are automatically**\n",
    "\\[\n",
    "a = 0, \\quad b = 1.\n",
    "\\]\n",
    "\n",
    "So the interval is still there — it is just implicit.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52b7fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n (messages containing 'free' or 'prize'): 345\n",
      "Hoeffding l for 90% interval: 0.06589112972293922\n",
      "90% Hoeffding interval: (np.float64(0.7428045224509738), np.float64(0.8745867818968522))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We assume you've already computed these from part 1:\n",
    "#   contains_free_or_prize\n",
    "#   spam\n",
    "#   problem4_hatP\n",
    "#\n",
    "# If not, re-create them exactly as in part 1.\n",
    "\n",
    "# ----------------------------\n",
    "# 1) n = number of samples used in the conditional estimate\n",
    "# ----------------------------\n",
    "# This is the number of messages that satisfy the condition B:\n",
    "#   B = (\"free\" or \"prize\" occurs in the SMS)\n",
    "n = contains_free_or_prize.sum()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Hoeffding setup for a 90% two-sided interval\n",
    "# ----------------------------\n",
    "# We want:\n",
    "#   P(|hatP - E[hatP]| <= l) >= 0.9\n",
    "# Equivalent to:\n",
    "#   P(|hatP - E[hatP]| >= l) <= 0.1\n",
    "delta = 0.1  # failure probability\n",
    "\n",
    "# Hoeffding (two-sided): P(|mean - E| >= l) <= 2 * exp(-2 n l^2)\n",
    "# Solve 2 * exp(-2 n l^2) = delta  -> l = sqrt( ln(2/delta) / (2n) )\n",
    "problem4_l = np.sqrt(np.log(2 / delta) / (2 * n))\n",
    "\n",
    "print(\"n (messages containing 'free' or 'prize'):\", n)\n",
    "print(\"Hoeffding l for 90% interval:\", problem4_l)\n",
    "\n",
    "# Optional: the actual 90% confidence interval around the estimate hatP\n",
    "lower = max(0.0, problem4_hatP - problem4_l)\n",
    "upper = min(1.0, problem4_hatP + problem4_l)\n",
    "\n",
    "print(\"90% Hoeffding interval:\", (lower, upper))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78401b7",
   "metadata": {},
   "source": [
    "## Goal of part 3\n",
    "\n",
    "Now we repeat part 1 and part 2, but with a **different conditioning event**.\n",
    "\n",
    "Before, the condition was:\n",
    "\n",
    "- “free OR prize appears at least once”\n",
    "\n",
    "Now the condition is:\n",
    "\n",
    "- “free appears **at least twice** in the SMS”\n",
    "\n",
    "So we want to estimate:\n",
    "\n",
    "$P(Y = 1 \\mid \\text{\"free\" appears twice in } X)$\n",
    "\n",
    "In words:\n",
    "\n",
    "> Among all messages where the word **free** appears at least two times, what fraction are spam?\n",
    "\n",
    "And then we compute a 90% Hoeffding confidence radius $l_2$ for this estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90047b2c",
   "metadata": {},
   "source": [
    "## Step-by-step reasoning\n",
    "\n",
    "### Step 1: Define the new conditioning event\n",
    "Let $B_2$ be the event:\n",
    "\n",
    "$B_2 = \\{\\text{\"free\" appears at least twice in the SMS text}\\}$\n",
    "\n",
    "So we need to find all SMS where the count of \"free\" (case-insensitive) is $\\ge 2$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Define the estimate\n",
    "Among the messages satisfying $B_2$:\n",
    "\n",
    "- numerator = number of those messages that are spam\n",
    "- denominator = number of those messages total\n",
    "\n",
    "So:\n",
    "\n",
    "$\\hat{P}_2 = \\frac{\\#(\\text{spam AND } B_2)}{\\#(B_2)}$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Hoeffding confidence radius for 90%\n",
    "Just like before, we treat each selected message as a Bernoulli variable:\n",
    "\n",
    "$Z_i = 1$ if spam, $0$ if ham.\n",
    "\n",
    "Then $Z_i \\in [0,1]$, and Hoeffding gives:\n",
    "\n",
    "$P(|\\hat{P}_2 - E[\\hat{P}_2]| \\ge l_2) \\le 2\\exp(-2 n_2 l_2^2)$\n",
    "\n",
    "We want 90% confidence:\n",
    "\n",
    "$P(|\\hat{P}_2 - E[\\hat{P}_2]| \\le l_2) \\ge 0.9$\n",
    "\n",
    "So with $\\delta = 0.1$:\n",
    "\n",
    "$l_2 = \\sqrt{\\frac{\\ln(2/\\delta)}{2n_2}} = \\sqrt{\\frac{\\ln(20)}{2n_2}}$\n",
    "\n",
    "where:\n",
    "\n",
    "$n_2 = \\#(B_2)$ = number of SMS where \"free\" appears at least twice.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Important edge case\n",
    "It’s possible that very few messages contain \"free\" twice.\n",
    "If $n_2 = 0$, then $\\hat{P}_2$ is undefined (division by zero).\n",
    "\n",
    "So we should always check that the denominator is > 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5cc7584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages with 'free' at least twice (n2): 50\n",
      "Spam among them: 49\n",
      "Estimated P(spam | free appears twice): 0.98\n",
      "Hoeffding l2 (90%): 0.17308183826022852\n",
      "90% Hoeffding interval: (np.float64(0.8069181617397715), 1.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Get text (X) and labels (Y) as in part 1\n",
    "# ----------------------------\n",
    "texts = data[\"v2\"].astype(str)              # SMS text\n",
    "spam  = (data[\"v1\"] == \"spam\").astype(int)  # 1 if spam, 0 if ham\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Conditioning event B2: \"free\" appears at least twice\n",
    "# ----------------------------\n",
    "# Count occurrences of \"free\" in a case-insensitive way.\n",
    "# We do this by lowercasing the text first, then counting \"free\".\n",
    "free_count = texts.str.lower().str.count(\"free\")\n",
    "\n",
    "# Event: at least 2 occurrences\n",
    "contains_free_twice = free_count >= 2\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Compute the conditional probability estimate hatP2\n",
    "# ----------------------------\n",
    "denominator2 = contains_free_twice.sum()                     # n2\n",
    "numerator2   = ((spam == 1) & contains_free_twice).sum()     # spam among those\n",
    "\n",
    "problem4_hatP2 = numerator2 / denominator2\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Hoeffding radius l2 for 90% confidence (two-sided)\n",
    "# ----------------------------\n",
    "delta = 0.1\n",
    "n2 = denominator2\n",
    "\n",
    "problem4_l2 = np.sqrt(np.log(2 / delta) / (2 * n2))\n",
    "\n",
    "# ----------------------------\n",
    "# Optional sanity prints\n",
    "# ----------------------------\n",
    "print(\"Messages with 'free' at least twice (n2):\", n2)\n",
    "print(\"Spam among them:\", numerator2)\n",
    "print(\"Estimated P(spam | free appears twice):\", problem4_hatP2)\n",
    "print(\"Hoeffding l2 (90%):\", problem4_l2)\n",
    "\n",
    "# Optional: show the interval (clipped to [0,1])\n",
    "lower2 = max(0.0, problem4_hatP2 - problem4_l2)\n",
    "upper2 = min(1.0, problem4_hatP2 + problem4_l2)\n",
    "print(\"90% Hoeffding interval:\", (lower2, upper2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3368a9",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 5  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### Markovian travel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffa976",
   "metadata": {},
   "source": [
    "The dataset Travel Dataset - Datathon 2019 is a simulated dataset designed to mimic real corporate\n",
    "travel systems -- focusing on flights and hotels. The file is at data/flights.csv in the same folder as Exam.ipynb, i.e. you can use the path data/flights.csv from the notebook to access the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18e49d",
   "metadata": {},
   "source": [
    "1. [2p] In the first code-box\n",
    "\n",
    "* 1. Load the csv from file data/flights.csv\n",
    "* 2. Fill in the value of the variables as specified by their names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270c91b",
   "metadata": {},
   "source": [
    "2. [2p] In the second code-box your goal is to estimate a Markov chain transition matrix for the travels\n",
    "of these users. For example, if we enumerate the cities according to alphabetical order, the first city\n",
    "'Aracaju (SE)' would correspond to $0$. Each row of the file corresponds to one flight, i.e. it has a starting city and an ending city. We model this as a stationary Markov chain, i.e. each user's travel trajectory is a realization of the Markov chain, $(X_t)_t$. Here, $X_t$ is the current city the user is at, at step $t$, and $X_{t+1}$ is the city the user travels to at the next time step. This means that to each row in the file there is a corresponding pair $(X_t, X_{t+1})_t$. The stationarity assumption gives that for all $t$ there is a transition density $p$ such that $P(X_{t+1} = y | X_t = x) = p(x, y)$ (for all $x, y$). The transition matrix should be $n_{\\text{cities}} \\times n_{\\text{cities}}$ in size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01911328",
   "metadata": {},
   "source": [
    "3. [2p] Use the transition matrix to compute out the stationary distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299deb24",
   "metadata": {},
   "source": [
    "4. [2p] Given that we start in 'Aracaju (SE)' what is the probability that after 3 steps we will be back\n",
    "in 'Aracaju (SE)'?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(\"data/flights.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Number of cities\n",
    "# ---------------------------------------\n",
    "# Cities appear in TWO different columns:\n",
    "#   - 'from' (departure city)\n",
    "#   - 'to'   (arrival city)\n",
    "#\n",
    "# We first combine both columns into one long Series\n",
    "# so that each city name appears only once overall.\n",
    "#\n",
    "# pd.concat([...]) stacks the two columns on top of each other.\n",
    "# nunique() then counts how many UNIQUE city names there are.\n",
    "number_of_cities = pd.concat([data[\"from\"], data[\"to\"]]).nunique()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Number of users\n",
    "# ---------------------------------------\n",
    "# Each user is identified by a unique 'userCode'.\n",
    "# nunique() counts how many different user codes exist.\n",
    "number_of_userCodes = data[\"userCode\"].nunique()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Number of observations\n",
    "# ---------------------------------------\n",
    "# Each row in the dataset corresponds to one flight observation.\n",
    "# len(data) returns the number of rows in the DataFrame.\n",
    "number_of_observations = len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea682e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very useful function that you can use for part 2. You have seen this\n",
    "# before when parsing the\n",
    "# pride and prejudice book.\n",
    "\n",
    "def makeFreqDict(myDataList):\n",
    "  '''Make a frequency mapping out of a list of data.\n",
    "  Param myDataList, a list of data.\n",
    "  Return a dictionary mapping each unique data value to its frequency count.'''\n",
    "  freqDict = {} # start with an empty dictionary\n",
    "  for res in myDataList:\n",
    "    if res in freqDict: # the data value already exists as a key\n",
    "      freqDict[res] = freqDict[res] + 1 # add 1 to the count using sage integers\n",
    "    else: # the data value does not exist as a key value\n",
    "      freqDict[res] = 1 # add a new key-value pair for this new data value, frequency 1\n",
    "  return freqDict # return the dictionary created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f04e86",
   "metadata": {},
   "source": [
    "2. [2p] In the second code-box your goal is to estimate a Markov chain transition matrix for the travels\n",
    "of these users. For example, if we enumerate the cities according to alphabetical order, the first city\n",
    "'Aracaju (SE)' would correspond to $0$. Each row of the file corresponds to one flight, i.e. it has a starting city and an ending city. We model this as a stationary Markov chain, i.e. each user's travel trajectory is a realization of the Markov chain, $(X_t)_t$. Here, $X_t$ is the current city the user is at, at step $t$, and $X_{t+1}$ is the city the user travels to at the next time step. This means that to each row in the file there is a corresponding pair $(X_t, X_{t+1})_t$. The stationarity assumption gives that for all $t$ there is a transition density $p$ such that $P(X_{t+1} = y | X_t = x) = p(x, y)$ (for all $x, y$). The transition matrix should be $n_{\\text{cities}} \\times n_{\\text{cities}}$ in size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33584a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix: \n",
      " [[0.         0.12983559 0.14487965 0.23218891 0.1057651  0.13120567\n",
      "  0.07570385 0.07839029 0.10203095]\n",
      " [0.15702265 0.         0.14675591 0.25273726 0.09704669 0.12417557\n",
      "  0.06478443 0.06527178 0.09220572]\n",
      " [0.15520318 0.12999309 0.         0.23751007 0.1019627  0.12979164\n",
      "  0.0695004  0.07252216 0.10351675]\n",
      " [0.15079296 0.1357189  0.14398869 0.         0.11705079 0.13275294\n",
      "  0.10131375 0.10119162 0.11719036]\n",
      " [0.16544797 0.1255253  0.14889057 0.28193814 0.         0.12161708\n",
      "  0.03992268 0.0389141  0.07774416]\n",
      " [0.16023622 0.1253937  0.14796588 0.24963911 0.09494751 0.\n",
      "  0.06223753 0.06404199 0.09553806]\n",
      " [0.16758846 0.1185846  0.14362177 0.34534642 0.05649718 0.11281594\n",
      "  0.         0.         0.05554564]\n",
      " [0.17060337 0.1174579  0.14733396 0.33910196 0.05413938 0.11412535\n",
      "  0.         0.         0.05723807]\n",
      " [0.1607619  0.12012698 0.15225397 0.28431746 0.07830688 0.12325926\n",
      "  0.03953439 0.04143915 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Collect all city names that appear anywhere in the dataset\n",
    "# ------------------------------------------------------------\n",
    "# We include BOTH origin and destination cities,\n",
    "# because the Markov chain state space is \"current city\"\n",
    "# and users can be at cities that appear as either 'from' or 'to'.\n",
    "cities = list(data[\"from\"]) + list(data[\"to\"])\n",
    "\n",
    "# Unique cities in alphabetical order (as the assignment states)\n",
    "unique_cities = sorted(set(cities))\n",
    "n_cities = len(unique_cities)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Build the list of transitions (X_t -> X_{t+1})\n",
    "# ------------------------------------------------------------\n",
    "# Each row corresponds to one flight: (from_city, to_city)\n",
    "# This is exactly a transition pair (X_t, X_{t+1}).\n",
    "transitions = list(zip(data[\"from\"], data[\"to\"]))\n",
    "\n",
    "# Count how many times each (from,to) transition happens\n",
    "transition_counts = makeFreqDict(transitions)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Create index mappings for cities\n",
    "# ------------------------------------------------------------\n",
    "# indexToCity: 0 -> \"Aracaju (SE)\", 1 -> ...\n",
    "indexToCity = {i: city for i, city in enumerate(unique_cities)}\n",
    "\n",
    "# cityToIndex: \"Aracaju (SE)\" -> 0, ...\n",
    "cityToIndex = {city: i for i, city in enumerate(unique_cities)}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Maximum likelihood estimate (MLE) of the transition matrix\n",
    "# ------------------------------------------------------------\n",
    "# We want an n_cities x n_cities matrix P where:\n",
    "# P[i, j] = P(X_{t+1} = city_j | X_t = city_i)\n",
    "#\n",
    "# MLE:  P[i, j] = (# transitions i->j) / (total # transitions out of i)\n",
    "\n",
    "# Count how many transitions leave each city (row totals)\n",
    "# Outgoing count for a city x is sum_y count(x,y)\n",
    "outgoing_counts = {city: 0 for city in unique_cities}\n",
    "for (from_city, to_city), c in transition_counts.items():\n",
    "    outgoing_counts[from_city] += c\n",
    "\n",
    "# Initialize the transition matrix with zeros\n",
    "transition_matrix = np.zeros((n_cities, n_cities), dtype=float)\n",
    "\n",
    "# Fill it using the MLE formula\n",
    "for (from_city, to_city), c in transition_counts.items():\n",
    "    i = cityToIndex[from_city]\n",
    "    j = cityToIndex[to_city]\n",
    "\n",
    "    # Avoid division by zero just in case a city never appears as 'from'\n",
    "    if outgoing_counts[from_city] > 0:\n",
    "        transition_matrix[i, j] = c / outgoing_counts[from_city]\n",
    "    else:\n",
    "        transition_matrix[i, j] = 0.0  # safe default\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Safety: ensure no NaNs (shouldn't happen with checks, but good practice)\n",
    "# ------------------------------------------------------------\n",
    "transition_matrix = np.nan_to_num(transition_matrix, nan=0.0)\n",
    "\n",
    "# Example lookup required by the assignment:\n",
    "transition_matrix[cityToIndex['Aracaju (SE)'], cityToIndex['Rio de Janeiro (RJ)']]\n",
    "\n",
    "print(\"Transition matrix: \\n\", transition_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07f70575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix: \n",
      " [[0.         0.12983559 0.14487965 0.23218891 0.1057651  0.13120567\n",
      "  0.07570385 0.07839029 0.10203095]\n",
      " [0.15702265 0.         0.14675591 0.25273726 0.09704669 0.12417557\n",
      "  0.06478443 0.06527178 0.09220572]\n",
      " [0.15520318 0.12999309 0.         0.23751007 0.1019627  0.12979164\n",
      "  0.0695004  0.07252216 0.10351675]\n",
      " [0.15079296 0.1357189  0.14398869 0.         0.11705079 0.13275294\n",
      "  0.10131375 0.10119162 0.11719036]\n",
      " [0.16544797 0.1255253  0.14889057 0.28193814 0.         0.12161708\n",
      "  0.03992268 0.0389141  0.07774416]\n",
      " [0.16023622 0.1253937  0.14796588 0.24963911 0.09494751 0.\n",
      "  0.06223753 0.06404199 0.09553806]\n",
      " [0.16758846 0.1185846  0.14362177 0.34534642 0.05649718 0.11281594\n",
      "  0.         0.         0.05554564]\n",
      " [0.17060337 0.1174579  0.14733396 0.33910196 0.05413938 0.11412535\n",
      "  0.         0.         0.05723807]\n",
      " [0.1607619  0.12012698 0.15225397 0.28431746 0.07830688 0.12325926\n",
      "  0.03953439 0.04143915 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# INSTEAD OF USING TEACHER'S CODE ABOVE, ONE CAN USE THIS INSTEAD:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Construct the state space of the Markov chain (cities)\n",
    "# ------------------------------------------------------------\n",
    "# A state corresponds to a city. Cities can appear both as\n",
    "# departure ('from') and arrival ('to'), so we include both.\n",
    "cities = list(data[\"from\"]) + list(data[\"to\"])\n",
    "\n",
    "# Get unique cities and sort them alphabetically, as required\n",
    "# by the assignment (this defines the state ordering).\n",
    "unique_cities = sorted(set(cities))\n",
    "n_cities = len(unique_cities)\n",
    "\n",
    "# Create a mapping from city name to matrix index.\n",
    "# This allows us to translate city labels into row/column indices.\n",
    "cityToIndex = {city: i for i, city in enumerate(unique_cities)}\n",
    "\n",
    "# Inverse mapping (index -> city). Not strictly needed for computation,\n",
    "# but useful for interpretation or debugging.\n",
    "indexToCity = {i: city for city, i in cityToIndex.items()}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Initialize the transition count matrix\n",
    "# ------------------------------------------------------------\n",
    "# counts[i, j] will store how many times a transition\n",
    "# from city i to city j is observed in the data.\n",
    "counts = np.zeros((n_cities, n_cities))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Count observed transitions\n",
    "# ------------------------------------------------------------\n",
    "# Each row in the dataset corresponds to one flight, i.e.\n",
    "# one transition (X_t, X_{t+1}) in the Markov chain.\n",
    "for from_city, to_city in zip(data[\"from\"], data[\"to\"]):\n",
    "    i = cityToIndex[from_city]  # index of current state X_t\n",
    "    j = cityToIndex[to_city]    # index of next state X_{t+1}\n",
    "    counts[i, j] += 1           # increment transition count\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Maximum Likelihood Estimate (MLE) of the transition matrix\n",
    "# ------------------------------------------------------------\n",
    "# The MLE for a Markov chain is:\n",
    "#   p(i, j) = (# transitions i -> j) / (total transitions out of i)\n",
    "#\n",
    "# We obtain this by normalizing each row of the count matrix.\n",
    "transition_matrix = np.zeros_like(counts)\n",
    "\n",
    "for i in range(n_cities):\n",
    "    row_sum = counts[i].sum()   # total outgoing transitions from city i\n",
    "    if row_sum > 0:             # avoid division by zero\n",
    "        transition_matrix[i] = counts[i] / row_sum\n",
    "        \n",
    "print(\"Transition matrix: \\n\", transition_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f128b3",
   "metadata": {},
   "source": [
    "-----\n",
    "3. [2p] Use the transition matrix to compute out the stationary distribution.\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfff666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero rows: 0\n",
      "No zero rows found; nothing to fix.\n",
      "Min row sum: 0.9999999999999999\n",
      "Max row sum: 1.0000000000000002\n",
      "This is stationary distribution: \n",
      " [0.13690932 0.1132047  0.12780262 0.21081107 0.08752133 0.11210498\n",
      " 0.06184532 0.06290826 0.0868924 ]\n"
     ]
    }
   ],
   "source": [
    "# This should be a numpy array of length n_cities which sums to 1 and is all positive\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def stationary_distribution(P, tol=1e-12, verify=True):\n",
    "    \"\"\"\n",
    "    Compute the stationary distribution of a finite-state Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix. Rows should sum to 1 (row-stochastic).\n",
    "    tol : float, optional\n",
    "        Tolerance used for cleaning small numerical noise.\n",
    "    verify : bool, optional\n",
    "        If True, checks that the result is approximately stationary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.ndarray, shape (n,)\n",
    "        Stationary distribution vector (non-negative, sums to 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system:\n",
    "\n",
    "        (P^T - I) * pi = 0,  with  sum(pi) = 1\n",
    "\n",
    "    by replacing one of the equations with the normalization condition.\n",
    "    It assumes that a (unique) stationary distribution exists.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError(\"P must be a square matrix.\")\n",
    "\n",
    "    # Build A * pi = b\n",
    "    A = P.T - np.eye(n)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    # Replace last row with normalization condition: sum_i pi_i = 1\n",
    "    A[-1, :] = 1.0\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    # Clean tiny numerical noise\n",
    "    pi[np.abs(pi) < tol] = 0.0\n",
    "\n",
    "    # If there are small negative values, clamp them to 0 and renormalize\n",
    "    if np.any(pi < -tol):\n",
    "        # Serious negativity -> indicate potential problem\n",
    "        raise RuntimeError(\n",
    "            \"Computed stationary distribution has significantly negative entries. \"\n",
    "            \"Check that P is a valid transition matrix with a unique stationary distribution.\"\n",
    "        )\n",
    "\n",
    "    # Clamp small negatives and renormalize\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        raise RuntimeError(\"Failed to compute a valid stationary distribution (sum <= 0).\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify:\n",
    "        # Check stationarity: pi P ≈ pi\n",
    "        if not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "            raise RuntimeError(\"Result does not satisfy pi P ≈ pi. Check the input matrix P.\")\n",
    "\n",
    "    return pi\n",
    "\n",
    "P = transition_matrix\n",
    "\n",
    "# row_sums = P.sum(axis=1)\n",
    "# zero_rows = (row_sums == 0)\n",
    "\n",
    "# print(\"Number of zero rows:\", zero_rows.sum())\n",
    "\n",
    "# if np.any(zero_rows):\n",
    "#     P[zero_rows] = 1.0 / P.shape[0]\n",
    "#     print(\"Fixed zero rows by setting them to uniform.\")\n",
    "# else:\n",
    "#     print(\"No zero rows found; nothing to fix.\")\n",
    "\n",
    "# # Optional sanity check after the fix\n",
    "# print(\"Min row sum:\", P.sum(axis=1).min())\n",
    "# print(\"Max row sum:\", P.sum(axis=1).max())\n",
    "\n",
    "\n",
    "stationary_distribution_problem5 = stationary_distribution(P)\n",
    "\n",
    "print(\"This is stationary distribution: \\n\", stationary_distribution_problem5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13dae6",
   "metadata": {},
   "source": [
    "-----\n",
    "4. [2p] Given that we start in 'Aracaju (SE)' what is the probability that after 3 steps we will be back\n",
    "in 'Aracaju (SE)'?\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7740114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the probability,  0.13331717737273135\n"
     ]
    }
   ],
   "source": [
    "# Compute the return probability for part 3 of problem 5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Index of Aracaju (SE)\n",
    "i = cityToIndex['Aracaju (SE)']\n",
    "\n",
    "# Compute P^3\n",
    "P3 = np.linalg.matrix_power(transition_matrix, 3)\n",
    "\n",
    "# Probability of returning to Aracaju (SE) after 3 steps\n",
    "return_probability_problem5 = P3[i, i]\n",
    "\n",
    "\n",
    "print(\"This is the probability, \", return_probability_problem5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f6f010",
   "metadata": {},
   "source": [
    "# Local Test for Exam vB, PROBLEM 5\n",
    "Evaluate cell below to make sure your answer is valid. You should not modify anything in the cell below\n",
    "when evaluating it to do a local test of your solution.You may need to include and evaluate code snippets\n",
    "from lecture notebooks in cells above to make the local test work correctly sometimes (see error messages\n",
    "for clues). This is meant to help you become efficient at recalling materials covered in lectures that relate\n",
    "to this problem. Such local tests will generally not be available in the exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba0f8657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aracaju (SE)->Recife (PE)->Campo Grande (MS)->Aracaju (SE)->Brasilia (DF)->Florianopolis (SC)->Recife (PE)->Campo Grande (MS)->Recife (PE)->Florianopolis (SC)->"
     ]
    }
   ],
   "source": [
    "# Once you have created all your functions, you can make a small test here to see\n",
    "# what would be generated from your model.\n",
    "import numpy as np\n",
    "start = np.zeros(shape=(n_cities,1))\n",
    "start[cityToIndex['Aracaju (SE)'],0] = 1\n",
    "current_pos = start\n",
    "for i in range(10):\n",
    "    random_word_index = np.random.choice(range(n_cities),p=current_pos.reshape(-1))\n",
    "    current_pos = np.zeros_like(start)\n",
    "    current_pos[random_word_index] = 1\n",
    "    print(indexToCity[random_word_index],end='->')\n",
    "    current_pos = (current_pos.T@transition_matrix).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252b5aa",
   "metadata": {},
   "source": [
    "# Exam vB, PROBLEM 6  \n",
    "Maximum Points = 8  \n",
    "\n",
    "### Black box testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd451087",
   "metadata": {},
   "source": [
    "In the following problem we will continue with our SMS spam / nospam data. This time we will try to approach the problem as a pattern recognition problem. For this particular problem I have provided you with everything -- data is prepared, split into train-test sets and a black-box model has been fitted on the training data\n",
    "and predicted on the test data. Your goal is to calculate test metrics and provide guarantees for each metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b831b",
   "metadata": {},
   "source": [
    "1. [2p] Compute precision for class 1 (see notes 8.3.2 for definition),\n",
    "then provide an interval using Hoeffding's inequality for a 95% confidence.\n",
    "\n",
    "2. [2p] Compute recall for class 1 (see notes 8.3.2 for definition),\n",
    "then provide an interval using Hoeffding's inequality for a 95% interval.\n",
    "\n",
    "3. [2p] Compute accuracy (0-1 loss),\n",
    "then provide an interval using Hoeffding's inequality for a 95% interval.\n",
    "\n",
    "4. [2p] If we would have used a classifier with VC-dimension 3,\n",
    "would we have obtained a smaller interval for accuracy by using all data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba9840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Y:  [0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Utilities from sklearn:\n",
    "# - train_test_split: splits data into training and test sets\n",
    "# - CountVectorizer: converts text data into numerical feature vectors\n",
    "# - KNeighborsClassifier: k-nearest neighbors classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the SMS spam dataset from CSV\n",
    "# The file contains:\n",
    "#   - column v1: label (\"spam\" or \"ham\")\n",
    "#   - column v2: SMS text\n",
    "data = pd.read_csv(\"data/spam.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Extract input data X:\n",
    "#   SMS messages as strings\n",
    "X = data[\"v2\"].astype(str).values\n",
    "#print(\"This is X: \", X)\n",
    "\n",
    "# Extract labels Y:\n",
    "#   Convert labels to binary values:\n",
    "#   1 if the message is spam, 0 otherwise\n",
    "Y = (data[\"v1\"] == \"spam\").astype(int).values\n",
    "#print(\"This is Y: \", Y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# 70% of the data is used for training, 30% for testing\n",
    "# random_state is fixed to make the split reproducible\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# Convert SMS text into numerical feature vectors\n",
    "# Each SMS is represented by word-count features\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on training data and transform training text\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform test text using the same fitted vectorizer\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize a k-nearest neighbors classifier with k = 4\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Train the classifier using the vectorized training data\n",
    "knn.fit(X_train_vec, Y_train)\n",
    "\n",
    "# Predict labels for the test data\n",
    "predictions_problem6 = knn.predict(X_test_vec)\n",
    "\n",
    "# Store true test labels (used later for evaluation metrics)\n",
    "Y_test_problem6 = Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d276e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn: 1434\n",
      "fp: 0\n",
      "fn: 160\n",
      "tp: 78\n"
     ]
    }
   ],
   "source": [
    "# This cell will basically calculate the Precision and Recall using a Confusion Matrix\n",
    "\n",
    "# === 2. Build the confusion matrix ===\n",
    "# We specify labels=[0, 1] to ensure the order is TN, FP, FN, TP when we ravel()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, predictions_problem6, labels=[0, 1]).ravel()\n",
    "print(\"tn:\", tn)\n",
    "print(\"fp:\", fp)\n",
    "print(\"fn:\", fn)\n",
    "print(\"tp:\", tp)\n",
    "\n",
    "\n",
    "prec1 = tp / (tp+fp)\n",
    "n_prec1 = tp+fp\n",
    "\n",
    "rec1 = tp / (tp + fn)\n",
    "n_rec1 = tp+fn\n",
    "\n",
    "prec0 = tn / (tn+fn)\n",
    "n_prec0 = tn+fn\n",
    "\n",
    "rec0 = tn / (tn+fp)\n",
    "n_rec0 = tn+fp\n",
    "\n",
    "def hoeffding(n, calc):\n",
    "    alpha = 0.05\n",
    "    a,b = 0, 1\n",
    "    \n",
    "    epsilon = (b-a) * np.sqrt(np.log(alpha/2) / (-2*n))\n",
    "    \n",
    "    # lower = (calc - epsilon)\n",
    "    # upper = (calc + epsilon)\n",
    "    \n",
    "    lower = max(0, calc - epsilon)\n",
    "    upper = min(1, calc + epsilon)\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5aa86",
   "metadata": {},
   "source": [
    "-----\n",
    "1. [2p] Compute precision for class 1 (see notes 8.3.2 for definition),\n",
    "then provide an interval using Hoeffding's inequality for a 95% confidence.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "41ce28e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the precision for Class 1:  1.0\n",
      "This is the hoeffding interval of Precision class 1: \n",
      " (np.float64(0.8462252843689931), 1)\n"
     ]
    }
   ],
   "source": [
    "# Compute the precision of predictions_problem6 with respect to Y_test_problem6\n",
    "\n",
    "\n",
    "problem6_precision = prec1\n",
    "\n",
    "print(\"This is the precision for Class 1: \", problem6_precision)\n",
    "\n",
    "interval_prec1 = hoeffding(n_prec1, prec1)\n",
    "\n",
    "print(\"This is the hoeffding interval of Precision class 1: \\n\", interval_prec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d17394f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the interval length:  0.15377471563100686\n"
     ]
    }
   ],
   "source": [
    "# Compute the interval length l of precision of predictions_problem6 with respect\n",
    "# to Y_test_problem6, with the same definition of l as in problem 4\n",
    "\n",
    "# The interval length is simply the epsilon value:\n",
    "a,b = 0,1\n",
    "n_prec1 = tp+fp\n",
    "alpha = 0.05\n",
    "\n",
    "epsilon = (b-a) * np.sqrt(np.log(alpha/2) / (-2*n_prec1))\n",
    "\n",
    "problem6_precision_l = epsilon\n",
    "\n",
    "print(\"This is the interval length: \", problem6_precision_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93797b",
   "metadata": {},
   "source": [
    "-----\n",
    "2. [2p] Compute recall for class 1 (see notes 8.3.2 for definition),\n",
    "then provide an interval using Hoeffding's inequality for a 95% interval.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c3373d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Recall:  0.3277310924369748\n",
      "This is the interval length of Recall Class 1:  0.0880326459464908\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same procedure but for recall\n",
    "problem6_recall = rec1\n",
    "\n",
    "print(\"This is the Recall: \", problem6_recall)\n",
    "\n",
    "a,b = 0,1\n",
    "n_rec1 = tp+fn\n",
    "alpha = 0.05\n",
    "\n",
    "epsilon = (b-a) * np.sqrt(np.log(alpha/2) / (-2*n_rec1))\n",
    "\n",
    "\n",
    "problem6_recall_l = epsilon\n",
    "\n",
    "print(\"This is the interval length of Recall Class 1: \", problem6_recall_l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682850c",
   "metadata": {},
   "source": [
    "-----\n",
    "3. [2p] Compute accuracy (0-1 loss),\n",
    "then provide an interval using Hoeffding's inequality for a 95% interval.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "243eaaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the accuracy:  0.9043062200956937\n",
      "This is the interval: 0.033213458236811565\n"
     ]
    }
   ],
   "source": [
    "# Repeat the same procedure but for accuracy or 0-1 loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(Y_test, predictions_problem6)\n",
    "\n",
    "print(\"This is the accuracy: \", accuracy)\n",
    "\n",
    "problem6_accuracy = accuracy\n",
    "\n",
    "n = len(Y_test)\n",
    "a,b = 0,1\n",
    "alpha = 0.05\n",
    "\n",
    "epsilon = (b-a) * np.sqrt(np.log(alpha/2) / (-2*n))\n",
    "\n",
    "problem6_accuracy_l = epsilon\n",
    "\n",
    "print(f\"This is the interval: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89148c92",
   "metadata": {},
   "source": [
    "-----\n",
    "4. [2p] If we would have used a classifier with VC-dimension 3,\n",
    "would we have obtained a smaller interval for accuracy by using all data?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c610b6",
   "metadata": {},
   "source": [
    "### Exercise 4 (VC-dimension 3): Step-by-step reasoning from the lecture notes\n",
    "\n",
    "We have already computed a **95% Hoeffding interval for accuracy** using the **test set**.  \n",
    "Now the question asks:\n",
    "\n",
    "*If we used a classifier with VC-dimension 3, would we have obtained a smaller interval for accuracy by using all data?*\n",
    "\n",
    "This is about the difference between:\n",
    "1) **A-posteriori testing bounds** (with a held-out test set), and  \n",
    "2) **A-priori generalization bounds** (without a test set, using VC-dimension).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Recall what you did for accuracy with a test set\n",
    "Accuracy is a mean of Bernoulli variables:\n",
    "- Define for each test example:  \n",
    "  $Z_i = 1$ if the classifier predicts correctly, else $Z_i = 0$  \n",
    "  so $Z_i \\in [0,1]$.\n",
    "\n",
    "Since the **test set is independent of the trained classifier**, the notes say we can apply Hoeffding to the test error/accuracy:\n",
    "- This is exactly the “guarantees with a held-out testing set” idea.\n",
    "\n",
    "This produces an interval width (radius) roughly like:\n",
    "$$\n",
    "\\epsilon_{\\text{test}} \\approx \\sqrt{\\frac{\\ln(2/\\alpha)}{2m}}\n",
    "$$\n",
    "where $m$ is the **test set size**.\n",
    "\n",
    "So: **bigger test set ⇒ smaller interval**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Why you cannot just “use all data” with Hoeffding testing\n",
    "If you “use all data” for training, you would be evaluating accuracy on the *training data*.\n",
    "\n",
    "But then the classifier is **not independent** of the data you evaluate on (it was chosen to fit it), and the notes warn that training error/empirical risk can be **downward biased** when the classifier is selected by empirical risk minimization. \n",
    "\n",
    "So:  \n",
    "- **Hoeffding testing bound (a-posteriori)** needs an **independent test set**, otherwise it does not apply in the same clean way.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: What VC-dimension changes (the point of the question)\n",
    "If we do **not** want/need a test set, we instead use **uniform convergence / complexity control**.\n",
    "\n",
    "The notes explicitly say that when the hypothesis class is not finite, we need tools like **VC-dimension and growth functions** to bound generalization.\n",
    "\n",
    "Key ingredients in the notes:\n",
    "- VC-dimension definition/examples and the fact that it controls how many labelings are possible. \n",
    "- Sauer–Shelah lemma gives a polynomial bound on the growth function in terms of VC-dimension. \n",
    "- This is used to get generalization results without a test set (complexity-based bounds). \n",
    "- The notes also emphasize that bounded VC-dimension is what makes ERM behave well asymptotically. \n",
    "\n",
    "So with **VC-dimension = 3**, the model class is *very simple*.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Compare “interval size” conceptually\n",
    "You are comparing two different kinds of uncertainty bounds:\n",
    "\n",
    "### A) Test-set Hoeffding interval (what you computed)\n",
    "- Uses only the test set size $m$\n",
    "- Radius scales like $1/\\sqrt{m}$\n",
    "\n",
    "### B) VC-based generalization interval (if you use all data)\n",
    "- Uses the total sample size $n$ (all data)\n",
    "- But pays a complexity term depending on VC-dimension $d$ (here $d=3$)\n",
    "- Radius typically behaves like something on the order of:\n",
    "$$\n",
    "\\epsilon_{\\text{VC}} \\sim \\sqrt{\\frac{d\\log(n) + \\log(1/\\alpha)}{n}}\n",
    "$$\n",
    "The *exact* constants depend on the specific theorem/corollary derived using Sauer–Shelah and Hoeffding/union bounds, but the important comparison is:\n",
    "- **larger n helps**\n",
    "- **smaller d helps**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Answer the question\n",
    "If VC-dimension is **as small as 3**, then the complexity penalty is small, and using **all data** gives a larger sample size $n$ than the test set size $m$ (since typically $m < n$).\n",
    "\n",
    "So **yes, in principle** you *could* get a **smaller accuracy interval** by:\n",
    "- not holding out a test set, and\n",
    "- instead using a **VC-based generalization bound** with $d=3$ on all $n$ samples.\n",
    "\n",
    "However, one should note:\n",
    "- VC bounds can be conservative (large constants), so “smaller” is not always guaranteed numerically, but the intended theoretical answer is:  \n",
    "  **low VC-dimension + more data ⇒ potentially tighter bound than using only the test set.**\n",
    "\n",
    "---\n",
    "\n",
    "### Final one-sentence conclusion (what to write as the answer)\n",
    "Because a held-out test bound only uses the test set size $m$, while a VC-based bound can use all $n$ samples and has a small complexity term when VC-dimension is 3, using all data with VC-dimension 3 could yield a smaller accuracy interval than the Hoeffding test-set interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3ff62",
   "metadata": {},
   "source": [
    "### VC-bound task: Step-by-step derivation of `problem6_VC_l` (VC-dimension = 3)\n",
    "\n",
    "We want a confidence interval (a bound) for **accuracy** (equivalently the 0–1 loss) when we **do not use a test set**, but instead use **all data** and a **VC-dimension bound**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1 — Identify the relevant theorem in the lecture notes\n",
    "\n",
    "The lecture notes provide a VC generalization bound in **Theorem 8.29**:\n",
    "\n",
    "$P\\left(\\sup_{A \\in \\mathcal{A}} |\\nu_n(A) - \\nu(A)| > \\epsilon \\right) \\le 8\\, s(\\mathcal{A}, n)\\, e^{-n\\epsilon^2/32}$  :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "Where:\n",
    "- $\\nu(A)$ is the **true probability** of event $A$\n",
    "- $\\nu_n(A)$ is the **empirical probability** (fraction of samples where $A$ happens)\n",
    "- $s(\\mathcal{A}, n)$ is the **growth function / shattering coefficient**\n",
    "\n",
    "This theorem is a “uniform convergence” result: it controls the maximum deviation between empirical and true probabilities over a whole class of sets.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2 — Relate accuracy / 0–1 loss to the theorem\n",
    "\n",
    "In classification with 0–1 loss, the event “a mistake happens” can be written as a set in the $(X,Y)$-space:\n",
    "\n",
    "$A_\\phi = \\{(x,y) : \\phi(x) \\ne y\\}$\n",
    "\n",
    "Then:\n",
    "- $\\nu(A_\\phi) = P(\\phi(X)\\ne Y)$ = true error (risk)\n",
    "- $\\nu_n(A_\\phi)$ = empirical error (fraction of mistakes in the sample)\n",
    "\n",
    "So bounding $|\\nu_n(A_\\phi) - \\nu(A_\\phi)|$ gives a bound on how close empirical accuracy/error is to the true accuracy/error. :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3 — Replace the unknown growth function using VC-dimension\n",
    "\n",
    "The theorem contains $s(\\mathcal{A}, n)$, which we need to upper bound.\n",
    "\n",
    "The notes give **Sauer–Shelah** (Lemma 8.36):\n",
    "\n",
    "$s(H, N) \\le \\sum_{i=0}^{V_H-1} \\binom{N}{i}$  :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "Here $V_H$ is the VC-dimension.  \n",
    "In the task, VC-dimension is **3**, so:\n",
    "\n",
    "$s(\\mathcal{A}, n) \\le \\sum_{i=0}^{2} \\binom{n}{i} = \\binom{n}{0}+\\binom{n}{1}+\\binom{n}{2}$\n",
    "\n",
    "This is why the code computes:\n",
    "\n",
    "`s_An = sum(comb(n_all, i) for i in range(3))`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4 — Solve the bound for $\\epsilon$ at 95% confidence\n",
    "\n",
    "We want a **95%** interval, so $\\alpha = 0.05$.\n",
    "\n",
    "From Theorem 8.29:\n",
    "\n",
    "$8\\, s(\\mathcal{A}, n)\\, e^{-n\\epsilon^2/32} \\le \\alpha$\n",
    "\n",
    "Divide both sides by $8 s(\\mathcal{A},n)$:\n",
    "\n",
    "$e^{-n\\epsilon^2/32} \\le \\frac{\\alpha}{8s(\\mathcal{A},n)}$\n",
    "\n",
    "Take $\\ln$:\n",
    "\n",
    "$-\\frac{n\\epsilon^2}{32} \\le \\ln\\left(\\frac{\\alpha}{8s(\\mathcal{A},n)}\\right)$\n",
    "\n",
    "Multiply by $-1$ (this flips the inequality):\n",
    "\n",
    "$\\frac{n\\epsilon^2}{32} \\ge \\ln\\left(\\frac{8s(\\mathcal{A},n)}{\\alpha}\\right)$\n",
    "\n",
    "Solve for $\\epsilon$:\n",
    "\n",
    "$\\epsilon \\ge \\sqrt{\\frac{32}{n}\\ln\\left(\\frac{8s(\\mathcal{A},n)}{\\alpha}\\right)}$\n",
    "\n",
    "We take the smallest such $\\epsilon$ as our interval half-width:\n",
    "\n",
    "$\\boxed{l = \\sqrt{\\frac{32}{n}\\ln\\left(\\frac{8s(\\mathcal{A},n)}{\\alpha}\\right)}}$\n",
    "\n",
    "This is exactly what the code stores in `problem6_VC_l`.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5 — Decide if the VC interval is smaller than the test Hoeffding interval\n",
    "\n",
    "You already computed a test-set Hoeffding half-width `problem6_accuracy_l`.\n",
    "\n",
    "So we answer:\n",
    "\n",
    "`problem6_VC_smaller = (problem6_VC_l < problem6_accuracy_l)`\n",
    "\n",
    "If True: VC bound using all data is smaller;  \n",
    "If False: the test Hoeffding interval is smaller.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "1. Use Theorem 8.29 for a VC-based uniform convergence bound. :contentReference[oaicite:3]{index=3}  \n",
    "2. Use Sauer–Shelah Lemma 8.36 to bound the growth function for VC=3. :contentReference[oaicite:4]{index=4}  \n",
    "3. Solve for $\\epsilon$ at $\\alpha=0.05$ to get the interval half-width $l$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "922690ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_all (all data) = 5572\n",
      "s(A,n) upper bound (VC=3) = 15526379\n",
      "problem6_VC_l = 0.3524764186156469\n",
      "problem6_accuracy_l (test Hoeffding) = 0.033213458236811565\n",
      "problem6_VC_smaller = False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import comb\n",
    "\n",
    "# ============================================================\n",
    "# VC-DIMENSION GENERALIZATION INTERVAL (for \"all data\" accuracy)\n",
    "# ============================================================\n",
    "#\n",
    "# Goal: Compute a bound parameter l (half-width) for accuracy when:\n",
    "#   - we DO NOT use a held-out test set\n",
    "#   - instead we use a VC-dimension generalization bound\n",
    "#   - given VC dimension d = 3\n",
    "#\n",
    "# In the lecture notes, the key result is Theorem 8.29:\n",
    "#   P( sup_{A in A} |nu_n(A) - nu(A)| > eps ) <= 8 * s(A,n) * exp(- n eps^2 / 32)\n",
    "# where:\n",
    "#   - nu(A) is the true probability of event A\n",
    "#   - nu_n(A) is the empirical probability (fraction in sample)\n",
    "#   - s(A,n) is the growth function / shattering coefficient for the class A\n",
    "#\n",
    "# We will:\n",
    "#   1) pick confidence level alpha = 0.05 (95% confidence)\n",
    "#   2) upper bound s(A,n) using Sauer–Shelah (Lemma 8.36):\n",
    "#        s(H,n) <= sum_{i=0}^{VC-1} binom(n, i)\n",
    "#      For VC=3 -> sum_{i=0}^{2} binom(n,i)\n",
    "#   3) solve the inequality for eps so that the RHS <= alpha\n",
    "#\n",
    "# Then eps is the \"l\" you store in problem6_VC_l.\n",
    "# Finally compare to test accuracy Hoeffding half-width (problem6_accuracy_l).\n",
    "# ============================================================\n",
    "\n",
    "alpha = 0.05          # 95% confidence => alpha = 0.05\n",
    "d_vc = 3              # VC-dimension given in the task\n",
    "\n",
    "# \"All data\" means: use the full dataset size (NOT only the test set size)\n",
    "# In your earlier code you had labels in Y (entire dataset):\n",
    "n_all = len(Y)\n",
    "\n",
    "# --- Growth function bound via Sauer–Shelah (Lemma 8.36) ---\n",
    "# For VC dimension = d, the shattering coefficient is bounded by:\n",
    "#   s(H,n) <= sum_{i=0}^{d-1} C(n,i)\n",
    "# Here d_vc = 3, so i = 0,1,2\n",
    "#\n",
    "# comb(n_all, 0) = 1\n",
    "# comb(n_all, 1) = n_all\n",
    "# comb(n_all, 2) = n_all*(n_all-1)/2\n",
    "s_An = sum(comb(n_all, i) for i in range(d_vc))\n",
    "\n",
    "# --- Solve Theorem 8.29 bound for eps ---\n",
    "#\n",
    "# Theorem 8.29 says:\n",
    "#   P( sup |nu_n - nu| > eps ) <= 8*s_An * exp( - n_all * eps^2 / 32 )\n",
    "#\n",
    "# We want the RHS <= alpha:\n",
    "#   8*s_An * exp( - n_all * eps^2 / 32 ) <= alpha\n",
    "#\n",
    "# Divide both sides by 8*s_An:\n",
    "#   exp( - n_all * eps^2 / 32 ) <= alpha / (8*s_An)\n",
    "#\n",
    "# Take ln:\n",
    "#   - n_all * eps^2 / 32 <= ln( alpha / (8*s_An) )\n",
    "#\n",
    "# Multiply by -1 (flips inequality):\n",
    "#   n_all * eps^2 / 32 >= ln( (8*s_An) / alpha )\n",
    "#\n",
    "# Solve for eps:\n",
    "#   eps >= sqrt( (32/n_all) * ln( (8*s_An)/alpha ) )\n",
    "#\n",
    "# We take eps exactly as that RHS (smallest eps making it hold).\n",
    "problem6_VC_l = np.sqrt((32.0 / n_all) * np.log((8.0 * s_An) / alpha))\n",
    "\n",
    "# --- Compare VC interval half-width to test Hoeffding half-width ---\n",
    "# problem6_accuracy_l should already exist from your earlier accuracy Hoeffding calculation\n",
    "problem6_VC_smaller = (problem6_VC_l < problem6_accuracy_l)\n",
    "\n",
    "print(f\"n_all (all data) = {n_all}\")\n",
    "print(f\"s(A,n) upper bound (VC={d_vc}) = {s_An}\")\n",
    "print(f\"problem6_VC_l = {problem6_VC_l}\")\n",
    "print(f\"problem6_accuracy_l (test Hoeffding) = {problem6_accuracy_l}\")\n",
    "print(f\"problem6_VC_smaller = {problem6_VC_smaller}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
