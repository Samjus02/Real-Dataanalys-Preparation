{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b465e0d9",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Exam 17th of January 2025, 8.00-13.00 for the course 1MS041 (Introduction to Data Science / Introduktion till dataanalys)\n",
    "\n",
    "## Instructions:\n",
    "1. Complete the problems by following instructions.\n",
    "2. When done, submit this file with your solutions saved, following the instruction sheet.\n",
    "\n",
    "This exam has 3 problems for a total of 40 points, to pass you need\n",
    "20 points. The bonus will be added to the score of the exam and rounded afterwards.\n",
    "\n",
    "## Some general hints and information:\n",
    "* Try to answer all questions even if you are uncertain.\n",
    "* Comment your code, so that if you get the wrong answer I can understand how you thought\n",
    "this can give you some points even though the code does not run.\n",
    "* Follow the instruction sheet rigorously.\n",
    "* This exam is partially autograded, but your code and your free text answers are manually graded anonymously.\n",
    "* If there are any questions, please ask the exam guards, they will escalate it to me if necessary.\n",
    "\n",
    "## Tips for free text answers\n",
    "* Be VERY clear with your reasoning, there should be zero ambiguity in what you are referring to.\n",
    "* If you want to include math, you can write LaTeX in the Markdown cells, for instance `$f(x)=x^2$` will be rendered as $f(x)=x^2$ and `$$f(x) = x^2$$` will become an equation line, as follows\n",
    "$$f(x) = x^2$$\n",
    "Another example is `$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$` which renders as\n",
    "$$f_{Y \\mid X}(y,x) = P(Y = y \\mid X = x) = \\exp(\\alpha \\cdot x + \\beta)$$\n",
    "\n",
    "## Finally some rules:\n",
    "* You may not communicate with others during the exam, for example:\n",
    "    * You cannot ask for help in Stack-Overflow or other such help forums during the Exam.\n",
    "    * You may not communicate with AI's, for instance ChatGPT.\n",
    "    * Your on-line and off-line activity is being monitored according to the examination rules.\n",
    "\n",
    "## Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "40240477",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Insert your anonymous exam ID as a string in the variable below\n",
    "examID=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a59a3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from scipy import optimize\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c67547",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 1\n",
    "Maximum Points = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea1654",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "This problem is about SVD and anomaly detection. In all the problems where you are asked to produce a matrix or vector, they should be **numpy arrays**. \n",
    "\n",
    "1. [4p] Load the file `data/SVD.csv` as instructed in the code cell. Compute the Singular Value Decomposition, i.e. construct the three matrices $U$, $D$, $V$ such that if $X$ is the data matrix of shape `n_samples x n_dimensions` then $X = UDV^T$. Put the resulting matrices in their variables, check that the shapes align with the instructions in the code cell. Finally, extract the first right and left singular vectors and store those as 1-d arrays in the instructed variables.\n",
    "2. [3p] The first goal is to calculate the explained variance, check the lecture notes for definition. Calculate the explained variance of using $1$, $2$,... number of singular vectors and select how many singular vectors are needed in order to explain at least $95\\%$ of the variance.\n",
    "3. [3p] With the number of components chosen in part 2, construct the best approximating matrix with the rank as the number of components. Explain what each row represents in the approximating matrix in terms of the original data, write your answer as free text in the Markdown cell below as instructed in the cells.\n",
    "4. [4p] Create a vector which corresponds to the row-wise (Euclidean) distance between the original matrix `problem1_data`and the approximating matrix `problem1_approximation` and plot the empirical distribution function of that distance. Based on the empirical distribution function choose a threshold such that 10 samples are above it and the rest below. Store the 10 samples in the instructed variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b66e0d",
   "metadata": {},
   "source": [
    "-----\n",
    "This problem is about SVD and anomaly detection. In all the problems where you are asked to produce a matrix or vector, they should be **numpy arrays**. \n",
    "\n",
    "1. [4p] Load the file `data/SVD.csv` as instructed in the code cell. Compute the Singular Value Decomposition, i.e. construct the three matrices $U$, $D$, $V$ such that if $X$ is the data matrix of shape `n_samples x n_dimensions` then $X = UDV^T$. Put the resulting matrices in their variables, check that the shapes align with the instructions in the code cell. Finally, extract the first right and left singular vectors and store those as 1-d arrays in the instructed variables.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8ea11d5a",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (1010, 100)\n",
      "U: (1010, 100)\n",
      "D: (100, 100)\n",
      "V: (100, 100)\n",
      "first right: (100,)\n",
      "first left: (1010,)\n",
      "Reconstruction correct? True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 1: 4 points\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# 1) Load data as a pure numeric matrix (no header)\n",
    "problem1_data = pd.read_csv(\"data/SVD.csv\", header=None).to_numpy(dtype=float)\n",
    "\n",
    "# We'll call the data matrix X to match the standard SVD notation.\n",
    "# Shape: (n_samples, n_dimensions)\n",
    "X = problem1_data\n",
    "\n",
    "# U: matrix of left singular vectors, shape (n_samples, n_dimensions)\n",
    "# D: vector of singular values, shape (n_dimensions,)\n",
    "# V: matrix of right singular vectors, shape (n_dimensions, n_dimensions)\n",
    "\n",
    "# 2) Compute SVD\n",
    "# NumPy returns U, s, VT such that:\n",
    "#   X = U @ diag(s) @ VT\n",
    "# With full_matrices=False:\n",
    "#   U  : (n_samples, r)\n",
    "#   s  : (r,)  where r = min(n_samples, n_dimensions)\n",
    "#   VT : (r, n_dimensions)\n",
    "problem1_U, s, VT = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# 3) Build D as a DIAGONAL MATRIX (as the assignment/notes define it)\n",
    "# D : (r, r)\n",
    "problem1_D = np.diag(s)\n",
    "\n",
    "# 4) Construct V (not VT)\n",
    "# VT is V^T, so V is VT.T\n",
    "# V : (n_dimensions, r)\n",
    "problem1_V = VT.T\n",
    "\n",
    "# This is so I understand how slicing works:\n",
    "# This is how the general slicing logic works like: array[rows, columns]\n",
    "# : → take all elements\n",
    "# 0 → take index 0\n",
    "# So this simply means: “Take all rows, but only column 0”\n",
    "# problem1_V looks like this: (n_dimensions​,n_dimensions) \n",
    "# Using the slicing logic above, this will give us the shape: (n_dimensions, )\n",
    "# So the result is a 1D NumPy array, exactly what you want for a singular vector.\n",
    "\n",
    "# 5) Extract first right and left singular vectors as 1D arrays\n",
    "# Right singular vectors are columns of V  -> shape (n_dimensions,)\n",
    "problem1_first_right_singular_vector = problem1_V[:, 0]\n",
    "\n",
    "# Left singular vectors are columns of U   -> shape (n_samples,)\n",
    "problem1_first_left_singular_vector = problem1_U[:, 0]\n",
    "#print(\"This is the first left singular vector: \\n\", problem1_first_left_singular_vector)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Optional sanity checks (safe to keep while debugging)\n",
    "# ------------------------------------------------------------\n",
    "print(\"X:\", X.shape)\n",
    "print(\"U:\", problem1_U.shape)\n",
    "print(\"D:\", problem1_D.shape)\n",
    "print(\"V:\", problem1_V.shape)\n",
    "print(\"first right:\", problem1_first_right_singular_vector.shape)\n",
    "print(\"first left:\", problem1_first_left_singular_vector.shape)\n",
    "\n",
    "# Check reconstruction: X ≈ U D V^T\n",
    "X_recon = problem1_U @ problem1_D @ problem1_V.T\n",
    "print(\"Reconstruction correct?\", np.allclose(X, X_recon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe8bd2",
   "metadata": {},
   "source": [
    "-----\n",
    "2. [3p] The first goal is to calculate the explained variance, check the lecture notes for definition. Calculate the explained variance of using $1$, $2$,... number of singular vectors and select how many singular vectors are needed in order to explain at least $95\\%$ of the variance.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284f758",
   "metadata": {},
   "source": [
    "## Explained variance for SVD\n",
    "\n",
    "When applying Singular Value Decomposition (SVD) to a data matrix $X$, it is decomposed as\n",
    "$$\n",
    "X = U \\Sigma V^\\top\n",
    "$$\n",
    "where $\\Sigma$ is a diagonal matrix containing the singular values $\\sigma_1, \\sigma_2, \\dots, \\sigma_r$, with $r = \\min(n_{\\text{samples}}, n_{\\text{dimensions}})$.\n",
    "\n",
    "Each singular value $\\sigma_j$ represents how much variation in the data is captured along the corresponding singular direction. Since variance is proportional to squared magnitude, the contribution of each component to the total variance is given by $\\sigma_j^2$.\n",
    "\n",
    "---\n",
    "\n",
    "## Definition of explained variance\n",
    "\n",
    "The explained variance for the first $k$ singular components is defined as\n",
    "$$\n",
    "\\text{ExplainedVariance}(k) = \\frac{\\sum_{j=1}^{k} \\sigma_j^2}{\\sum_{j=1}^{r} \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "This value lies between $0$ and $1$ and measures the fraction of the total variance in the data that is captured by the first $k$ components.\n",
    "\n",
    "---\n",
    "\n",
    "## How explained variance is computed in the code\n",
    "\n",
    "1. The singular values $\\sigma_1, \\dots, \\sigma_r$ are extracted from the diagonal of the matrix $D$.\n",
    "2. Each singular value is squared to obtain its variance contribution $\\sigma_j^2$.\n",
    "3. The total variance is computed as\n",
    "$$\n",
    "\\sum_{j=1}^{r} \\sigma_j^2\n",
    "$$\n",
    "4. The cumulative explained variance is computed as\n",
    "$$\n",
    "\\frac{\\sum_{j=1}^{k} \\sigma_j^2}{\\sum_{j=1}^{r} \\sigma_j^2}\n",
    "$$\n",
    "for $k = 1, \\dots, r$.\n",
    "5. The smallest number of components $k$ such that the explained variance is at least $95\\%$ is selected.\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "Explained variance quantifies how much information from the original data is preserved when keeping only the first $k$ singular components. A high explained variance with a small number of components indicates that the data lies close to a low-dimensional subspace.\n",
    "\n",
    "Selecting $k$ based on explained variance provides a principled trade-off between dimensionality reduction and information loss, and is commonly used before reconstruction, compression, or anomaly detection.\n",
    "\n",
    "---\n",
    "\n",
    "## Key conclusion\n",
    "\n",
    "The explained variance measures the proportion of total data variability captured by the first $k$ singular values, and choosing the smallest $k$ that reaches a given threshold (such as $95\\%$) ensures that most of the structure in the data is retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "598b5e94",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for >=95% variance: 10\n",
      "Explained variance at that k: 0.9990748276370391\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Part 2: 3 points\n",
    "# Explained variance for SVD:\n",
    "# explained_variance[k] = (sum_{j=1..k} sigma_j^2) / (sum_{j=1..r} sigma_j^2)\n",
    "# where r = min(n_samples, n_dimensions)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# If problem1_D is a diagonal matrix (r x r), extract singular values from its diagonal.\n",
    "# (If you instead stored singular values directly in a vector s, you can use that.)\n",
    "\n",
    "# singular_values = np.diag(problem1_D)  # shape (r,)\n",
    "# I can also just simply use (s) instead of the singular_values since s is the same from Part 1. \n",
    "singular_values = s\n",
    "# singular_values now contains the singular values:\n",
    "# [sigma_1, sigma_2, ..., sigma_r]\n",
    "# These are ordered from largest to smallest by NumPy's SVD implementation.\n",
    "\n",
    "\n",
    "# 1) Convert singular values to \"variance explained\" contributions by squaring\n",
    "# Each singular value sigma_j represents a scale along a singular direction.\n",
    "# Squaring gives sigma_j^2, which is proportional to the amount of variance\n",
    "# explained by that singular component.\n",
    "singular_values_squared = singular_values ** 2\n",
    "\n",
    "\n",
    "# 2) Total variance in this SVD sense\n",
    "# This is the sum of all variance contributions from all singular components:\n",
    "# sum_{j=1..r} sigma_j^2\n",
    "total_variance = np.sum(singular_values_squared)\n",
    "\n",
    "\n",
    "# 3) Cumulative explained variance for k = 1..r\n",
    "# np.cumsum computes the cumulative sum:\n",
    "# [sigma_1^2,\n",
    "#  sigma_1^2 + sigma_2^2,\n",
    "#  sigma_1^2 + sigma_2^2 + sigma_3^2, ...]\n",
    "#\n",
    "# Dividing by total_variance normalizes this so that the values lie in [0, 1],\n",
    "# giving the fraction of total variance explained by the first k components.\n",
    "problem1_explained_variance = np.cumsum(singular_values_squared) / total_variance  # shape (r,)\n",
    "\n",
    "\n",
    "# 4) Smallest number of components to reach at least 95%\n",
    "# problem1_explained_variance >= 0.95 gives a boolean array indicating\n",
    "# which indices have reached the 95% threshold.\n",
    "# np.argmax returns the index of the first True value.\n",
    "# We add 1 because indices start at 0, but k counts components starting at 1.\n",
    "problem1_num_components = int(np.argmax(problem1_explained_variance >= 0.95) + 1)\n",
    "\n",
    "\n",
    "# Print the selected number of components and the explained variance at that k\n",
    "print(\"Number of components for >=95% variance:\", problem1_num_components)\n",
    "print(\"Explained variance at that k:\", problem1_explained_variance[problem1_num_components - 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d50b6",
   "metadata": {},
   "source": [
    "-----\n",
    "3. [3p] With the number of components chosen in part 2, construct the best approximating matrix with the rank as the number of components. Explain what each row represents in the approximating matrix in terms of the original data, write your answer as free text in the Markdown cell below as instructed in the cells.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c980f2",
   "metadata": {},
   "source": [
    "# Part 3: Rank-$k$ approximation of the data matrix (3p)\n",
    "\n",
    "We are given the Singular Value Decomposition (SVD) of the data matrix\n",
    "$X \\in \\mathbb{R}^{n_{\\text{samples}} \\times n_{\\text{dimensions}}}$:\n",
    "$$\n",
    "X = U D V^T\n",
    "$$\n",
    "where:\n",
    "- $U$ contains the left singular vectors,\n",
    "- $D$ is a diagonal matrix with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_m$,\n",
    "- $V$ contains the right singular vectors.\n",
    "\n",
    "From Part 2, we have selected the smallest number of components\n",
    "$$\n",
    "k = \\texttt{problem1\\_num\\_components}\n",
    "$$\n",
    "such that at least $95\\%$ of the variance is explained.\n",
    "\n",
    "To construct the **best rank-$k$ approximation** of $X$, we keep only the\n",
    "first $k$ singular values and their corresponding singular vectors:\n",
    "- $U_k$: the first $k$ columns of $U$,\n",
    "- $D_k$: the $k \\times k$ diagonal matrix with $\\sigma_1, \\dots, \\sigma_k$,\n",
    "- $V_k$: the first $k$ columns of $V$.\n",
    "\n",
    "The approximating matrix is then given by:\n",
    "$$\n",
    "X_k = U_k D_k V_k^T\n",
    "$$\n",
    "\n",
    "Each row of $X_k$ represents the original sample reconstructed using only\n",
    "the $k$ most important latent directions (those explaining the most\n",
    "variance). This removes low-variance structure and keeps the dominant\n",
    "patterns present in the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "517b319e",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (1010, 100)\n",
      "Approximated X shape: (1010, 100)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Part 3: 3 points\n",
    "# Construct the best rank-k approximation of X using SVD,\n",
    "# where k is the number of components chosen in Part 2.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# We already computed the SVD in Part 1:\n",
    "# X = problem1_U @ problem1_D @ problem1_V.T\n",
    "#\n",
    "# The best rank-k approximation is obtained by keeping only\n",
    "# the first k singular values and their corresponding singular vectors.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# NOTE:\n",
    "# We do NOT need to redefine or extract singular values here.\n",
    "# The vector `s` already exists from Part 1 and contains the singular values.\n",
    "# Recomputing them would be redundant.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# s = np.diag(problem1_D)  # NOT needed, since s already exists from Part 1\n",
    "\n",
    "\n",
    "# Number of components selected in Part 2\n",
    "k = problem1_num_components\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Truncate matrices to keep only the first k components\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Keep the first k columns of U (left singular vectors)\n",
    "# Shape: (n_samples, k)\n",
    "problem1_U_k = problem1_U[:, :k]\n",
    "\n",
    "# problem1_D has shape (r, r) and is a diagonal matrix containing the singular values\n",
    "# on its diagonal, ordered from largest to smallest.\n",
    "#\n",
    "# The slicing [:k, :k] means:\n",
    "# - :k on rows  → take rows 0 to k-1\n",
    "# - :k on cols  → take columns 0 to k-1\n",
    "#\n",
    "# This extracts the top-left k x k submatrix of problem1_D, which contains\n",
    "# only the largest k singular values and discards the remaining smaller ones.\n",
    "#\n",
    "# Keeping only this block is necessary for constructing the rank-k SVD\n",
    "# approximation, where only the most important k components are retained.\n",
    "problem1_D_k = problem1_D[:k, :k]\n",
    "\n",
    "\n",
    "# Keep the first k columns of V (right singular vectors)\n",
    "# Shape: (n_dimensions, k)\n",
    "problem1_V_k = problem1_V[:, :k]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Construct the rank-k approximation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# The rank-k approximation is given by:\n",
    "# X_k = U_k @ D_k @ V_k^T\n",
    "#\n",
    "# This matrix is the best approximation of X among all\n",
    "# matrices of rank k in the least-squares sense.\n",
    "problem1_approximation = problem1_U_k @ problem1_D_k @ problem1_V_k.T\n",
    "\n",
    "\n",
    "# Optional sanity check: the approximated matrix must have\n",
    "# the same shape as the original data matrix X\n",
    "print(\"Original X shape:\", X.shape)\n",
    "print(\"Approximated X shape:\", problem1_approximation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d90bbc",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put the explanation for **part 3** of the rows of the approximating matrix below this line in this cell. In order to enter edit mode you can doubleclick this cell or select it and just press enter.\n",
    "\n",
    "I looked at wikipedia and old posts stack overflow to solve the problem. What does each row represent in the matrix? Each row represents the results of a multiplication between right and left singular vector values with singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cb70e",
   "metadata": {},
   "source": [
    "-----\n",
    "4. [4p] Create a vector which corresponds to the row-wise (Euclidean) distance between the original matrix `problem1_data`and the approximating matrix `problem1_approximation` and plot the empirical distribution function of that distance. Based on the empirical distribution function choose a threshold such that 10 samples are above it and the rest below. Store the 10 samples in the instructed variable.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1c0b0",
   "metadata": {},
   "source": [
    "# Part 4: Reconstruction error, EDF, threshold and outliers (4p)\n",
    "\n",
    "## Reconstruction error\n",
    "To detect anomalies, we measure how well each row (sample) is reconstructed by the rank-$k$\n",
    "approximation from Part 3. For each sample $i$ we compute the row-wise Euclidean distance\n",
    "between the original row and the reconstructed row:\n",
    "$$\n",
    "e_i = \\lVert X_{i,:} - (X_k)_{i,:} \\rVert_2\n",
    "$$\n",
    "This produces a vector of reconstruction errors of shape $(n_{\\text{samples}},)$.\n",
    "\n",
    "## Empirical Distribution Function (EDF)\n",
    "To visualize the distribution of reconstruction errors, we plot the empirical distribution function (EDF).\n",
    "We sort the errors and plot:\n",
    "- x-axis: sorted reconstruction errors\n",
    "- y-axis: $(1/n, 2/n, \\dots, n/n)$\n",
    "\n",
    "## Threshold choice and outliers\n",
    "We choose the threshold so that exactly 10 samples have reconstruction error strictly larger than the threshold. We do this by sorting the errors and setting the threshold to a value between the 11th-largest and 10th-largest errors. If they are equal (tie), we set the threshold to be the largest value strictly smaller than the 10th-largest error using np.nextafter, which guarantees exactly 10 samples satisfy $e_i >threshold$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c3ae2296",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem1_reconstruction_error shape: (1010,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected threshold (problem1_threshold): 4.382758304950724\n",
      "Number above threshold: 10\n",
      "problem1_outliers.shape: (10, 100)\n",
      "Largest 15 errors: [ 0.01892771  0.01987526  0.02063347  0.02149633  0.02203522  8.74348139\n",
      "  8.76943228  8.8704323   9.15587334  9.44257285  9.46089921  9.69737044\n",
      "  9.83200702 10.25749798 10.72215972]\n",
      "10th largest error: 8.743481393856763\n",
      "11th largest error: 0.02203521604468554\n",
      "Threshold: 4.382758304950724\n",
      "# above threshold: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 4: 4 points\n",
    "# Goal:\n",
    "#   1) Compute row-wise reconstruction error:\n",
    "#        e_i = ||X[i,:] - X_hat[i,:]||_2\n",
    "#   2) Plot the empirical distribution function (EDF) of errors\n",
    "#   3) Choose a threshold so that exactly 10 samples are above it\n",
    "#   4) Store the 10 outlier samples in problem1_outliers\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Reconstruction error per row: e_i = ||X[i,:] - X_hat[i,:]||_2\n",
    "# ------------------------------------------------------------\n",
    "# diff_matrix has shape (n_samples, n_dimensions)\n",
    "diff_matrix = problem1_data - problem1_approximation\n",
    "\n",
    "# Row-wise Euclidean norm -> vector of length n_samples\n",
    "problem1_reconstruction_error = np.linalg.norm(diff_matrix, axis=1)\n",
    "\n",
    "print(\"problem1_reconstruction_error shape:\", problem1_reconstruction_error.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Plot Empirical Distribution Function (EDF)\n",
    "# ------------------------------------------------------------\n",
    "# Sort errors ascending for EDF x-axis\n",
    "sorted_errors = np.sort(problem1_reconstruction_error)\n",
    "n = sorted_errors.size\n",
    "\n",
    "# EDF y-values are 1/n, 2/n, ..., n/n\n",
    "edf_values = np.arange(1, n + 1) / n\n",
    "\n",
    "plt.figure()\n",
    "plt.step(sorted_errors, edf_values, where=\"post\")\n",
    "plt.xlabel(\"Reconstruction error (row-wise Euclidean distance)\")\n",
    "plt.ylabel(\"Empirical distribution function (EDF)\")\n",
    "plt.title(\"EDF of reconstruction error\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Choose threshold so that exactly 10 samples satisfy error > threshold\n",
    "# ------------------------------------------------------------\n",
    "# Argsort gives indices that would sort the errors ascending\n",
    "sorted_indices = np.argsort(problem1_reconstruction_error)\n",
    "\n",
    "# The 10 largest errors correspond to the last 10 indices\n",
    "outlier_indices = sorted_indices[-10:]\n",
    "\n",
    "# Boundary values:\n",
    "# e10 = 10th largest error, e11 = 11th largest error\n",
    "e11 = sorted_errors[-11]\n",
    "e10 = sorted_errors[-10]\n",
    "\n",
    "if e10 > e11:\n",
    "    # If the boundary errors are distinct, pick any number strictly between them.\n",
    "    # Then exactly the largest 10 errors are > threshold.\n",
    "    problem1_threshold = (e10 + e11) / 2.0\n",
    "else:\n",
    "    # Tie at the boundary (e10 == e11):\n",
    "    # If we want to avoid counting errors equal to e10 as \"above\",\n",
    "    # we move the threshold slightly ABOVE e10.\n",
    "    problem1_threshold = np.nextafter(e10, np.inf)\n",
    "\n",
    "count_above = np.sum(problem1_reconstruction_error > problem1_threshold)\n",
    "print(\"Selected threshold (problem1_threshold):\", problem1_threshold)\n",
    "print(\"Number above threshold:\", count_above)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Store the 10 outlier samples (rows of the original data)\n",
    "# ------------------------------------------------------------\n",
    "problem1_outliers = problem1_data[outlier_indices]\n",
    "print(\"problem1_outliers.shape:\", problem1_outliers.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Debug prints (optional)\n",
    "# ------------------------------------------------------------\n",
    "print(\"Largest 15 errors:\", sorted_errors[-15:])\n",
    "print(\"10th largest error:\", sorted_errors[-10])\n",
    "print(\"11th largest error:\", sorted_errors[-11])\n",
    "print(\"Threshold:\", problem1_threshold)\n",
    "print(\"# above threshold:\", np.sum(problem1_reconstruction_error > problem1_threshold))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d0d75",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 2\n",
    "Maximum Points = 14"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3b664ad",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "2. [4p] A page loads in $\\text{Exp}(1)$ (Exponentially distributed with mean $1$) seconds if not preloaded and loads with $\\text{Exp}(10)$ (Exponentially distributed with mean $1/10$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $1$ simulate $10000$ load times from page $1$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02d2a5",
   "metadata": {},
   "source": [
    "-----\n",
    "In this problem we have data consisting of user behavior on a website. The pages of the website are just numbers in the dataset $0,1,2,\\ldots$ and each row consists of a user, a source and a destination page. This signifies that the user was on the source page and clicked a link leading them to the destination page. The goal is to improve the user experience by decreasing load time of the next page visited, as such we need a good estimate for the next site likely to be visited. We will model this using a homogeneous Markov chain, each row in the data-file then corresponds to a single realization of a transition. \n",
    "\n",
    "1. [3p] Load the data in the file `data/websites.csv` and construct a matrix of size `n_pages x n_pages` which is the maximum likelihood estimate of the true transition matrix for the Markov chain. Here the ordering of the states are exactly the ones in the data-file, that is page $0$ has index $0$ in the matrix.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46c32b",
   "metadata": {},
   "source": [
    "## Estimation of the transition matrix\n",
    "\n",
    "Each row in the dataset corresponds to one observed transition in a homogeneous\n",
    "Markov chain, where a user moves from a source page to a destination page.\n",
    "The user identity is not relevant for the Markov assumption and is therefore ignored.\n",
    "\n",
    "Let $( P_{ij} = P(X_{t+1} = j \\mid X_t = i) )$ denote the transition probability from\n",
    "page $( i )$ to page $( j )$. The maximum likelihood estimate of the transition matrix\n",
    "is obtained by empirical frequencies:\n",
    "\n",
    "$$\n",
    "[\n",
    "\\hat{P}_{ij} = \\frac{N_{ij}}{\\sum_j N_{ij}}\n",
    "]\n",
    "$$\n",
    "\n",
    "where $( N_{ij} )$ is the number of observed transitions from page $( i )$ to page $( j )$.\n",
    "\n",
    "The procedure is as follows:\n",
    "1. Determine the number of states as the maximum page index plus one.\n",
    "2. Count the number of transitions from each source page to each destination page.\n",
    "3. Normalize each row so that it sums to one, yielding a valid transition matrix.\n",
    "\n",
    "This produces the maximum likelihood estimate of the transition matrix for the\n",
    "Markov chain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877dc08",
   "metadata": {},
   "source": [
    "### How the transition-count loop works (4×4 example summary)\n",
    "\n",
    "Assume we have a website with **4 pages**, labeled $0, 1, 2, 3$.  \n",
    "We therefore construct a **$4 \\times 4$ transition-count matrix** called `counts`, initialized with zeros:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{counts} &= \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each row in the dataset corresponds to **one observed transition** from a *source page* to a *destination page*.\n",
    "\n",
    "The loop\n",
    "\n",
    "```python\n",
    "for _, row in data.iterrows():\n",
    "    i = int(row[\"source\"])\n",
    "    j = int(row[\"destination\"])\n",
    "    counts[i, j] += 1\n",
    "```\n",
    "\n",
    "works as follows:\n",
    "\n",
    "- The loop iterates through the dataset **row by row**\n",
    "- For each row:\n",
    "  - $i$ is the current page (value in `source`)\n",
    "  - $j$ is the next page (value in `destination`)\n",
    "  - The matrix entry `counts[i, j]` is increased by $1$\n",
    "\n",
    "This means:\n",
    "- **Rows** represent the current page\n",
    "- **Columns** represent the next page\n",
    "- `counts[i, j]` stores how many times the transition $i \\rightarrow j$ was observed\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose the dataset contains the following transitions:\n",
    "\n",
    "$1 \\rightarrow 3$  \n",
    "$1 \\rightarrow 2$  \n",
    "$3 \\rightarrow 2$  \n",
    "$1 \\rightarrow 3$\n",
    "\n",
    "After processing all rows, the count matrix becomes:\n",
    "\n",
    "$$\n",
    "\\text{counts} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 2 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This tells us:\n",
    "- From page $1$, users moved to page $2$ once and to page $3$ twice\n",
    "- From page $3$, users moved to page $2$ once\n",
    "\n",
    "This count matrix is then **row-normalized** to obtain the **maximum likelihood estimate of the Markov transition matrix**, where each row represents a probability distribution over the next page given the current page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "521439a9",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the problem2_n_states:  10\n",
      "Counts: \n",
      " [[ 0. 22.  0. 10.  4.  1.  7.  9. 13. 17.]\n",
      " [13.  0. 12. 27.  2. 30. 16. 18.  4.  6.]\n",
      " [19. 12.  0. 11. 14. 16.  2.  0.  4. 14.]\n",
      " [ 1. 10. 21.  0.  8.  7. 15. 21.  0. 23.]\n",
      " [13. 12.  8. 10.  0.  7. 12.  4.  2.  4.]\n",
      " [ 1. 19.  3.  6. 16.  0. 16.  2. 18. 21.]\n",
      " [ 4.  7. 20.  9.  0. 16.  0. 16. 27. 16.]\n",
      " [15.  5.  1. 12.  8.  3. 20.  0. 16. 13.]\n",
      " [12. 10. 12.  4.  6. 14.  7.  8.  0. 11.]\n",
      " [ 4. 31. 15. 17. 14.  8. 20. 15.  0.  0.]]\n",
      "This is the correct transition matrix with correct probabilities: \n",
      " [[0.         0.26506024 0.         0.12048193 0.04819277 0.01204819\n",
      "  0.08433735 0.10843373 0.15662651 0.20481928]\n",
      " [0.1015625  0.         0.09375    0.2109375  0.015625   0.234375\n",
      "  0.125      0.140625   0.03125    0.046875  ]\n",
      " [0.20652174 0.13043478 0.         0.11956522 0.15217391 0.17391304\n",
      "  0.02173913 0.         0.04347826 0.15217391]\n",
      " [0.00943396 0.09433962 0.19811321 0.         0.0754717  0.06603774\n",
      "  0.14150943 0.19811321 0.         0.21698113]\n",
      " [0.18055556 0.16666667 0.11111111 0.13888889 0.         0.09722222\n",
      "  0.16666667 0.05555556 0.02777778 0.05555556]\n",
      " [0.00980392 0.18627451 0.02941176 0.05882353 0.15686275 0.\n",
      "  0.15686275 0.01960784 0.17647059 0.20588235]\n",
      " [0.03478261 0.06086957 0.17391304 0.07826087 0.         0.13913043\n",
      "  0.         0.13913043 0.23478261 0.13913043]\n",
      " [0.16129032 0.05376344 0.01075269 0.12903226 0.08602151 0.03225806\n",
      "  0.21505376 0.         0.17204301 0.13978495]\n",
      " [0.14285714 0.11904762 0.14285714 0.04761905 0.07142857 0.16666667\n",
      "  0.08333333 0.0952381  0.         0.13095238]\n",
      " [0.03225806 0.25       0.12096774 0.13709677 0.11290323 0.06451613\n",
      "  0.16129032 0.12096774 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 1: Maximum Likelihood Estimation of a Markov chain\n",
    "# ------------------------------------------------------------\n",
    "# We model user navigation on a website as a homogeneous Markov chain.\n",
    "# Each page is a state, and each row in the dataset corresponds to\n",
    "# one observed transition from a source page to a destination page.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Load the dataset\n",
    "# ------------------------------------------------------------\n",
    "# The CSV file contains three columns:\n",
    "#   - user        : user identifier (NOT used in the Markov model)\n",
    "#   - source      : page the user is currently on\n",
    "#   - destination : page the user clicks next\n",
    "#\n",
    "# Each row corresponds to exactly ONE observed transition:\n",
    "#     source -> destination\n",
    "#\n",
    "# Since the Markov chain is assumed to be homogeneous, the transition\n",
    "# probabilities depend ONLY on the current page, not on the user.\n",
    "# Therefore, we completely ignore the \"user\" column.\n",
    "data = pd.read_csv(\"data/websites.csv\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Determine how many states (pages) exist\n",
    "# ------------------------------------------------------------\n",
    "# Pages are labeled as integers: 0, 1, 2, ...\n",
    "#\n",
    "# If the largest page ID appearing in the data is M,\n",
    "# then the total number of states is M + 1.\n",
    "#\n",
    "# We must check BOTH the \"source\" and \"destination\" columns:\n",
    "# - A page might appear only as a destination and still be a valid state.\n",
    "max_page_id = max(data[\"source\"].max(), data[\"destination\"].max())\n",
    "problem2_n_states = int(max_page_id + 1)\n",
    "\n",
    "# This tells us the size of the transition matrix:\n",
    "# problem2_n_states x problem2_n_states\n",
    "print(\"This is the problem2_n_states: \", problem2_n_states)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Create a transition count matrix\n",
    "# ------------------------------------------------------------\n",
    "# counts[i, j] will store how many times we observed a transition:\n",
    "#     page i  ->  page j\n",
    "#\n",
    "# Initially, we set all counts to zero.\n",
    "counts = np.zeros((problem2_n_states, problem2_n_states), dtype=float)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Count transitions from the dataset\n",
    "# ------------------------------------------------------------\n",
    "# We loop through each row of the dataset.\n",
    "# For each observed transition:\n",
    "#   - i = source page\n",
    "#   - j = destination page\n",
    "# we increment counts[i, j] by 1.\n",
    "#\n",
    "# After this loop, counts[i, j] equals the total number of times\n",
    "# users moved from page i to page j in the data.\n",
    "for _, row in data.iterrows():\n",
    "    i = int(row[\"source\"])        # current page (state i)\n",
    "    j = int(row[\"destination\"])   # next page (state j)\n",
    "    counts[i, j] += 1             # record one observed transition i -> j\n",
    "\n",
    "# Display the raw transition counts\n",
    "print(\"Counts: \\n\", counts)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Convert counts to transition probabilities (MLE)\n",
    "# ------------------------------------------------------------\n",
    "# For a Markov chain, the maximum likelihood estimator (MLE) is:\n",
    "#\n",
    "#   P_hat[i, j] = counts[i, j] / sum_j counts[i, j]\n",
    "#\n",
    "# That is:\n",
    "# - Each row is normalized so it sums to 1\n",
    "# - Each row becomes a probability distribution over next pages\n",
    "#\n",
    "# IMPORTANT EDGE CASE:\n",
    "# If a page i never appears as a source in the dataset, then\n",
    "#   sum_j counts[i, j] = 0\n",
    "# In that case, division by zero is impossible.\n",
    "# We handle this by leaving that entire row as zeros.\n",
    "problem2_transition_matrix = np.zeros_like(counts)\n",
    "\n",
    "for i in range(problem2_n_states):\n",
    "    # Total number of observed outgoing transitions from page i\n",
    "    row_sum = counts[i].sum()\n",
    "\n",
    "    if row_sum > 0:\n",
    "        # Normalize the row so probabilities sum to 1\n",
    "        problem2_transition_matrix[i, :] = counts[i, :] / row_sum\n",
    "    else:\n",
    "        # No observed transitions leaving page i\n",
    "        # We leave the row as all zeros\n",
    "        problem2_transition_matrix[i, :] = 0.0\n",
    "\n",
    "# Print the estimated transition matrix\n",
    "print(\"This is the correct transition matrix with correct probabilities: \\n\",\n",
    "      problem2_transition_matrix)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# (Optional) 6) Sanity check\n",
    "# ------------------------------------------------------------\n",
    "# For a valid transition matrix:\n",
    "# - Rows with outgoing transitions should sum to 1\n",
    "# - Rows with no outgoing transitions should sum to 0\n",
    "#\n",
    "# This line ONLY computes the row sums; it does not change anything.\n",
    "# Uncomment it if you want to verify correctness.\n",
    "# print(problem2_transition_matrix.sum(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e573271",
   "metadata": {},
   "source": [
    "-----\n",
    "2. [4p] A page loads in $\\text{Exp}(1)$ (Exponentially distributed with mean $1$) seconds if not preloaded and loads with $\\text{Exp}(10)$ (Exponentially distributed with mean $1/10$) seconds if preloaded and we only preload the most likely next site. Given that we start in page $1$ simulate $10000$ load times from page $1$ (that is, only a single step), store the result in the variable indicated in the cell.\n",
    "Repeat the experiment but this time preload the two most likely pages and store the result in the indicated variable.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842eb2cf",
   "metadata": {},
   "source": [
    "## Simulation of page load times with preloading\n",
    "\n",
    "We start in state (page) 1. The next page is simulated using the estimated Markov chain:\n",
    "$$\n",
    "[\n",
    "P(X_{t+1}=j \\mid X_t=1) = \\hat{P}_{1j}\n",
    "]\n",
    "$$\n",
    "Thus, for each of 10,000 users we sample the next page from the categorical distribution given by\n",
    "row 1 of the transition matrix.\n",
    "\n",
    "We then simulate the load time depending on whether the realized next page was preloaded:\n",
    "\n",
    "- If a page is **not preloaded**, the load time is $( \\text{Exp}(1) )$, which has mean $(1)$.\n",
    "- If a page **is preloaded**, the load time is $( \\text{Exp}(10) )$, which has mean $(1/10)$.\n",
    "\n",
    "We run two experiments:\n",
    "1. Preload only the **most likely** next page (top-1).\n",
    "2. Preload the **two most likely** next pages (top-2).\n",
    "\n",
    "In NumPy, `np.random.exponential(scale=...)` uses `scale` equal to the mean, so we use\n",
    "`scale=1.0` for $( \\text{Exp}(1) )$ and `scale=0.1` for $( \\text{Exp}(10) )$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3adcdd2",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 preloaded page: 5\n",
      "Top-2 preloaded pages: [5 3]\n",
      "Mean load time (top-1 preloaded): 0.8040508536789033\n",
      "Mean load time (top-2 preloaded): 0.605934983857993\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 2: Simulate page load times starting from page 1\n",
    "# ------------------------------------------------------------\n",
    "# We simulate a SINGLE transition from page 1:\n",
    "#   next_page ~ categorical(problem2_transition_matrix[1, :])\n",
    "#\n",
    "# Load time rules:\n",
    "# - If the realized next page is preloaded:\n",
    "#       load time ~ Exp(10)  -> mean = 0.1 seconds\n",
    "# - Otherwise:\n",
    "#       load time ~ Exp(1)   -> mean = 1.0 seconds\n",
    "#\n",
    "# NOTE:\n",
    "# In NumPy, np.random.exponential(scale=...) uses \"scale = mean\".\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "n_users = 10000\n",
    "current_page = 1  # start in page 1 (0-indexed)\n",
    "\n",
    "# Probability distribution for the next page given current page = 1\n",
    "p_next = problem2_transition_matrix[current_page, :]\n",
    "\n",
    "# -----------------------------\n",
    "# Find the most likely next pages\n",
    "# -----------------------------\n",
    "# np.argsort returns indices sorted in ascending order,\n",
    "# so we reverse it to get descending order of probabilities\n",
    "sorted_pages = np.argsort(p_next)[::-1]\n",
    "\n",
    "top1_page = sorted_pages[0]      # most likely next page\n",
    "top2_pages = sorted_pages[:2]    # two most likely next pages\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Preloading ONLY the most likely page (top-1)\n",
    "# -----------------------------\n",
    "\n",
    "# Sample next pages for 10,000 users\n",
    "next_pages_top = np.random.choice(\n",
    "    problem2_n_states,\n",
    "    size=n_users,\n",
    "    p=p_next\n",
    ")\n",
    "\n",
    "# Check which users get a preloaded page\n",
    "is_preloaded_top = (next_pages_top == top1_page)\n",
    "\n",
    "# Draw load times\n",
    "# - Not preloaded: Exp(1)  -> mean 1.0\n",
    "# - Preloaded:     Exp(10) -> mean 0.1\n",
    "load_times_not_preloaded = np.random.exponential(scale=1.0, size=n_users)\n",
    "load_times_preloaded = np.random.exponential(scale=0.1, size=n_users)\n",
    "\n",
    "# Assign load times based on whether the page was preloaded\n",
    "problem2_page_load_times_top = np.where(\n",
    "    is_preloaded_top,\n",
    "    load_times_preloaded,\n",
    "    load_times_not_preloaded\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Preloading the TWO most likely pages (top-2)\n",
    "# -----------------------------\n",
    "\n",
    "# Sample next pages again for a new experiment\n",
    "next_pages_two = np.random.choice(\n",
    "    problem2_n_states,\n",
    "    size=n_users,\n",
    "    p=p_next\n",
    ")\n",
    "\n",
    "# A page is preloaded if it is one of the two most likely pages\n",
    "is_preloaded_two = np.isin(next_pages_two, top2_pages)\n",
    "\n",
    "# Draw new load times\n",
    "load_times_not_preloaded = np.random.exponential(scale=1.0, size=n_users)\n",
    "load_times_preloaded = np.random.exponential(scale=0.1, size=n_users)\n",
    "\n",
    "# Assign load times\n",
    "problem2_page_load_times_two = np.where(\n",
    "    is_preloaded_two,\n",
    "    load_times_preloaded,\n",
    "    load_times_not_preloaded\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Optional sanity-check outputs\n",
    "# -----------------------------\n",
    "print(\"Top-1 preloaded page:\", top1_page)\n",
    "print(\"Top-2 preloaded pages:\", top2_pages)\n",
    "print(\"Mean load time (top-1 preloaded):\", problem2_page_load_times_top.mean())\n",
    "print(\"Mean load time (top-2 preloaded):\", problem2_page_load_times_two.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47daf353",
   "metadata": {},
   "source": [
    "-----\n",
    "3. [3p] Compare the average (empirical) load time from part 2 with the theoretical one of no pre-loading. Does the load time improve, how did you come to this conclusion? (Explain in the free text field).\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149696af",
   "metadata": {},
   "source": [
    "## Comparison with no pre-loading\n",
    "\n",
    "If no page is preloaded, every page load time is distributed as\n",
    "$( \\text{Exp}(1) )$, which has mean $( \\mathbb{E}[T] = 1 )$.\n",
    "Thus, the theoretical expected load time without pre-loading is 1 second.\n",
    "\n",
    "From the simulation in Part 2, we estimate the average load time when\n",
    "pre-loading the most likely next page by the empirical mean of the\n",
    "simulated load times.\n",
    "\n",
    "We conclude that pre-loading improves the load time if the empirical\n",
    "average is smaller than the theoretical mean of 1. In our simulation,\n",
    "this is the case, showing that pre-loading the most likely next page\n",
    "reduces the expected page load time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b5eccd1",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the mena of problem2_page_load_times_top:  0.8040508536789033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Part 3: Compare empirical and theoretical average load times\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# If NO page is preloaded, every page load time follows Exp(1),\n",
    "# i.e. an exponential distribution with rate 1.\n",
    "# The expected value of an Exp(lambda) random variable is:\n",
    "#     E[T] = 1 / lambda\n",
    "# Here lambda = 1, so the theoretical expected load time is exactly 1 second.\n",
    "problem2_avg = 1.0\n",
    "\n",
    "\n",
    "# We now compare this theoretical average (no pre-loading)\n",
    "# with the EMPIRICAL average load time obtained in Part 2\n",
    "# when pre-loading the most likely next page.\n",
    "#\n",
    "# problem2_page_load_times_top contains 10,000 simulated load times.\n",
    "# Taking the mean gives an estimate of the expected load time\n",
    "# under pre-loading.\n",
    "#\n",
    "# If the theoretical mean (1.0) is larger than the empirical mean,\n",
    "# then pre-loading improves the expected load time.\n",
    "problem2_comparison = problem2_avg > np.mean(problem2_page_load_times_top)\n",
    "\n",
    "print(\"This is the mena of problem2_page_load_times_top: \", np.mean(problem2_page_load_times_top))\n",
    "\n",
    "# This boolean answers the question directly:\n",
    "# True  -> pre-loading improves load time\n",
    "# False -> no improvement observed\n",
    "problem2_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f007420",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put the explanation for **part 3** of how you made the decision about `problem2_comparison` below this line in this **cell**. In order to enter edit mode you can doubleclick this cell or select it and press enter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930fee2",
   "metadata": {},
   "source": [
    "-----\n",
    "4. [4p] Calculate the stationary distribution of the Markov chain and calculate the expected load time with respect to it. \n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ac1fb",
   "metadata": {},
   "source": [
    "## Stationary distribution and expected load time under stationarity\n",
    "\n",
    "A stationary distribution $\\pi$ for a Markov chain with transition matrix $P$ satisfies\n",
    "$$\n",
    "\\pi P = \\pi, \\qquad \\sum_i \\pi_i = 1, \\qquad \\pi_i \\ge 0.\n",
    "$$\n",
    "Equivalently, $\\pi^\\top$ is a (left) eigenvector of $P$ with eigenvalue $1$, i.e.\n",
    "$P^\\top \\pi^\\top = \\pi^\\top$. We compute the eigenvector of $P^\\top$ corresponding to\n",
    "eigenvalue $1$, take the real part (numerical eigenvectors can become complex), and normalize it\n",
    "to sum to $1$.\n",
    "\n",
    "To compute the expected load time under the stationary regime, we assume the current page $i$\n",
    "is distributed as $\\pi$. For each current page $i$, we preload the most likely next page\n",
    "$j^*(i) = \\arg\\max_j P_{ij}$. The next page is then preloaded with probability\n",
    "$P_{i, j^*(i)}$, giving mean load time $0.1$ seconds (Exp(10)), otherwise mean $1$\n",
    "second (Exp(1)). Therefore the conditional expected load time from page $i$ is\n",
    "$$\n",
    "\\mathbb{E}[T \\mid X_t=i]\n",
    "= 0.1 \\cdot P_{i,j^*(i)} + 1 \\cdot (1 - P_{i,j^*(i)})\n",
    "= 1 - 0.9 \\cdot P_{i,j^*(i)}.\n",
    "$$\n",
    "Finally, the stationary expected load time is\n",
    "$$\n",
    "\\mathbb{E}[T] = \\sum_i \\pi_i \\, \\mathbb{E}[T \\mid X_t=i].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bbfec65d",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stationary distribution: \n",
      " [0.08212278 0.12810291 0.09219365 0.10612032 0.0721337  0.10215028\n",
      " 0.11517528 0.09308925 0.08396098 0.12495085]\n",
      "Stationary expected loading time:  0.8018160947549082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 4.1: Stationary distribution via eigenvector of P^T\n",
    "# ------------------------------------------------------------\n",
    "# A stationary distribution pi satisfies:\n",
    "#   pi P = pi\n",
    "# Equivalently, pi^T is a right eigenvector of P^T with eigenvalue 1.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Transition matrix and number of states\n",
    "P = problem2_transition_matrix\n",
    "n = problem2_n_states\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of the transpose P^T\n",
    "# We use P^T because we want a LEFT eigenvector of P\n",
    "eigvals, eigvecs = np.linalg.eig(P.T)\n",
    "\n",
    "# Find the index of the eigenvalue that is closest to 1\n",
    "# (due to numerical precision, it may not be exactly 1)\n",
    "idx = np.argmin(np.abs(eigvals - 1.0))\n",
    "\n",
    "# Extract the corresponding eigenvector\n",
    "# Eigenvectors may be complex due to numerical reasons,\n",
    "# but the stationary distribution is real, so we take the real part\n",
    "pi = np.real(eigvecs[:, idx])\n",
    "\n",
    "# The eigenvector is only defined up to a multiplicative constant\n",
    "# (and sign), so if the sum is negative we flip the sign\n",
    "if pi.sum() < 0:\n",
    "    pi = -pi\n",
    "\n",
    "# Due to numerical noise, some entries may be very small negatives.\n",
    "# We clamp them to zero and normalize so the distribution sums to 1.\n",
    "pi = np.maximum(pi, 0.0)\n",
    "pi = pi / pi.sum()\n",
    "\n",
    "# Store the stationary distribution\n",
    "problem2_stationary_distribution = pi  # shape (n,)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Part 4.2: Expected load time under stationary distribution\n",
    "# ------------------------------------------------------------\n",
    "# We assume that from each current page i, we preload the single\n",
    "# most likely next page according to the transition probabilities.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# For each page i, find the index of the most likely next page\n",
    "top_next = np.argmax(P, axis=1)  # shape (n,)\n",
    "\n",
    "# For each page i, extract the probability that the next page\n",
    "# equals the preloaded (most likely) one\n",
    "p_preloaded = P[np.arange(n), top_next]  # shape (n,)\n",
    "\n",
    "# Expected load time starting from page i:\n",
    "# - With probability p_preloaded[i], the page is preloaded:\n",
    "#       load time ~ Exp(10) with mean 0.1\n",
    "# - With probability 1 - p_preloaded[i], it is not preloaded:\n",
    "#       load time ~ Exp(1) with mean 1.0\n",
    "#\n",
    "# Therefore:\n",
    "#   E[T | i] = 0.1 * p_preloaded[i] + 1.0 * (1 - p_preloaded[i])\n",
    "#            = 1 - 0.9 * p_preloaded[i]\n",
    "expected_time_given_i = 1.0 - 0.9 * p_preloaded  # shape (n,)\n",
    "\n",
    "# Expected load time under the stationary distribution:\n",
    "#   E[T] = sum_i pi_i * E[T | i]\n",
    "problem2_avg_stationary = float(problem2_stationary_distribution @ expected_time_given_i)\n",
    "\n",
    "# Optional prints for inspection\n",
    "print(\"Stationary distribution: \\n\", problem2_stationary_distribution)\n",
    "print(\"Stationary expected loading time: \", problem2_avg_stationary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beccb8",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "---\n",
    "## Exam vB, PROBLEM 3\n",
    "Maximum Points = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21deee81",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "3. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826a7f8",
   "metadata": {},
   "source": [
    "## Explanation of variables produced by `train_test_validation`\n",
    "\n",
    "The function `train_test_validation(X, Y, shuffle=True, random_state=1)` splits the dataset into **three disjoint parts**: training, validation, and test data. Each part contains feature vectors and their corresponding labels.\n",
    "\n",
    "### Training set\n",
    "- **`PROBLEM3_X_train`**  \n",
    "  Feature matrix for the **training set**.  \n",
    "  Shape: $(n_{\\text{train}}, d)$, where $d$ is the number of features.  \n",
    "  Used to **fit (train)** the logistic regression model.\n",
    "\n",
    "- **`PROBLEM3_y_train`**  \n",
    "  Label vector for the **training set**.  \n",
    "  Shape: $(n_{\\text{train}},)$.  \n",
    "  Contains the true class labels used during training.\n",
    "\n",
    "### Validation set\n",
    "- **`PROBLEM3_X_val`**  \n",
    "  Feature matrix for the **validation set**.  \n",
    "  Shape: $(n_{\\text{val}}, d)$.  \n",
    "  Used for **model selection**, in particular for choosing the decision threshold.\n",
    "\n",
    "- **`PROBLEM3_y_val`**  \n",
    "  Label vector for the **validation set**.  \n",
    "  Shape: $(n_{\\text{val}},)$.  \n",
    "  Used to evaluate cost, precision, recall, and 0–1 loss when selecting thresholds.\n",
    "\n",
    "- **`PROBLEM3_y_true_val`**  \n",
    "  Same as `PROBLEM3_y_val`, stored under a clearer name to emphasize that these are the **true labels** for the validation set.\n",
    "\n",
    "- **`PROBLEM3_y_pred_proba_val`**  \n",
    "  Predicted probabilities $P(Y=1 \\mid X)$ for the **validation set**, produced by the trained logistic regression model.  \n",
    "  Used to:\n",
    "  - compute cost as a function of the threshold,\n",
    "  - find the optimal threshold,\n",
    "  - compute precision and recall on validation data.\n",
    "\n",
    "### Test set\n",
    "- **`PROBLEM3_X_test`**  \n",
    "  Feature matrix for the **test set**.  \n",
    "  Shape: $(n_{\\text{test}}, d)$.  \n",
    "  Used **only for final evaluation**, after all thresholds and decisions are fixed.\n",
    "\n",
    "- **`PROBLEM3_y_test`**  \n",
    "  Label vector for the **test set**.  \n",
    "  Shape: $(n_{\\text{test}},)$.  \n",
    "  Contains the true class labels for final evaluation.\n",
    "\n",
    "- **`PROBLEM3_y_true_test`**  \n",
    "  Same as `PROBLEM3_y_test`, emphasizing that these are the **true labels** for the test set.\n",
    "\n",
    "- **`PROBLEM3_y_pred_proba_test`**  \n",
    "  Predicted probabilities $P(Y=1 \\mid X)$ for the **test set**.  \n",
    "  Used to compute the final cost and construct a confidence interval.\n",
    "\n",
    "### Summary\n",
    "- **Training set**: used to learn model parameters  \n",
    "- **Validation set**: used to choose thresholds and compare decision rules  \n",
    "- **Test set**: used to estimate final performance and uncertainty  \n",
    "\n",
    "This separation prevents information leakage and ensures an unbiased evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3f1aadd3",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [],
   "source": [
    "\n",
    "# RUN THIS CELL TO GET THE DATA\n",
    "\n",
    "# We start by loading the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROBLEM3_DF = pd.read_csv('data/fraud.csv')\n",
    "Y = PROBLEM3_DF['Class'].values\n",
    "X = PROBLEM3_DF[['V%d' % i for i in range(1,5)]+['Amount']].values\n",
    "\n",
    "# We will split the data into training, testing and validation sets\n",
    "from Utils import train_test_validation\n",
    "PROBLEM3_X_train, PROBLEM3_X_test, PROBLEM3_X_val, PROBLEM3_y_train, PROBLEM3_y_test, PROBLEM3_y_val = train_test_validation(X,Y,shuffle=True,random_state=1)\n",
    "\n",
    "# From this we will train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(PROBLEM3_X_train,PROBLEM3_y_train)\n",
    "\n",
    "# THE FOLLOWING CODE WILL PRODUCE THE ARRAYS YOU NEED FOR THE PROBLEM\n",
    "\n",
    "PROBLEM3_y_pred_proba_val = lr.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "PROBLEM3_y_true_val = PROBLEM3_y_val\n",
    "\n",
    "PROBLEM3_y_pred_proba_test = lr.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "PROBLEM3_y_true_test = PROBLEM3_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80162f",
   "metadata": {},
   "source": [
    "\n",
    "In this problem we are interested in fraud detection in an e-commerce system. In this problem we are given the outputs of a classifier that predicts the probabilities of fraud, your goal is to explore the threshold choice as in individual assignment 4. The costs associated with the predictions are:\n",
    "\n",
    "* **True Positive (TP)**: Detecting fraud and blocking the transaction costs the company 100 (manual review etc.)\n",
    "* **True Negative (TN)**: Allowing a legitimate transaction has no cost.\n",
    "* **False Positive (FP)**: Incorrectly classifying a legitimate transaction as fraudulent costs 120 (customer dissatisfaction plus operational expenses for reversing the decision).\n",
    "* **False Negative (FN)**: Missing a fraudulent transaction costs the company 600 (e.g., fraud loss plus potential reputational damage or penalties).\n",
    "\n",
    "**The code cells contain more detailed instructions, THE FIRST CODE CELL INITIALIZES YOUR VARIABLES**\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c588e528",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 1: 3 points\n",
    "\n",
    "# Implement the following function that calculates the cost of a binary classifier\n",
    "# according to the specification in the problem statement.\n",
    "# The function evaluates how expensive a given threshold choice is.\n",
    "def cost(y_true, y_predict_proba, threshold):\n",
    "    # y_true: numpy array of shape (n_samples,)\n",
    "    #         Contains the true binary labels (0 = legitimate, 1 = fraud)\n",
    "    #\n",
    "    # y_predict_proba: numpy array of shape (n_samples,)\n",
    "    #                  Contains predicted probabilities of fraud\n",
    "    #\n",
    "    # threshold: float in [0, 1]\n",
    "    #            Probabilities >= threshold are classified as fraud (1),\n",
    "    #            otherwise as legitimate (0)\n",
    "\n",
    "    # Convert predicted probabilities into binary predictions\n",
    "    # Fraud (1) if probability >= threshold, otherwise legitimate (0)\n",
    "    y_pred = (y_predict_proba >= threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    # Compute confusion matrix components by counting outcomes\n",
    "    # True Positives: fraud correctly detected\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    \n",
    "    # True Negatives: legitimate transactions correctly allowed\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    # False Positives: legitimate transactions incorrectly blocked\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    # False Negatives: fraud that was missed\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Compute the total cost according to the problem specification\n",
    "    # TP  -> cost 100\n",
    "    # TN  -> cost 0\n",
    "    # FP  -> cost 120\n",
    "    # FN  -> cost 600\n",
    "    total_cost = (\n",
    "        100 * TP +     # Cost for detecting fraud (manual review, etc.)\n",
    "        0   * TN +     # No cost for correct legitimate transactions\n",
    "        120 * FP +     # Cost for wrongly blocking legitimate users\n",
    "        600 * FN       # High cost for missed fraud\n",
    "    )\n",
    "    \n",
    "    # Compute the average cost per sample\n",
    "    # This makes the cost comparable across datasets of different sizes\n",
    "    avg_cost = total_cost / len(y_true)\n",
    "    \n",
    "    return avg_cost\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot the cost as a function of the threshold\n",
    "# using validation data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Create thresholds from 0 to 1 (inclusive) with step size 0.01\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Compute the average cost for each threshold value\n",
    "# using the validation labels and predicted probabilities\n",
    "costs = [\n",
    "    cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, t)\n",
    "    for t in thresholds\n",
    "]\n",
    "\n",
    "# Plot cost vs threshold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, costs)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Average cost\")\n",
    "plt.title(\"Cost as a function of threshold (validation data)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322bd09",
   "metadata": {},
   "source": [
    "-----\n",
    "2. [2.5p] Find the threshold that minimizes the cost and calculate the cost at that threshold on the validation data. Also calculate the precision and recall at the optimal threshold on the validation data on class 1 and 0.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "25387d22",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold (min cost): 0.2\n",
      "Validation cost at optimal threshold: 43.15492957746479\n",
      "Class 1 (fraud) precision: 0.7961165048543689\n",
      "Class 1 (fraud) recall: 0.9144981412639405\n",
      "Class 0 (legitimate) precision: 0.9695767195767195\n",
      "Class 0 (legitimate) recall: 0.9208542713567839\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Part 2: Find best threshold (min cost) on validation set\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Define candidate thresholds from 0 to 1 with step size 0.01\n",
    "# These are the thresholds we will evaluate\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# 2) Compute the average cost for each threshold\n",
    "# Uses the cost() function implemented in Part 1\n",
    "costs = np.array([\n",
    "    cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, t)\n",
    "    for t in thresholds\n",
    "])\n",
    "\n",
    "# 3) Find the index of the threshold that minimizes the cost\n",
    "min_idx = np.argmin(costs)\n",
    "\n",
    "# Extract the optimal threshold as a float\n",
    "problem3_threshold = float(thresholds[min_idx])  # Value in [0, 1]\n",
    "\n",
    "# 4) Compute the cost at the optimal threshold on the validation data\n",
    "problem3_cost_val = float(\n",
    "    cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, problem3_threshold)\n",
    ")\n",
    "\n",
    "# 5) Generate binary predictions on the validation set\n",
    "# using the optimal threshold\n",
    "problem3_y_pred_val = (PROBLEM3_y_pred_proba_val >= problem3_threshold).astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Precision & Recall for class 1 and class 0\n",
    "# Computed explicitly from confusion-matrix counts\n",
    "# ----------------------------\n",
    "\n",
    "# Compute confusion matrix components\n",
    "TP = np.sum((PROBLEM3_y_true_val == 1) & (problem3_y_pred_val == 1))  # Fraud correctly detected\n",
    "TN = np.sum((PROBLEM3_y_true_val == 0) & (problem3_y_pred_val == 0))  # Legit correctly allowed\n",
    "FP = np.sum((PROBLEM3_y_true_val == 0) & (problem3_y_pred_val == 1))  # Legit incorrectly blocked\n",
    "FN = np.sum((PROBLEM3_y_true_val == 1) & (problem3_y_pred_val == 0))  # Fraud missed\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics for class 1 (fraud)\n",
    "# ----------------------------\n",
    "\n",
    "# Precision: fraction of predicted fraud that is actually fraud\n",
    "problem3_precision_1 = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "# Recall: fraction of actual fraud that is detected\n",
    "problem3_recall_1 = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics for class 0 (legitimate)\n",
    "# ----------------------------\n",
    "\n",
    "# Precision: fraction of predicted legitimate that is actually legitimate\n",
    "problem3_precision_0 = TN / (TN + FN) if (TN + FN) > 0 else 0.0\n",
    "\n",
    "# Recall: fraction of actual legitimate that is correctly allowed\n",
    "problem3_recall_0 = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Print results\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Optimal threshold (min cost):\", problem3_threshold)\n",
    "print(\"Validation cost at optimal threshold:\", problem3_cost_val)\n",
    "\n",
    "print(\"Class 1 (fraud) precision:\", problem3_precision_1)\n",
    "print(\"Class 1 (fraud) recall:\", problem3_recall_1)\n",
    "\n",
    "print(\"Class 0 (legitimate) precision:\", problem3_precision_0)\n",
    "print(\"Class 0 (legitimate) recall:\", problem3_recall_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde750e",
   "metadata": {},
   "source": [
    "-----\n",
    "3. [2.5p] Repeat step 2, but this time find the best threshold to minimize the $0-1$ loss. Calculate the difference in cost between the threshold found in part 2 with the one just found in part 3.\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0362a128",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold (0–1 loss): 0.6900000000000001\n",
      "0–1 loss at optimal threshold: 0.0647887323943662\n",
      "Cost at Part 2 (cost-optimal) threshold: 43.15492957746479\n",
      "Cost at Part 3 (0–1 loss-optimal) threshold: 54.08450704225352\n",
      "Difference in cost (Part 3 − Part 2): 10.929577464788728\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Part 3: Threshold minimizing 0–1 loss (validation set)\n",
    "# ----------------------------\n",
    "\n",
    "# 1) Define candidate thresholds from 0 to 1 with step size 0.01\n",
    "# These thresholds will be evaluated using the 0–1 loss criterion\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# 2) Compute the 0–1 loss for each threshold\n",
    "# 0–1 loss is defined as the fraction of incorrect predictions:\n",
    "#\n",
    "#   0–1 loss = (1 / n) * sum( I(y_pred != y_true) )\n",
    "#\n",
    "# where I(.) is an indicator that equals 1 if the condition is true\n",
    "# and 0 otherwise.\n",
    "#\n",
    "# In NumPy, the expression (y_pred != y_true) produces a boolean array:\n",
    "#   - True  -> incorrect prediction\n",
    "#   - False -> correct prediction\n",
    "#\n",
    "# When taking the mean, True is treated as 1 and False as 0,\n",
    "# so np.mean(y_pred != y_true) directly computes the fraction\n",
    "# of misclassified samples.\n",
    "losses_01 = []\n",
    "\n",
    "for t in thresholds:\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    # Fraud (1) if probability >= threshold, otherwise legitimate (0)\n",
    "    y_pred = (PROBLEM3_y_pred_proba_val >= t).astype(int)\n",
    "    \n",
    "    # Compute 0–1 loss as the average number of incorrect predictions\n",
    "    # Using \"!=\" counts mistakes, which is exactly what 0–1 loss measures\n",
    "    loss = np.mean(y_pred != PROBLEM3_y_true_val)\n",
    "    \n",
    "    losses_01.append(loss)\n",
    "\n",
    "# Convert list to NumPy array for easier indexing\n",
    "losses_01 = np.array(losses_01)\n",
    "\n",
    "# 3) Find the threshold that minimizes the 0–1 loss\n",
    "# If multiple thresholds give the same minimum loss,\n",
    "# np.argmin selects the first one\n",
    "min_idx_01 = np.argmin(losses_01)\n",
    "\n",
    "# Extract the optimal threshold as a float\n",
    "problem3_threshold_01 = float(thresholds[min_idx_01])  # Value in [0, 1]\n",
    "\n",
    "# ----------------------------\n",
    "# Cost difference between Part 2 threshold and 0–1 loss threshold\n",
    "# ----------------------------\n",
    "\n",
    "# Cost at the threshold found in Part 2 (cost-minimizing threshold)\n",
    "# Computed on the validation set\n",
    "cost_part2 = cost(\n",
    "    PROBLEM3_y_true_val,\n",
    "    PROBLEM3_y_pred_proba_val,\n",
    "    problem3_threshold\n",
    ")\n",
    "\n",
    "# Cost at the threshold that minimizes 0–1 loss\n",
    "# Also computed on the validation set\n",
    "cost_part3 = cost(\n",
    "    PROBLEM3_y_true_val,\n",
    "    PROBLEM3_y_pred_proba_val,\n",
    "    problem3_threshold_01\n",
    ")\n",
    "\n",
    "# Difference in cost between the two thresholds\n",
    "# This preserves the sign:\n",
    "#   > 0  -> 0–1 loss threshold gives higher cost\n",
    "#   < 0  -> 0–1 loss threshold gives lower cost\n",
    "problem3_cost_difference = float(cost_part3 - cost_part2)\n",
    "\n",
    "# ----------------------------\n",
    "# Print results\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Optimal threshold (0–1 loss):\", problem3_threshold_01)\n",
    "print(\"0–1 loss at optimal threshold:\", losses_01[min_idx_01])\n",
    "\n",
    "print(\"Cost at Part 2 (cost-optimal) threshold:\", cost_part2)\n",
    "print(\"Cost at Part 3 (0–1 loss-optimal) threshold:\", cost_part3)\n",
    "\n",
    "print(\"Difference in cost (Part 3 − Part 2):\", problem3_cost_difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da759a5",
   "metadata": {},
   "source": [
    "-----\n",
    "4. [4p] Provide a confidence interval around the optimal cost (with $95\\%$ confidence) applied to the test data and explain all the assumption you made.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de1a7b",
   "metadata": {},
   "source": [
    "## Part 4: 95% confidence interval for the cost using Hoeffding's inequality\n",
    "\n",
    "We want a $95\\%$ confidence interval for the *expected* (average) cost of the classifier on the test distribution.\n",
    "Let $C_i$ be the random cost for test sample $i$ when we use the fixed threshold $problem3\\_threshold$.\n",
    "We estimate the expected cost by the empirical mean on the test set:\n",
    "$$\n",
    "\\hat{C} = \\frac{1}{n}\\sum_{i=1}^n C_i,\n",
    "$$\n",
    "where $n$ is the number of test samples.\n",
    "\n",
    "We use Hoeffding's inequality, which applies under the assumptions that:\n",
    "\n",
    "1. The test samples (and therefore $C_1,\\dots,C_n$) are independent and identically distributed (i.i.d.).\n",
    "2. The per-sample cost is bounded in an interval $[a,b]$.\n",
    "   In this problem the smallest possible cost is $a=0$ (true negative) and the largest possible cost is $b=600$ (false negative).\n",
    "\n",
    "Hoeffding's inequality states that for any $\\epsilon>0$,\n",
    "$$\n",
    "P(|\\hat{C} - \\mathbb{E}[C]| \\ge \\epsilon) \\le 2\\exp\\left(\\frac{-2n\\epsilon^2}{(b-a)^2}\\right).\n",
    "$$\n",
    "Setting the right-hand side to $\\alpha=0.05$ (for $95\\%$ confidence) and solving for $\\epsilon$ gives\n",
    "$$\n",
    "\\epsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\alpha)}{2n}}.\n",
    "$$\n",
    "Thus a $95\\%$ confidence interval for the expected cost is\n",
    "$$\n",
    "[\\hat{C}-\\epsilon,\\ \\hat{C}+\\epsilon],\n",
    "$$\n",
    "where $\\hat{C}$ is computed from the test data using $problem3\\_threshold$ and $b-a=600$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eb3d73e4",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cost on test data: 42.70422535211268\n",
      "95% confidence interval for expected cost:\n",
      "[ 17.734792363077773 , 67.67365834114759 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# Part 4: 95% confidence interval using Hoeffding's inequality\n",
    "# ----------------------------\n",
    "\n",
    "# Compute the average cost on the test data\n",
    "# using the optimal threshold found on the validation set (Part 2)\n",
    "#\n",
    "# This is the empirical estimate of the expected cost\n",
    "mean_cost_test = cost(\n",
    "    PROBLEM3_y_true_test,\n",
    "    PROBLEM3_y_pred_proba_test,\n",
    "    problem3_threshold\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Parameters for Hoeffding's inequality\n",
    "# ----------------------------\n",
    "\n",
    "# Significance level alpha = 0.05 corresponds to 95% confidence\n",
    "alpha = 0.05\n",
    "\n",
    "# Number of independent test samples\n",
    "n = len(PROBLEM3_y_true_test)\n",
    "\n",
    "# ----------------------------\n",
    "# Cost bounds\n",
    "# ----------------------------\n",
    "\n",
    "# The per-sample cost is bounded:\n",
    "#   Minimum cost = 0   (True Negative)\n",
    "#   Maximum cost = 600 (False Negative)\n",
    "#\n",
    "# Thus, b - a = 600\n",
    "cost_range = 600\n",
    "\n",
    "# ----------------------------\n",
    "# Hoeffding bound\n",
    "# ----------------------------\n",
    "\n",
    "# Hoeffding's inequality gives:\n",
    "#   P(|mean_cost_test - E[C]| >= epsilon) <= 2 * exp(-2n epsilon^2 / (b - a)^2)\n",
    "#\n",
    "# Solving for epsilon with confidence level (1 - alpha) gives:\n",
    "#   epsilon = (b - a) * sqrt( ln(2 / alpha) / (2n) )\n",
    "epsilon = np.sqrt(np.log(2 / alpha) / (2 * n)) * cost_range\n",
    "\n",
    "# ----------------------------\n",
    "# Confidence interval\n",
    "# ----------------------------\n",
    "\n",
    "# 95% confidence interval for the expected cost\n",
    "problem3_lower_bound = mean_cost_test - epsilon\n",
    "problem3_upper_bound = mean_cost_test + epsilon\n",
    "\n",
    "# ----------------------------\n",
    "# Print results\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Mean cost on test data:\", mean_cost_test)\n",
    "print(\"95% confidence interval for expected cost:\")\n",
    "print(\"[\", problem3_lower_bound, \",\", problem3_upper_bound, \"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67721024",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "vB",
    "lx_assignment_type": "EXAM",
    "lx_assignment_type2print": "Exam",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "12"
   },
   "source": [
    "\n",
    "## Free text answer\n",
    "\n",
    "Put your explanation for part 4 below this line in this **cell**. Doubleclick to enter edit mode as before.\n",
    "\n",
    "\n",
    "I assume that all variables are sub-Gaussian, to be able to use Hoeffdinger's inequality. \n",
    "I furthermore assume that the variables X are IID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "lx_assignment_number": "vB",
  "lx_course_instance": "2024",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
