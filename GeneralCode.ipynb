{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b5f837",
   "metadata": {},
   "source": [
    "## Here is an example of how to check reverisibility of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P = matrix\n",
    "# pi = linear equation system we have \n",
    "\n",
    "# Transition matrix\n",
    "P = np.array([\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Downtown\n",
    "    [0.2, 0.5, 0.3, 0.0],  # Suburbs\n",
    "    [0.0, 0.0, 0.5, 0.5],  # Countryside\n",
    "    [0.0, 0.0, 0.0, 1.0]   # Workshop\n",
    "])\n",
    "\n",
    "\n",
    "stationary_dist = stationary_distribution(P)\n",
    "\n",
    "\n",
    "# Code for checking reversibility for ANY matrix!:\n",
    "\n",
    "def is_reversible(P, stationary_dist, tol=1e-12):\n",
    "    n = P.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            left = stationary_dist[i] * P[i,j]\n",
    "            right = stationary_dist[j] * P[j,i]\n",
    "            if not np.isclose(left, right, atol=tol):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(\"Reversible:\", is_reversible(P, stationary_dist))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eb88f",
   "metadata": {},
   "source": [
    "## This is how you can always find the Stationary distribution of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64913b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution(P, tol=1e-12, verify=True):\n",
    "    \"\"\"\n",
    "    Compute the stationary distribution of a finite-state Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix. Rows should sum to 1 (row-stochastic).\n",
    "    tol : float, optional\n",
    "        Tolerance used for cleaning small numerical noise.\n",
    "    verify : bool, optional\n",
    "        If True, checks that the result is approximately stationary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.ndarray, shape (n,)\n",
    "        Stationary distribution vector (non-negative, sums to 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system:\n",
    "\n",
    "        (P^T - I) * pi = 0,  with  sum(pi) = 1\n",
    "\n",
    "    by replacing one of the equations with the normalization condition.\n",
    "    It assumes that a (unique) stationary distribution exists.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError(\"P must be a square matrix.\")\n",
    "\n",
    "    # Build A * pi = b\n",
    "    A = P.T - np.eye(n)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    # Replace last row with normalization condition: sum_i pi_i = 1\n",
    "    A[-1, :] = 1.0\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    # Clean tiny numerical noise\n",
    "    pi[np.abs(pi) < tol] = 0.0\n",
    "\n",
    "    # If there are small negative values, clamp them to 0 and renormalize\n",
    "    if np.any(pi < -tol):\n",
    "        # Serious negativity -> indicate potential problem\n",
    "        raise RuntimeError(\n",
    "            \"Computed stationary distribution has significantly negative entries. \"\n",
    "            \"Check that P is a valid transition matrix with a unique stationary distribution.\"\n",
    "        )\n",
    "\n",
    "    # Clamp small negatives and renormalize\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        raise RuntimeError(\"Failed to compute a valid stationary distribution (sum <= 0).\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify:\n",
    "        # Check stationarity: pi P ≈ pi\n",
    "        if not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "            raise RuntimeError(\"Result does not satisfy pi P ≈ pi. Check the input matrix P.\")\n",
    "\n",
    "    return pi\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0aa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# This stationary distribution fiunction could also work, just not always the same and correct way:\n",
    "def stationary_distribution(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution of a Markov chain\n",
    "    by finding the eigenvector corresponding to eigenvalue 1.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    \n",
    "    # Find the eigenvector associated with eigenvalue 1\n",
    "    idx = np.argmin(np.abs(eigenvalues - 1))\n",
    "    vec = np.real(eigenvectors[:, idx])\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    stationary = vec / np.sum(vec)\n",
    "    return stationary\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085bfc2",
   "metadata": {},
   "source": [
    "### Always use this stationary distribution function below to check, it WILL always work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2878ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [ 0. -0. -0.  1.]\n"
     ]
    }
   ],
   "source": [
    "def stationary_distribution_always_works(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution by solving\n",
    "    (P^T - I) * pi = 0  with  sum(pi) = 1.\n",
    "    This method ALWAYS works for any Markov chain with a stationary distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Build system: (P^T - I) * pi = 0\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Replace last equation with the normalization condition sum(pi)=1\n",
    "    A[-1] = np.ones(n)\n",
    "\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve the linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4df2f",
   "metadata": {},
   "source": [
    "### This is another stationary distribution function which the bot said will actually always for for any chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb10f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution_any_markov(P, tol=1e-12, cleanup_tol=1e-15, verify=True):\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "    if P.ndim != 2 or n != P.shape[1]:\n",
    "        raise ValueError(\"P must be square.\")\n",
    "\n",
    "    # Validate Markov matrix\n",
    "    if np.any(P < -cleanup_tol):\n",
    "        raise ValueError(\"P has negative entries.\")\n",
    "    if not np.allclose(P.sum(axis=1), 1.0, atol=1e-12):\n",
    "        raise ValueError(\"Rows of P must sum to 1.\")\n",
    "\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Add normalization as an extra equation (least squares)\n",
    "    A_aug = np.vstack([A, np.ones((1, n))])\n",
    "    b_aug = np.zeros(n + 1)\n",
    "    b_aug[-1] = 1.0\n",
    "\n",
    "    # Least-squares solution (works even if A is singular)\n",
    "    pi, *_ = np.linalg.lstsq(A_aug, b_aug, rcond=None)\n",
    "\n",
    "    # Cleanup / project to simplex\n",
    "    pi[np.abs(pi) < cleanup_tol] = 0.0\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        raise RuntimeError(\"Could not recover a valid distribution.\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify and not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "        # If periodic/reducible, this should still pass; if not, input likely invalid/ill-conditioned\n",
    "        raise RuntimeError(\"Result does not satisfy pi P ≈ pi (within tolerance).\")\n",
    "\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7b41",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6437185",
   "metadata": {},
   "source": [
    "### Expected hitting time function\n",
    "\n",
    "This function computes expected hitting times to a given **target set of states** in a finite Markov chain with transition matrix \\( P \\).\n",
    "\n",
    "We consider a Markov chain with state space \\( \\{0, 1, \\dots, n-1\\} \\) and transition matrix\n",
    "\n",
    "$$\n",
    "P = (P_{ij})_{i,j=0}^{n-1},\n",
    "$$\n",
    "\n",
    "where \\( P_{ij} = \\mathbb{P}(X_{t+1} = j \\mid X_t = i) \\).\n",
    "\n",
    "Given a set of **target states** \\( T \\subset \\{0, \\dots, n-1\\} \\), the *hitting time* of \\( T \\) is\n",
    "\n",
    "$$\n",
    "T_{\\text{hit}} = \\min\\{ t \\ge 0 : X_t \\in T \\}.\n",
    "$$\n",
    "\n",
    "For each state \\( i \\), we define the expected hitting time\n",
    "\n",
    "$$\n",
    "h(i) = \\mathbb{E}[T_{\\text{hit}} \\mid X_0 = i].\n",
    "$$\n",
    "\n",
    "These satisfy\n",
    "\n",
    "$$\n",
    "h(i) = 0 \\quad \\text{for } i \\in T,\n",
    "$$\n",
    "\n",
    "and for \\( i \\notin T \\),\n",
    "\n",
    "$$\n",
    "h(i) = 1 + \\sum_{j=0}^{n-1} P_{ij} h(j).\n",
    "$$\n",
    "\n",
    "If we collect the non-target states into a set \\( S = \\{0, \\dots, n-1\\} \\setminus T \\), and form the submatrix \\( Q \\) of \\( P \\) with rows and columns indexed by \\( S \\), then the vector \\( h_S = (h(i))_{i \\in S} \\) solves\n",
    "\n",
    "$$\n",
    "(I - Q) h_S = \\mathbf{1},\n",
    "$$\n",
    "\n",
    "where \\( \\mathbf{1} \\) is a vector of ones.\n",
    "\n",
    "The function `expected_hitting_time` implements this:\n",
    "\n",
    "- **Parameters**\n",
    "  - `P`: `np.ndarray` of shape `(n, n)`  \n",
    "    Transition matrix of the Markov chain.\n",
    "  - `target_states`: iterable of integers  \n",
    "    Indices of the target states \\( T \\).\n",
    "  - `start_state` (optional): integer  \n",
    "    If provided, the function returns \\( h(\\text{start\\_state}) \\).\n",
    "  - `start_dist` (optional): 1D array-like of length `n`  \n",
    "    Initial distribution \\( \\alpha \\). If provided, the function returns\n",
    "    $$\n",
    "    \\mathbb{E}[T_{\\text{hit}}] = \\sum_{i=0}^{n-1} \\alpha_i h(i).\n",
    "    $$\n",
    "\n",
    "- **Return value**\n",
    "  - If `start_state` is given: a single float, \\( h(\\text{start\\_state}) \\).\n",
    "  - If `start_dist` is given: a single float, the expected hitting time under that initial distribution.\n",
    "  - If neither is given: a length-`n` NumPy array, containing \\( h(i) \\) for all states `i` (targets get value `0`).\n",
    "\n",
    "- **Usage example (this exam problem)**\n",
    "\n",
    "For the three-region chain\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "0.3 & 0.4 & 0.3 \\\\\\\\\n",
    "0.2 & 0.5 & 0.3 \\\\\\\\\n",
    "0.4 & 0.3 & 0.3\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "with downtown = state 0 and suburbs = state 1:\n",
    "\n",
    "```python\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # should be 50/13 ≈ 3.8461538\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9228617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def expected_hitting_time(P, target_states, start_state=None, start_dist=None):\n",
    "    \"\"\"\n",
    "    Compute expected hitting times to a given set of target states in a finite Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix of the Markov chain.\n",
    "    target_states : iterable of int\n",
    "        Indices of the target states.\n",
    "    start_state : int, optional\n",
    "        If provided, return the expected hitting time starting from this state.\n",
    "    start_dist : array-like, shape (n,), optional\n",
    "        If provided, return the expected hitting time under this initial distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or np.ndarray\n",
    "        - If start_state is given: expected hitting time from that state.\n",
    "        - If start_dist is given: expected hitting time under that distribution.\n",
    "        - If neither is given: array h of length n with expected hitting times\n",
    "          from all states (targets have value 0).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system\n",
    "\n",
    "        (I - Q) h_S = 1\n",
    "\n",
    "    where Q is the submatrix of P restricted to non-target states,\n",
    "    and 1 is a vector of ones. Assumes that the target set is hit\n",
    "    with probability 1 from the relevant starting states.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    target_states = np.array(sorted(set(target_states)), dtype=int)\n",
    "    all_states = np.arange(n, dtype=int)\n",
    "\n",
    "    # Non-target states S\n",
    "    non_target_states = np.array([s for s in all_states if s not in target_states], dtype=int)\n",
    "\n",
    "    # If all states are targets, hitting time is identically zero\n",
    "    if non_target_states.size == 0:\n",
    "        h = np.zeros(n, dtype=float)\n",
    "        if start_state is not None:\n",
    "            return float(h[start_state])\n",
    "        if start_dist is not None:\n",
    "            start_dist = np.asarray(start_dist, dtype=float)\n",
    "            return float(start_dist @ h)\n",
    "        return h\n",
    "\n",
    "    # Build Q and solve (I - Q) h_S = 1\n",
    "    Q = P[np.ix_(non_target_states, non_target_states)]\n",
    "    I = np.eye(Q.shape[0])\n",
    "    ones = np.ones(Q.shape[0])\n",
    "\n",
    "    # Solve for h_S\n",
    "    h_S = np.linalg.solve(I - Q, ones)\n",
    "\n",
    "    # Put back into full vector h of length n\n",
    "    h = np.zeros(n, dtype=float)\n",
    "    h[target_states] = 0.0\n",
    "    for idx, s in enumerate(non_target_states):\n",
    "        h[s] = h_S[idx]\n",
    "\n",
    "    # Return according to user request\n",
    "    if (start_state is not None) and (start_dist is not None):\n",
    "        raise ValueError(\"Provide either start_state or start_dist, not both.\")\n",
    "\n",
    "    if start_state is not None:\n",
    "        return float(h[start_state])\n",
    "\n",
    "    if start_dist is not None:\n",
    "        start_dist = np.asarray(start_dist, dtype=float)\n",
    "        if start_dist.shape[0] != n:\n",
    "            raise ValueError(\"start_dist must have length equal to number of states.\")\n",
    "        return float(start_dist @ h)\n",
    "\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03d2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.846153846153846\n"
     ]
    }
   ],
   "source": [
    "# Example from above code: \n",
    "\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "# Expected steps until first time in Downtown (state 0) starting from Suburbs (state 1)\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # ~3.846153846153846 (50/13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dde39",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is how you can always find M in a finite interval for the Reject-Accept sampling algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_x(x):\n",
    "    return np.exp(x)          # target pdf\n",
    "\n",
    "def g_x(x):\n",
    "    return 1/np.log(2)        # uniform(0, ln 2) pdf\n",
    "\n",
    "# Remember to change the interval to your interval you have\n",
    "xs = np.linspace(0, np.log(2), 1000)\n",
    "\n",
    "ratio = f_x(xs) / g_x(xs)\n",
    "\n",
    "# This works since M is always maximum of f / g\n",
    "M_num = ratio.max()\n",
    "\n",
    "print(\"Answer: \", 2*np.log(2))\n",
    "\n",
    "print(\"Numeric M ≈\", M_num)   # should be close to 2*np.log(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24c8a7",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is the different Hoeffding intervals from the Lecture notes:\n",
    "\n",
    "## Summary of Concentration Inequalities from the Lecture Notes\n",
    "\n",
    "This cell summarizes the main inequalities used in Monte Carlo estimation and empirical distribution analysis:  \n",
    "- Hoeffding’s inequality for Monte Carlo means  \n",
    "- The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality  \n",
    "- Alternative concentration inequalities listed in the notes  \n",
    "For each formula we also show how to solve for $$\\varepsilon$$ when forming a confidence interval.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hoeffding’s Inequality for Monte Carlo Estimation\n",
    "\n",
    "Assume we estimate a mean using  \n",
    "$$\n",
    "\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i,\n",
    "$$  \n",
    "where the samples satisfy $Y_i \\in [a,b]$.\n",
    "\n",
    "The **two-sided Hoeffding bound** is:\n",
    "$$\n",
    "\\mathbb{P}\\left( \\left| \\overline{Y} - \\mathbb{E}[Y] \\right| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "$$\n",
    "2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Thus a $(1-\\delta)100\\%$ confidence interval is:\n",
    "$$\n",
    "\\left[\\, \\overline{Y} - \\varepsilon,\\; \\overline{Y} + \\varepsilon \\,\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dvoretzky–Kiefer–Wolfowitz (DKW) Inequality\n",
    "\n",
    "For empirical CDF $$F_n(x)$$ based on i.i.d. samples with true CDF $$F(x)$$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( \\sup_x |F_n(x) - F(x)| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp(-2n\\varepsilon^2).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "\n",
    "$$\n",
    "2 \\exp(-2n\\varepsilon^2) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Useful for constructing confidence bands:\n",
    "$$\n",
    "F_n(x) - \\varepsilon \\le F(x) \\le F_n(x) + \\varepsilon.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Other Alternatives Mentioned in the Lecture Notes\n",
    "\n",
    "### (a) Chebyshev’s Inequality\n",
    "Assuming finite variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le \\frac{\\sigma^2}{n\\varepsilon^2}.\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ by setting RHS = $\\delta$:\n",
    "$$\n",
    "\\varepsilon = \\sigma \\sqrt{\\frac{1}{n\\delta}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Central Limit Theorem (CLT) Approximation\n",
    "For large $n$:\n",
    "\n",
    "$$\n",
    "\\overline{Y} \\approx \\mathcal{N}\\!\\left(\\mathbb{E}[Y],\\, \\frac{\\sigma^2}{n}\\right).\n",
    "$$\n",
    "\n",
    "A $(1-\\delta)$ interval is:\n",
    "$$\n",
    "\\overline{Y} \\;\\pm\\; z_{1-\\delta/2}\\,\\frac{\\sigma}{\\sqrt{n}},\n",
    "$$\n",
    "where $z_{1-\\delta/2}$ is the standard normal quantile.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Bernstein (or Chernoff–Hoeffding) Inequality  \n",
    "Sometimes given in extended form when variance is known. In bounded case (same assumptions as Hoeffding):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le 2 \\exp\\!\\left( \n",
    "\\frac{-n\\varepsilon^2}{2\\sigma^2 + \\frac{2}{3}(b-a)\\varepsilon}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ requires numerical methods; not algebraic in closed form.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table of $$\\varepsilon$$ Solutions\n",
    "\n",
    "| Inequality | Bound | Solution for $$\\varepsilon$$ |\n",
    "|-----------|-------|-------------------------------|\n",
    "| Hoeffding | $2\\exp\\!\\left(-\\frac{2n\\varepsilon^2}{(b-a)^2}\\right) \\le \\delta$ | $\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| DKW | $2\\exp(-2n\\varepsilon^2) \\le \\delta$ | $\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| Chebyshev | $\\frac{\\sigma^2}{n\\varepsilon^2} \\le \\delta$ | $\\varepsilon = \\sigma\\sqrt{\\frac{1}{n\\delta}}$ |\n",
    "| CLT | approx | $$\\varepsilon = z_{1-\\delta/2}\\,\\sigma/\\sqrt{n}$$ |\n",
    "| Bernstein | not closed form | requires numerical solution |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03c909",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### General information that could be important:\n",
    "\n",
    "* .values and .to_numpy() both convert pandas DataFrames or Series into NumPy arrays; \n",
    "* .to_numpy() is the recommended modern approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003d142",
   "metadata": {},
   "source": [
    "-----\n",
    "### Permutation Importance\n",
    "\n",
    "Permutation importance is a model-agnostic method for measuring feature importance. It works by randomly permuting the values of a single feature in the test set and then measuring how much the model’s predictive performance decreases. If permuting a feature leads to a large drop in performance, the model relied heavily on that feature, and it is considered important.\n",
    "\n",
    "This method measures the **impact of each feature on the model’s predictive performance**, rather than relying on model-specific parameters. Because it only requires the ability to make predictions and evaluate them with a chosen metric, permutation importance is applicable to **any type of predictive model**, including linear models, tree-based models, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa904c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERAL TEMPLATE: Permutation Importance (works for ANY model)\n",
    "# ============================================================\n",
    "# What you MUST change depending on your setup:\n",
    "#   1) estimator      -> set this to your TRAINED model (or Pipeline)\n",
    "#   2) X_test, y_test -> set these to your TEST split\n",
    "#   3) feature_names  -> set these to your column names (list of strings)\n",
    "#   4) scoring        -> choose a metric appropriate for your task\n",
    "#\n",
    "# Notes:\n",
    "# - This works for any model as long as it has predict() (or predict_proba() for some scorers)\n",
    "# - If you used preprocessing (scaling, one-hot encoding, etc.), it's best to wrap it in a Pipeline\n",
    "#   and pass the Pipeline as the estimator to avoid mismatches.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --------------------------\n",
    "# 1) CHOOSE YOUR TRAINED MODEL\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If you trained a Pipeline (recommended): estimator = my_pipeline\n",
    "# - If you trained a plain model:           estimator = my_model\n",
    "#\n",
    "# Examples:\n",
    "# estimator = problem3_model                   # Pipeline: scaler + logistic regression\n",
    "# estimator = trained_random_forest_model      # e.g., RandomForestClassifier already fit\n",
    "# estimator = trained_svm_model                # e.g., SVC already fit\n",
    "estimator = problem3_model  # <-- CHANGE to your trained model / pipeline\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) PROVIDE TEST DATA\n",
    "# --------------------------\n",
    "# CHANGE THESE:\n",
    "# - X_test should be the test features (NumPy array or pandas DataFrame)\n",
    "# - y_test should be the test labels\n",
    "#\n",
    "# Examples:\n",
    "# X_test = problem3_X_test\n",
    "# y_test = problem3_y_test\n",
    "X_test = problem3_X_test   # <-- CHANGE if your variables are named differently\n",
    "y_test = problem3_y_test   # <-- CHANGE if your variables are named differently\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) PROVIDE FEATURE NAMES\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If X_test is a pandas DataFrame, you can do: feature_names = X_test.columns\n",
    "# - If X_test is a NumPy array, you must supply a list yourself (same order as columns in X_test)\n",
    "#\n",
    "# Examples:\n",
    "# feature_names = problem3_features\n",
    "# feature_names = list(X_test.columns)\n",
    "feature_names = problem3_features  # <-- CHANGE to your feature name list (correct order!)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) CHOOSE A SCORING METRIC\n",
    "# --------------------------\n",
    "# CHANGE THIS depending on your task:\n",
    "# Classification examples:\n",
    "#   scoring = \"accuracy\"            (simple, common)\n",
    "#   scoring = \"balanced_accuracy\"   (good if classes are imbalanced)\n",
    "#   scoring = \"f1\"                  (if you care about positive class quality)\n",
    "#   scoring = \"roc_auc\"             (needs probability or decision scores; many models support it)\n",
    "#\n",
    "# Regression examples:\n",
    "#   scoring = \"r2\"\n",
    "#   scoring = \"neg_mean_squared_error\"\n",
    "#   scoring = \"neg_mean_absolute_error\"\n",
    "#\n",
    "# Tip:\n",
    "# - If \"roc_auc\" fails, your estimator may not provide predict_proba/decision_function.\n",
    "scoring = \"accuracy\"  # <-- CHANGE if needed\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) RUN PERMUTATION IMPORTANCE\n",
    "# --------------------------\n",
    "perm = permutation_importance(\n",
    "    estimator=estimator,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    n_repeats=30,        # increase for more stable estimates (slower)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6) FORMAT RESULTS\n",
    "# --------------------------\n",
    "perm_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance Mean\": perm.importances_mean,\n",
    "    \"Importance Std\": perm.importances_std\n",
    "}).sort_values(by=\"Importance Mean\", ascending=False)\n",
    "\n",
    "print(\"Top features by permutation importance:\")\n",
    "print(perm_df.head(15))\n",
    "\n",
    "# --------------------------\n",
    "# 7) OPTIONAL: GET MOST IMPORTANT FEATURE\n",
    "# --------------------------\n",
    "most_important_feature = perm_df.iloc[0][\"Feature\"]\n",
    "print(\"\\nMost important feature (by permutation importance):\", most_important_feature)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 8) OPTIONAL: RESTRICT TO A SUBSET OF FEATURES\n",
    "# --------------------------\n",
    "# Example: if you want \"most important one-hot encoded feature\"\n",
    "# CHANGE the selection rule to match your one-hot naming scheme.\n",
    "# For your diabetes case (features starting with smoking_ or sex_):\n",
    "subset = [f for f in feature_names if str(f).startswith(\"smoking_\") or str(f).startswith(\"sex_\")]\n",
    "\n",
    "if len(subset) > 0:\n",
    "    perm_subset = perm_df[perm_df[\"Feature\"].isin(subset)].sort_values(by=\"Importance Mean\", ascending=False)\n",
    "    print(\"\\nPermutation importance for subset features:\")\n",
    "    print(perm_subset)\n",
    "\n",
    "    most_important_in_subset = perm_subset.iloc[0][\"Feature\"]\n",
    "    print(\"\\nMost important feature in subset:\", most_important_in_subset)\n",
    "else:\n",
    "    print(\"\\nSubset list is empty. Adjust the subset selection rule to match your feature names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f29983",
   "metadata": {},
   "source": [
    "### Remember:\n",
    "If the dataset does not have a header, then you need to use header=None in the pd.read_csv(\"data\", header=None). Otherwise we will miss one column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124bb2",
   "metadata": {},
   "source": [
    "-----\n",
    "### Using Utils.py file\n",
    "\n",
    "If the exam question requires the file Utils.py, then simply copy paste the file into the current folder I am in and then the code will be able to find the file Utils.py. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f693a4e",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is general information when to use which data set in the exam:\n",
    "\n",
    "## When to use each dataset and variable\n",
    "\n",
    "The data in this assignment is split into **training**, **validation**, and **test** sets. Each set has a specific role, and using them correctly is essential to avoid data leakage and to obtain an unbiased evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Training set: fit the model\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_train`\n",
    "- `PROBLEM3_y_train`\n",
    "\n",
    "**When to use**\n",
    "Use the training set **only to learn the model parameters**.\n",
    "\n",
    "**Typical operations**\n",
    "- Fit a model:\n",
    "\n",
    "```python\n",
    "    model.fit(PROBLEM3_X_train, PROBLEM3_y_train)\n",
    "```\n",
    "\n",
    "\n",
    "- Do **not** compute performance metrics or choose thresholds using training data.\n",
    "\n",
    "**Purpose**\n",
    "The training set teaches the model the relationship between features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Validation set: model selection and threshold choice\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_val`\n",
    "- `PROBLEM3_y_val` (or `PROBLEM3_y_true_val`)\n",
    "- `PROBLEM3_y_pred_proba_val`\n",
    "\n",
    "**When to use**\n",
    "Use the validation set to **make decisions about the model**, such as:\n",
    "- choosing a classification threshold,\n",
    "- comparing different loss functions,\n",
    "- computing cost, precision, recall, and 0–1 loss.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_val = model.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "```\n",
    "\n",
    "- Convert probabilities to predictions:\n",
    "\n",
    "```python\n",
    "    y_pred_val = (y_pred_proba_val >= threshold).astype(int)\n",
    "```\n",
    "\n",
    "- Compute metrics:\n",
    "- cost\n",
    "- precision\n",
    "- recall\n",
    "- 0–1 loss\n",
    "\n",
    "**Purpose**\n",
    "The validation set is used to **tune decisions** without biasing the final evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Test set: final evaluation only\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_test`\n",
    "- `PROBLEM3_y_test` (or `PROBLEM3_y_true_test`)\n",
    "- `PROBLEM3_y_pred_proba_test`\n",
    "\n",
    "**When to use**\n",
    "Use the test set **only after**:\n",
    "- the model has been trained,\n",
    "- the threshold has been chosen using validation data.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_test = model.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "```\n",
    "\n",
    "- Evaluate final performance:\n",
    "- compute final cost,\n",
    "- build a confidence interval,\n",
    "- report final metrics.\n",
    "\n",
    "**Purpose**\n",
    "The test set provides an **unbiased estimate of real-world performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary table (conceptual)\n",
    "\n",
    "- Training set → **fit the model**\n",
    "- Validation set → **choose thresholds and compare decision rules**\n",
    "- Test set → **final evaluation and confidence intervals**\n",
    "\n",
    "---\n",
    "\n",
    "## Important rules to remember\n",
    "\n",
    "- Never choose thresholds using the test set.\n",
    "- Never report final performance using the validation set.\n",
    "- The test set must only be used **once**, at the very end.\n",
    "- Predicted probabilities (`predict_proba`) are used for **threshold-based decisions**.\n",
    "- Binary predictions (`>= threshold`) are used for **cost, precision, recall, and loss**.\n",
    "\n",
    "Following these rules ensures a correct and exam-safe machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cea85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1e8f93c",
   "metadata": {},
   "source": [
    "# General Guide: Which Dataset to Use, When, and Why (Logistic Regression & Classification)\n",
    "\n",
    "THIS RESPONSE IS **INTENTIONALLY AND EXCLUSIVELY** A SINGLE MARKDOWN TEXT CELL.  \n",
    "There is **NO TEXT OUTSIDE THIS BLOCK**.  \n",
    "You can copy **once** and paste directly into a Jupyter Notebook **Markdown cell**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Defining Features and Target\n",
    "\n",
    "### Features (`X`)\n",
    "- Features are the **input variables** used by the model to make predictions.\n",
    "- These typically include:\n",
    "  - Numerical variables (e.g. age, BMI, blood glucose)\n",
    "  - One-Hot encoded categorical variables (e.g. `sex_Male`, `smoking_former`)\n",
    "- **Rule**: Features must represent information that is available **before** a prediction is made.\n",
    "- **Rule**: Never include the target variable inside the feature set.\n",
    "\n",
    "### Target (`y`)\n",
    "- The target is what the model is trying to predict.\n",
    "- For classification:\n",
    "  - Binary variable (e.g. diabetes = 0 or 1)\n",
    "- **Rule**: The target must NEVER be included among the features.\n",
    "\n",
    "```python\n",
    "problem3_X = problem3_df[feature_columns].values\n",
    "problem3_y = problem3_df[target_column].values\n",
    "```\n",
    "\n",
    "## 2. Train–Test Split (Why and How)\n",
    "\n",
    "Training Dataset\n",
    "- Used to train (fit) the model.\n",
    "- The model learns patterns from this data.\n",
    "\n",
    "Test Dataset\n",
    "- Used to evaluate final performance.\n",
    "- Simulates unseen, real-world data.\n",
    "- Must NEVER be used during training.\n",
    "\n",
    "Standard Split\n",
    "- 80% training\n",
    "- 20% testing\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    problem3_X,\n",
    "    problem3_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## 3. Training the Model (.fit())\n",
    "Which Dataset Goes Into .fit()?\n",
    "- ONLY the training dataset.\n",
    "\n",
    "```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "  model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Notes\n",
    "- C controls regularization:\n",
    "  - Smaller C → stronger penalization\n",
    "  - Important when many One-Hot encoded features exist\n",
    "- max_iter is increased to avoid convergence warnings\n",
    "\n",
    "\n",
    "## 4. Making Predictions (.predict())\n",
    "Which Dataset Goes Into .predict()?\n",
    "\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "```python\n",
    "  y_pred = model.predict(X_test)\n",
    "```\n",
    "Rule: Never evaluate performance using predictions from X_train.\n",
    "\n",
    "\n",
    "## 5. Probability Predictions (.predict_proba())\n",
    "Used when:\n",
    "- You want class probabilities instead of labels\n",
    "- You want threshold-based decisions\n",
    "- You want confidence-aware analysis\n",
    "\n",
    "```python\n",
    "  y_prob = model.predict_proba(X_test)\n",
    "```\n",
    "* Output shape: (n_samples, 2)\n",
    "  * Column 0 → probability of class 0\n",
    "  * Column 1 → probability of class 1\n",
    "\n",
    "\n",
    "## 6. Evaluation Metrics: Precision & Recall\n",
    "Precision\n",
    "* Of all predicted positives, how many are correct?\n",
    "* Interpretation:\n",
    "  * “When the model predicts diabetes, how often is it right?”\n",
    "\n",
    "Recall\n",
    "* Of all actual positives, how many were found?\n",
    "* Interpretation:\n",
    "  * “How many diabetes cases did the model detect?”\n",
    "\n",
    "```python\n",
    "  from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "  precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "  recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "  precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "  recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "```\n",
    "\n",
    "## 7. Feature Importance (Logistic Regression)\n",
    "\n",
    "How Feature Importance Is Defined\n",
    "* Logistic Regression uses coefficients\n",
    "* Larger absolute coefficient ⇒ stronger influence\n",
    "\n",
    "```python\n",
    "  coefficients = model.coef_[0]\n",
    "```\n",
    "\n",
    "One-Hot Encoded Features\n",
    "* Compare absolute values of coefficients\n",
    "* The most important One-Hot encoded feature is the one with the largest absolute coefficient\n",
    "\n",
    "```python\n",
    "  important_idx = np.argmax(np.abs(coefficients))\n",
    "  important_feature = feature_columns[important_idx]\n",
    "```\n",
    "\n",
    "## 8. What Dataset to Use for Each Task (Summary Table)\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "\n",
    "## 10. Exam-Safe Golden Rules (MEMORIZE)\n",
    "* Never train on test data\n",
    "* Never evaluate on training data\n",
    "* .fit() → training data ONLY\n",
    "* .predict() → test or unseen data ONLY\n",
    "* Metrics → always computed using y_test\n",
    "* One-Hot feature importance → coefficient magnitude\n",
    "* If unsure: ask which dataset is allowed before using it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b50549",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1493",
   "metadata": {},
   "source": [
    "-----\n",
    "## How to think when choosing a proposal distribution \\( g(x) \\) in rejection sampling\n",
    "\n",
    "When facing a difficult rejection sampling problem, do **not** start by guessing formulas. Instead, follow this reasoning process.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Locate where the probability mass is\n",
    "Ask: *Where does the distribution actually concentrate its mass?*\n",
    "\n",
    "Look for terms like:\n",
    "- $( e^{-1/x} \\), \\( e^{x^2} \\), \\( x^\\alpha )$\n",
    "- behavior near boundaries (0, infinity, endpoints)\n",
    "\n",
    "**Rule:**  \n",
    "Your proposal must put mass where the target puts mass.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Identify the dominant term\n",
    "Ignore constants and lower-order factors at first.\n",
    "\n",
    "Ask: *Which part of the density controls the shape?*\n",
    "\n",
    "Examples:\n",
    "- $( e^{-x} )$ → exponential\n",
    "- $( e^{-x^2} )$ → Gaussian-like\n",
    "- $( e^{-1/x} )$ → strong boundary concentration\n",
    "\n",
    "**Rule:**  \n",
    "Match the dominant term first; fix the rest using rejection.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Consider a change of variables\n",
    "If the density contains:\n",
    "- $( 1/x \\), \\( \\log x )$, or sharp boundary behavior\n",
    "\n",
    "Ask: *Would this look simpler in another variable?*\n",
    "\n",
    "Common transformations:\n",
    "- $( Y = 1/X )$ for $( e^{-1/x} )$\n",
    "- $( Y = \\log X )$ for multiplicative scales\n",
    "\n",
    "**Rule:**  \n",
    "If the density is ugly in $( x )$, change coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Choose a proposal that is easy to sample from\n",
    "Good proposals:\n",
    "- Uniform (only if the target is fairly flat)\n",
    "- Exponential or shifted exponential\n",
    "- Gaussian\n",
    "\n",
    "Bad proposals:\n",
    "- hard-to-invert CDFs\n",
    "- complicated expressions\n",
    "\n",
    "**Rule:**  \n",
    "If sampling from $( g(x) )$ is hard, you chose the wrong proposal.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Immediately check the ratio $( f(x)/g(x) )$\n",
    "Before coding, compute:\n",
    "$$\n",
    "\\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "Ask:\n",
    "- Is it bounded?\n",
    "- Does it simplify?\n",
    "- Do exponentials cancel?\n",
    "\n",
    "**Rule:**  \n",
    "If exponentials cancel and the ratio is simple, the proposal is good.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Estimate the rejection constant $( M )$\n",
    "Check where the maximum of $( f(x)/g(x) )$ occurs:\n",
    "- often at boundaries\n",
    "- sometimes at symmetry points\n",
    "\n",
    "Good signs:\n",
    "- $( M \\approx 1 )$: very efficient\n",
    "- $( M \\gg 10 )$: rethink your proposal\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Key mindset\n",
    "Rejection sampling is not mechanical algebra — it is **distribution engineering**:\n",
    "- understand the shape\n",
    "- match it intelligently\n",
    "- use rejection only to correct small differences\n",
    "\n",
    "---\n",
    "\n",
    "### One-line checklist (exam-ready)\n",
    "1. Where is the mass?  \n",
    "2. What term dominates?  \n",
    "3. Should I change variables?  \n",
    "4. Can I sample from $( g )$ easily?  \n",
    "5. Is $( f/g )$ bounded?  \n",
    "6. Is $( M )$ small?\n",
    "\n",
    "If all answers are yes, your choice of $( g(x) )$ is good.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
