{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b5f837",
   "metadata": {},
   "source": [
    "## Here is an example of how to check reverisibility of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P = matrix\n",
    "# pi = linear equation system we have \n",
    "\n",
    "# Transition matrix\n",
    "P = np.array([\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Downtown\n",
    "    [0.2, 0.5, 0.3, 0.0],  # Suburbs\n",
    "    [0.0, 0.0, 0.5, 0.5],  # Countryside\n",
    "    [0.0, 0.0, 0.0, 1.0]   # Workshop\n",
    "])\n",
    "\n",
    "\n",
    "stationary_dist = stationary_distribution(P)\n",
    "\n",
    "\n",
    "# Code for checking reversibility for ANY matrix!:\n",
    "\n",
    "def is_reversible(P, stationary_dist, tol=1e-12):\n",
    "    n = P.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            left = stationary_dist[i] * P[i,j]\n",
    "            right = stationary_dist[j] * P[j,i]\n",
    "            if not np.isclose(left, right, atol=tol):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(\"Reversible:\", is_reversible(P, stationary_dist))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eb88f",
   "metadata": {},
   "source": [
    "## This is how you can always find the Stationary distribution of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64913b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution(P, tol=1e-12, verify=True):\n",
    "    \"\"\"\n",
    "    Compute the stationary distribution of a finite-state Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix. Rows should sum to 1 (row-stochastic).\n",
    "    tol : float, optional\n",
    "        Tolerance used for cleaning small numerical noise.\n",
    "    verify : bool, optional\n",
    "        If True, checks that the result is approximately stationary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.ndarray, shape (n,)\n",
    "        Stationary distribution vector (non-negative, sums to 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system:\n",
    "\n",
    "        (P^T - I) * pi = 0,  with  sum(pi) = 1\n",
    "\n",
    "    by replacing one of the equations with the normalization condition.\n",
    "    It assumes that a (unique) stationary distribution exists.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError(\"P must be a square matrix.\")\n",
    "\n",
    "    # Build A * pi = b\n",
    "    A = P.T - np.eye(n)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    # Replace last row with normalization condition: sum_i pi_i = 1\n",
    "    A[-1, :] = 1.0\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    # Clean tiny numerical noise\n",
    "    pi[np.abs(pi) < tol] = 0.0\n",
    "\n",
    "    # If there are small negative values, clamp them to 0 and renormalize\n",
    "    if np.any(pi < -tol):\n",
    "        # Serious negativity -> indicate potential problem\n",
    "        raise RuntimeError(\n",
    "            \"Computed stationary distribution has significantly negative entries. \"\n",
    "            \"Check that P is a valid transition matrix with a unique stationary distribution.\"\n",
    "        )\n",
    "\n",
    "    # Clamp small negatives and renormalize\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        raise RuntimeError(\"Failed to compute a valid stationary distribution (sum <= 0).\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify:\n",
    "        # Check stationarity: pi P ≈ pi\n",
    "        if not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "            raise RuntimeError(\"Result does not satisfy pi P ≈ pi. Check the input matrix P.\")\n",
    "\n",
    "    return pi\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0aa664",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     stationary = vec / np.sum(vec)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stationary\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m P = \u001b[43mnp\u001b[49m.array([[\u001b[32m0.3\u001b[39m,\u001b[32m0.7\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0.2\u001b[39m,\u001b[32m0.5\u001b[39m,\u001b[32m0.3\u001b[39m,\u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0.5\u001b[39m,\u001b[32m0.5\u001b[39m], [\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m]])\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis is stationary distribution: \u001b[39m\u001b[33m\"\u001b[39m, stationary_distribution(P))\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# This stationary distribution fiunction could also work, just not always the same and correct way:\n",
    "def stationary_distribution(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution of a Markov chain\n",
    "    by finding the eigenvector corresponding to eigenvalue 1.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    \n",
    "    # Find the eigenvector associated with eigenvalue 1\n",
    "    idx = np.argmin(np.abs(eigenvalues - 1))\n",
    "    vec = np.real(eigenvectors[:, idx])\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    stationary = vec / np.sum(vec)\n",
    "    return stationary\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085bfc2",
   "metadata": {},
   "source": [
    "### Always use this stationary distribution function below to check, it WILL always work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2878ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [ 0. -0. -0.  1.]\n"
     ]
    }
   ],
   "source": [
    "def stationary_distribution_always_works(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution by solving\n",
    "    (P^T - I) * pi = 0  with  sum(pi) = 1.\n",
    "    This method ALWAYS works for any Markov chain with a stationary distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Build system: (P^T - I) * pi = 0\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Replace last equation with the normalization condition sum(pi)=1\n",
    "    A[-1] = np.ones(n)\n",
    "\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve the linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4df2f",
   "metadata": {},
   "source": [
    "### This is another stationary distribution function which the bot said will actually always for for any chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb10f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution_any_markov(P, tol=1e-12, cleanup_tol=1e-15, verify=True):\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "    if P.ndim != 2 or n != P.shape[1]:\n",
    "        raise ValueError(\"P must be square.\")\n",
    "\n",
    "    # Validate Markov matrix\n",
    "    if np.any(P < -cleanup_tol):\n",
    "        raise ValueError(\"P has negative entries.\")\n",
    "    if not np.allclose(P.sum(axis=1), 1.0, atol=1e-12):\n",
    "        raise ValueError(\"Rows of P must sum to 1.\")\n",
    "\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Add normalization as an extra equation (least squares)\n",
    "    A_aug = np.vstack([A, np.ones((1, n))])\n",
    "    b_aug = np.zeros(n + 1)\n",
    "    b_aug[-1] = 1.0\n",
    "\n",
    "    # Least-squares solution (works even if A is singular)\n",
    "    pi, *_ = np.linalg.lstsq(A_aug, b_aug, rcond=None)\n",
    "\n",
    "    # Cleanup / project to simplex\n",
    "    pi[np.abs(pi) < cleanup_tol] = 0.0\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        raise RuntimeError(\"Could not recover a valid distribution.\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify and not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "        # If periodic/reducible, this should still pass; if not, input likely invalid/ill-conditioned\n",
    "        raise RuntimeError(\"Result does not satisfy pi P ≈ pi (within tolerance).\")\n",
    "\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7b41",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6437185",
   "metadata": {},
   "source": [
    "### Expected hitting time function\n",
    "\n",
    "This function computes expected hitting times to a given **target set of states** in a finite Markov chain with transition matrix \\( P \\).\n",
    "\n",
    "We consider a Markov chain with state space \\( \\{0, 1, \\dots, n-1\\} \\) and transition matrix\n",
    "\n",
    "$$\n",
    "P = (P_{ij})_{i,j=0}^{n-1},\n",
    "$$\n",
    "\n",
    "where \\( P_{ij} = \\mathbb{P}(X_{t+1} = j \\mid X_t = i) \\).\n",
    "\n",
    "Given a set of **target states** \\( T \\subset \\{0, \\dots, n-1\\} \\), the *hitting time* of \\( T \\) is\n",
    "\n",
    "$$\n",
    "T_{\\text{hit}} = \\min\\{ t \\ge 0 : X_t \\in T \\}.\n",
    "$$\n",
    "\n",
    "For each state \\( i \\), we define the expected hitting time\n",
    "\n",
    "$$\n",
    "h(i) = \\mathbb{E}[T_{\\text{hit}} \\mid X_0 = i].\n",
    "$$\n",
    "\n",
    "These satisfy\n",
    "\n",
    "$$\n",
    "h(i) = 0 \\quad \\text{for } i \\in T,\n",
    "$$\n",
    "\n",
    "and for \\( i \\notin T \\),\n",
    "\n",
    "$$\n",
    "h(i) = 1 + \\sum_{j=0}^{n-1} P_{ij} h(j).\n",
    "$$\n",
    "\n",
    "If we collect the non-target states into a set \\( S = \\{0, \\dots, n-1\\} \\setminus T \\), and form the submatrix \\( Q \\) of \\( P \\) with rows and columns indexed by \\( S \\), then the vector \\( h_S = (h(i))_{i \\in S} \\) solves\n",
    "\n",
    "$$\n",
    "(I - Q) h_S = \\mathbf{1},\n",
    "$$\n",
    "\n",
    "where \\( \\mathbf{1} \\) is a vector of ones.\n",
    "\n",
    "The function `expected_hitting_time` implements this:\n",
    "\n",
    "- **Parameters**\n",
    "  - `P`: `np.ndarray` of shape `(n, n)`  \n",
    "    Transition matrix of the Markov chain.\n",
    "  - `target_states`: iterable of integers  \n",
    "    Indices of the target states \\( T \\).\n",
    "  - `start_state` (optional): integer  \n",
    "    If provided, the function returns \\( h(\\text{start\\_state}) \\).\n",
    "  - `start_dist` (optional): 1D array-like of length `n`  \n",
    "    Initial distribution \\( \\alpha \\). If provided, the function returns\n",
    "    $$\n",
    "    \\mathbb{E}[T_{\\text{hit}}] = \\sum_{i=0}^{n-1} \\alpha_i h(i).\n",
    "    $$\n",
    "\n",
    "- **Return value**\n",
    "  - If `start_state` is given: a single float, \\( h(\\text{start\\_state}) \\).\n",
    "  - If `start_dist` is given: a single float, the expected hitting time under that initial distribution.\n",
    "  - If neither is given: a length-`n` NumPy array, containing \\( h(i) \\) for all states `i` (targets get value `0`).\n",
    "\n",
    "- **Usage example (this exam problem)**\n",
    "\n",
    "For the three-region chain\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "0.3 & 0.4 & 0.3 \\\\\\\\\n",
    "0.2 & 0.5 & 0.3 \\\\\\\\\n",
    "0.4 & 0.3 & 0.3\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "with downtown = state 0 and suburbs = state 1:\n",
    "\n",
    "```python\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # should be 50/13 ≈ 3.8461538\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9228617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def expected_hitting_time(P, target_states, start_state=None, start_dist=None):\n",
    "    \"\"\"\n",
    "    Compute expected hitting times to a given set of target states in a finite Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix of the Markov chain.\n",
    "    target_states : iterable of int\n",
    "        Indices of the target states.\n",
    "    start_state : int, optional\n",
    "        If provided, return the expected hitting time starting from this state.\n",
    "    start_dist : array-like, shape (n,), optional\n",
    "        If provided, return the expected hitting time under this initial distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or np.ndarray\n",
    "        - If start_state is given: expected hitting time from that state.\n",
    "        - If start_dist is given: expected hitting time under that distribution.\n",
    "        - If neither is given: array h of length n with expected hitting times\n",
    "          from all states (targets have value 0).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system\n",
    "\n",
    "        (I - Q) h_S = 1\n",
    "\n",
    "    where Q is the submatrix of P restricted to non-target states,\n",
    "    and 1 is a vector of ones. Assumes that the target set is hit\n",
    "    with probability 1 from the relevant starting states.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    target_states = np.array(sorted(set(target_states)), dtype=int)\n",
    "    all_states = np.arange(n, dtype=int)\n",
    "\n",
    "    # Non-target states S\n",
    "    non_target_states = np.array([s for s in all_states if s not in target_states], dtype=int)\n",
    "\n",
    "    # If all states are targets, hitting time is identically zero\n",
    "    if non_target_states.size == 0:\n",
    "        h = np.zeros(n, dtype=float)\n",
    "        if start_state is not None:\n",
    "            return float(h[start_state])\n",
    "        if start_dist is not None:\n",
    "            start_dist = np.asarray(start_dist, dtype=float)\n",
    "            return float(start_dist @ h)\n",
    "        return h\n",
    "\n",
    "    # Build Q and solve (I - Q) h_S = 1\n",
    "    Q = P[np.ix_(non_target_states, non_target_states)]\n",
    "    I = np.eye(Q.shape[0])\n",
    "    ones = np.ones(Q.shape[0])\n",
    "\n",
    "    # Solve for h_S\n",
    "    h_S = np.linalg.solve(I - Q, ones)\n",
    "\n",
    "    # Put back into full vector h of length n\n",
    "    h = np.zeros(n, dtype=float)\n",
    "    h[target_states] = 0.0\n",
    "    for idx, s in enumerate(non_target_states):\n",
    "        h[s] = h_S[idx]\n",
    "\n",
    "    # Return according to user request\n",
    "    if (start_state is not None) and (start_dist is not None):\n",
    "        raise ValueError(\"Provide either start_state or start_dist, not both.\")\n",
    "\n",
    "    if start_state is not None:\n",
    "        return float(h[start_state])\n",
    "\n",
    "    if start_dist is not None:\n",
    "        start_dist = np.asarray(start_dist, dtype=float)\n",
    "        if start_dist.shape[0] != n:\n",
    "            raise ValueError(\"start_dist must have length equal to number of states.\")\n",
    "        return float(start_dist @ h)\n",
    "\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03d2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.846153846153846\n"
     ]
    }
   ],
   "source": [
    "# Example from above code: \n",
    "\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "# Expected steps until first time in Downtown (state 0) starting from Suburbs (state 1)\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # ~3.846153846153846 (50/13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dde39",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is how you can always find M in a finite interval for the Reject-Accept sampling algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_x(x):\n",
    "    return np.exp(x)          # target pdf\n",
    "\n",
    "def g_x(x):\n",
    "    return 1/np.log(2)        # uniform(0, ln 2) pdf\n",
    "\n",
    "# Remember to change the interval to your interval you have\n",
    "xs = np.linspace(0, np.log(2), 1000)\n",
    "\n",
    "ratio = f_x(xs) / g_x(xs)\n",
    "\n",
    "# This works since M is always maximum of f / g\n",
    "M_num = ratio.max()\n",
    "\n",
    "print(\"Answer: \", 2*np.log(2))\n",
    "\n",
    "print(\"Numeric M ≈\", M_num)   # should be close to 2*np.log(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24c8a7",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is the different Hoeffding intervals from the Lecture notes:\n",
    "\n",
    "## Summary of Concentration Inequalities from the Lecture Notes\n",
    "\n",
    "This cell summarizes the main inequalities used in Monte Carlo estimation and empirical distribution analysis:  \n",
    "- Hoeffding’s inequality for Monte Carlo means  \n",
    "- The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality  \n",
    "- Alternative concentration inequalities listed in the notes  \n",
    "For each formula we also show how to solve for $$\\varepsilon$$ when forming a confidence interval.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hoeffding’s Inequality for Monte Carlo Estimation\n",
    "\n",
    "Assume we estimate a mean using  \n",
    "$$\n",
    "\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i,\n",
    "$$  \n",
    "where the samples satisfy $Y_i \\in [a,b]$.\n",
    "\n",
    "The **two-sided Hoeffding bound** is:\n",
    "$$\n",
    "\\mathbb{P}\\left( \\left| \\overline{Y} - \\mathbb{E}[Y] \\right| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "$$\n",
    "2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Thus a $(1-\\delta)100\\%$ confidence interval is:\n",
    "$$\n",
    "\\left[\\, \\overline{Y} - \\varepsilon,\\; \\overline{Y} + \\varepsilon \\,\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dvoretzky–Kiefer–Wolfowitz (DKW) Inequality\n",
    "\n",
    "For empirical CDF $$F_n(x)$$ based on i.i.d. samples with true CDF $$F(x)$$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( \\sup_x |F_n(x) - F(x)| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp(-2n\\varepsilon^2).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "\n",
    "$$\n",
    "2 \\exp(-2n\\varepsilon^2) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Useful for constructing confidence bands:\n",
    "$$\n",
    "F_n(x) - \\varepsilon \\le F(x) \\le F_n(x) + \\varepsilon.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Other Alternatives Mentioned in the Lecture Notes\n",
    "\n",
    "### (a) Chebyshev’s Inequality\n",
    "Assuming finite variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le \\frac{\\sigma^2}{n\\varepsilon^2}.\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ by setting RHS = $\\delta$:\n",
    "$$\n",
    "\\varepsilon = \\sigma \\sqrt{\\frac{1}{n\\delta}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Central Limit Theorem (CLT) Approximation\n",
    "For large $n$:\n",
    "\n",
    "$$\n",
    "\\overline{Y} \\approx \\mathcal{N}\\!\\left(\\mathbb{E}[Y],\\, \\frac{\\sigma^2}{n}\\right).\n",
    "$$\n",
    "\n",
    "A $(1-\\delta)$ interval is:\n",
    "$$\n",
    "\\overline{Y} \\;\\pm\\; z_{1-\\delta/2}\\,\\frac{\\sigma}{\\sqrt{n}},\n",
    "$$\n",
    "where $z_{1-\\delta/2}$ is the standard normal quantile.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Bernstein (or Chernoff–Hoeffding) Inequality  \n",
    "Sometimes given in extended form when variance is known. In bounded case (same assumptions as Hoeffding):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le 2 \\exp\\!\\left( \n",
    "\\frac{-n\\varepsilon^2}{2\\sigma^2 + \\frac{2}{3}(b-a)\\varepsilon}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ requires numerical methods; not algebraic in closed form.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table of $$\\varepsilon$$ Solutions\n",
    "\n",
    "| Inequality | Bound | Solution for $$\\varepsilon$$ |\n",
    "|-----------|-------|-------------------------------|\n",
    "| Hoeffding | $2\\exp\\!\\left(-\\frac{2n\\varepsilon^2}{(b-a)^2}\\right) \\le \\delta$ | $\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| DKW | $2\\exp(-2n\\varepsilon^2) \\le \\delta$ | $\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| Chebyshev | $\\frac{\\sigma^2}{n\\varepsilon^2} \\le \\delta$ | $\\varepsilon = \\sigma\\sqrt{\\frac{1}{n\\delta}}$ |\n",
    "| CLT | approx | $$\\varepsilon = z_{1-\\delta/2}\\,\\sigma/\\sqrt{n}$$ |\n",
    "| Bernstein | not closed form | requires numerical solution |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03c909",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### General information that could be important:\n",
    "\n",
    "* .values and .to_numpy() both convert pandas DataFrames or Series into NumPy arrays; \n",
    "* .to_numpy() is the recommended modern approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003d142",
   "metadata": {},
   "source": [
    "-----\n",
    "### Permutation Importance\n",
    "\n",
    "Permutation importance is a model-agnostic method for measuring feature importance. It works by randomly permuting the values of a single feature in the test set and then measuring how much the model’s predictive performance decreases. If permuting a feature leads to a large drop in performance, the model relied heavily on that feature, and it is considered important.\n",
    "\n",
    "This method measures the **impact of each feature on the model’s predictive performance**, rather than relying on model-specific parameters. Because it only requires the ability to make predictions and evaluate them with a chosen metric, permutation importance is applicable to **any type of predictive model**, including linear models, tree-based models, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa904c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERAL TEMPLATE: Permutation Importance (works for ANY model)\n",
    "# ============================================================\n",
    "# What you MUST change depending on your setup:\n",
    "#   1) estimator      -> set this to your TRAINED model (or Pipeline)\n",
    "#   2) X_test, y_test -> set these to your TEST split\n",
    "#   3) feature_names  -> set these to your column names (list of strings)\n",
    "#   4) scoring        -> choose a metric appropriate for your task\n",
    "#\n",
    "# Notes:\n",
    "# - This works for any model as long as it has predict() (or predict_proba() for some scorers)\n",
    "# - If you used preprocessing (scaling, one-hot encoding, etc.), it's best to wrap it in a Pipeline\n",
    "#   and pass the Pipeline as the estimator to avoid mismatches.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --------------------------\n",
    "# 1) CHOOSE YOUR TRAINED MODEL\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If you trained a Pipeline (recommended): estimator = my_pipeline\n",
    "# - If you trained a plain model:           estimator = my_model\n",
    "#\n",
    "# Examples:\n",
    "# estimator = problem3_model                   # Pipeline: scaler + logistic regression\n",
    "# estimator = trained_random_forest_model      # e.g., RandomForestClassifier already fit\n",
    "# estimator = trained_svm_model                # e.g., SVC already fit\n",
    "estimator = problem3_model  # <-- CHANGE to your trained model / pipeline\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) PROVIDE TEST DATA\n",
    "# --------------------------\n",
    "# CHANGE THESE:\n",
    "# - X_test should be the test features (NumPy array or pandas DataFrame)\n",
    "# - y_test should be the test labels\n",
    "#\n",
    "# Examples:\n",
    "# X_test = problem3_X_test\n",
    "# y_test = problem3_y_test\n",
    "X_test = problem3_X_test   # <-- CHANGE if your variables are named differently\n",
    "y_test = problem3_y_test   # <-- CHANGE if your variables are named differently\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) PROVIDE FEATURE NAMES\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If X_test is a pandas DataFrame, you can do: feature_names = X_test.columns\n",
    "# - If X_test is a NumPy array, you must supply a list yourself (same order as columns in X_test)\n",
    "#\n",
    "# Examples:\n",
    "# feature_names = problem3_features\n",
    "# feature_names = list(X_test.columns)\n",
    "feature_names = problem3_features  # <-- CHANGE to your feature name list (correct order!)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) CHOOSE A SCORING METRIC\n",
    "# --------------------------\n",
    "# CHANGE THIS depending on your task:\n",
    "# Classification examples:\n",
    "#   scoring = \"accuracy\"            (simple, common)\n",
    "#   scoring = \"balanced_accuracy\"   (good if classes are imbalanced)\n",
    "#   scoring = \"f1\"                  (if you care about positive class quality)\n",
    "#   scoring = \"roc_auc\"             (needs probability or decision scores; many models support it)\n",
    "#\n",
    "# Regression examples:\n",
    "#   scoring = \"r2\"\n",
    "#   scoring = \"neg_mean_squared_error\"\n",
    "#   scoring = \"neg_mean_absolute_error\"\n",
    "#\n",
    "# Tip:\n",
    "# - If \"roc_auc\" fails, your estimator may not provide predict_proba/decision_function.\n",
    "scoring = \"accuracy\"  # <-- CHANGE if needed\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) RUN PERMUTATION IMPORTANCE\n",
    "# --------------------------\n",
    "perm = permutation_importance(\n",
    "    estimator=estimator,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    n_repeats=30,        # increase for more stable estimates (slower)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6) FORMAT RESULTS\n",
    "# --------------------------\n",
    "perm_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance Mean\": perm.importances_mean,\n",
    "    \"Importance Std\": perm.importances_std\n",
    "}).sort_values(by=\"Importance Mean\", ascending=False)\n",
    "\n",
    "print(\"Top features by permutation importance:\")\n",
    "print(perm_df.head(15))\n",
    "\n",
    "# --------------------------\n",
    "# 7) OPTIONAL: GET MOST IMPORTANT FEATURE\n",
    "# --------------------------\n",
    "most_important_feature = perm_df.iloc[0][\"Feature\"]\n",
    "print(\"\\nMost important feature (by permutation importance):\", most_important_feature)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 8) OPTIONAL: RESTRICT TO A SUBSET OF FEATURES\n",
    "# --------------------------\n",
    "# Example: if you want \"most important one-hot encoded feature\"\n",
    "# CHANGE the selection rule to match your one-hot naming scheme.\n",
    "# For your diabetes case (features starting with smoking_ or sex_):\n",
    "subset = [f for f in feature_names if str(f).startswith(\"smoking_\") or str(f).startswith(\"sex_\")]\n",
    "\n",
    "if len(subset) > 0:\n",
    "    perm_subset = perm_df[perm_df[\"Feature\"].isin(subset)].sort_values(by=\"Importance Mean\", ascending=False)\n",
    "    print(\"\\nPermutation importance for subset features:\")\n",
    "    print(perm_subset)\n",
    "\n",
    "    most_important_in_subset = perm_subset.iloc[0][\"Feature\"]\n",
    "    print(\"\\nMost important feature in subset:\", most_important_in_subset)\n",
    "else:\n",
    "    print(\"\\nSubset list is empty. Adjust the subset selection rule to match your feature names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f29983",
   "metadata": {},
   "source": [
    "### Remember:\n",
    "If the dataset does not have a header, then you need to use header=None in the pd.read_csv(\"data\", header=None). Otherwise we will miss one column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124bb2",
   "metadata": {},
   "source": [
    "-----\n",
    "### Using Utils.py file\n",
    "\n",
    "If the exam question requires the file Utils.py, then simply copy paste the file into the current folder I am in and then the code will be able to find the file Utils.py. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f693a4e",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is general information when to use which data set in the exam:\n",
    "\n",
    "## When to use each dataset and variable\n",
    "\n",
    "The data in this assignment is split into **training**, **validation**, and **test** sets. Each set has a specific role, and using them correctly is essential to avoid data leakage and to obtain an unbiased evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Training set: fit the model\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_train`\n",
    "- `PROBLEM3_y_train`\n",
    "\n",
    "**When to use**\n",
    "Use the training set **only to learn the model parameters**.\n",
    "\n",
    "**Typical operations**\n",
    "- Fit a model:\n",
    "\n",
    "```python\n",
    "    model.fit(PROBLEM3_X_train, PROBLEM3_y_train)\n",
    "```\n",
    "\n",
    "\n",
    "- Do **not** compute performance metrics or choose thresholds using training data.\n",
    "\n",
    "**Purpose**\n",
    "The training set teaches the model the relationship between features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Validation set: model selection and threshold choice\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_val`\n",
    "- `PROBLEM3_y_val` (or `PROBLEM3_y_true_val`)\n",
    "- `PROBLEM3_y_pred_proba_val`\n",
    "\n",
    "**When to use**\n",
    "Use the validation set to **make decisions about the model**, such as:\n",
    "- choosing a classification threshold,\n",
    "- comparing different loss functions,\n",
    "- computing cost, precision, recall, and 0–1 loss.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_val = model.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "```\n",
    "\n",
    "- Convert probabilities to predictions:\n",
    "\n",
    "```python\n",
    "    y_pred_val = (y_pred_proba_val >= threshold).astype(int)\n",
    "```\n",
    "\n",
    "- Compute metrics:\n",
    "- cost\n",
    "- precision\n",
    "- recall\n",
    "- 0–1 loss\n",
    "\n",
    "**Purpose**\n",
    "The validation set is used to **tune decisions** without biasing the final evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Test set: final evaluation only\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_test`\n",
    "- `PROBLEM3_y_test` (or `PROBLEM3_y_true_test`)\n",
    "- `PROBLEM3_y_pred_proba_test`\n",
    "\n",
    "**When to use**\n",
    "Use the test set **only after**:\n",
    "- the model has been trained,\n",
    "- the threshold has been chosen using validation data.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_test = model.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "```\n",
    "\n",
    "- Evaluate final performance:\n",
    "- compute final cost,\n",
    "- build a confidence interval,\n",
    "- report final metrics.\n",
    "\n",
    "**Purpose**\n",
    "The test set provides an **unbiased estimate of real-world performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary table (conceptual)\n",
    "\n",
    "- Training set → **fit the model**\n",
    "- Validation set → **choose thresholds and compare decision rules**\n",
    "- Test set → **final evaluation and confidence intervals**\n",
    "\n",
    "---\n",
    "\n",
    "## Important rules to remember\n",
    "\n",
    "- Never choose thresholds using the test set.\n",
    "- Never report final performance using the validation set.\n",
    "- The test set must only be used **once**, at the very end.\n",
    "- Predicted probabilities (`predict_proba`) are used for **threshold-based decisions**.\n",
    "- Binary predictions (`>= threshold`) are used for **cost, precision, recall, and loss**.\n",
    "\n",
    "Following these rules ensures a correct and exam-safe machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cea85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1e8f93c",
   "metadata": {},
   "source": [
    "# General Guide: Which Dataset to Use, When, and Why (Logistic Regression & Classification)\n",
    "\n",
    "THIS RESPONSE IS **INTENTIONALLY AND EXCLUSIVELY** A SINGLE MARKDOWN TEXT CELL.  \n",
    "There is **NO TEXT OUTSIDE THIS BLOCK**.  \n",
    "You can copy **once** and paste directly into a Jupyter Notebook **Markdown cell**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Defining Features and Target\n",
    "\n",
    "### Features (`X`)\n",
    "- Features are the **input variables** used by the model to make predictions.\n",
    "- These typically include:\n",
    "  - Numerical variables (e.g. age, BMI, blood glucose)\n",
    "  - One-Hot encoded categorical variables (e.g. `sex_Male`, `smoking_former`)\n",
    "- **Rule**: Features must represent information that is available **before** a prediction is made.\n",
    "- **Rule**: Never include the target variable inside the feature set.\n",
    "\n",
    "### Target (`y`)\n",
    "- The target is what the model is trying to predict.\n",
    "- For classification:\n",
    "  - Binary variable (e.g. diabetes = 0 or 1)\n",
    "- **Rule**: The target must NEVER be included among the features.\n",
    "\n",
    "```python\n",
    "problem3_X = problem3_df[feature_columns].values\n",
    "problem3_y = problem3_df[target_column].values\n",
    "```\n",
    "\n",
    "## 2. Train–Test Split (Why and How)\n",
    "\n",
    "Training Dataset\n",
    "- Used to train (fit) the model.\n",
    "- The model learns patterns from this data.\n",
    "\n",
    "Test Dataset\n",
    "- Used to evaluate final performance.\n",
    "- Simulates unseen, real-world data.\n",
    "- Must NEVER be used during training.\n",
    "\n",
    "Standard Split\n",
    "- 80% training\n",
    "- 20% testing\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    problem3_X,\n",
    "    problem3_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## 3. Training the Model (.fit())\n",
    "Which Dataset Goes Into .fit()?\n",
    "- ONLY the training dataset.\n",
    "\n",
    "```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "  model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Notes\n",
    "- C controls regularization:\n",
    "  - Smaller C → stronger penalization\n",
    "  - Important when many One-Hot encoded features exist\n",
    "- max_iter is increased to avoid convergence warnings\n",
    "\n",
    "\n",
    "## 4. Making Predictions (.predict())\n",
    "Which Dataset Goes Into .predict()?\n",
    "\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "```python\n",
    "  y_pred = model.predict(X_test)\n",
    "```\n",
    "Rule: Never evaluate performance using predictions from X_train.\n",
    "\n",
    "\n",
    "## 5. Probability Predictions (.predict_proba())\n",
    "Used when:\n",
    "- You want class probabilities instead of labels\n",
    "- You want threshold-based decisions\n",
    "- You want confidence-aware analysis\n",
    "\n",
    "```python\n",
    "  y_prob = model.predict_proba(X_test)\n",
    "```\n",
    "* Output shape: (n_samples, 2)\n",
    "  * Column 0 → probability of class 0\n",
    "  * Column 1 → probability of class 1\n",
    "\n",
    "\n",
    "## 6. Evaluation Metrics: Precision & Recall\n",
    "Precision\n",
    "* Of all predicted positives, how many are correct?\n",
    "* Interpretation:\n",
    "  * “When the model predicts diabetes, how often is it right?”\n",
    "\n",
    "Recall\n",
    "* Of all actual positives, how many were found?\n",
    "* Interpretation:\n",
    "  * “How many diabetes cases did the model detect?”\n",
    "\n",
    "```python\n",
    "  from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "  precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "  recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "  precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "  recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "  precision0_n = np.sum(y_pred == 0)\n",
    "  precision1_n = np.sum(y_pred == 1)\n",
    "\n",
    "  recall0_n = np.sum(y_true == 0)\n",
    "  recall1_n = np.sum(y_true == 1)\n",
    "```\n",
    "\n",
    "## 7. Feature Importance (Logistic Regression)\n",
    "\n",
    "How Feature Importance Is Defined\n",
    "* Logistic Regression uses coefficients\n",
    "* Larger absolute coefficient ⇒ stronger influence\n",
    "\n",
    "```python\n",
    "  coefficients = model.coef_[0]\n",
    "```\n",
    "\n",
    "One-Hot Encoded Features\n",
    "* Compare absolute values of coefficients\n",
    "* The most important One-Hot encoded feature is the one with the largest absolute coefficient\n",
    "\n",
    "```python\n",
    "  important_idx = np.argmax(np.abs(coefficients))\n",
    "  important_feature = feature_columns[important_idx]\n",
    "```\n",
    "\n",
    "## 8. What Dataset to Use for Each Task (Summary Table)\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "\n",
    "## 10. Exam-Safe Golden Rules (MEMORIZE)\n",
    "* Never train on test data\n",
    "* Never evaluate on training data\n",
    "* .fit() → training data ONLY\n",
    "* .predict() → test or unseen data ONLY\n",
    "* Metrics → always computed using y_test\n",
    "* One-Hot feature importance → coefficient magnitude\n",
    "* If unsure: ask which dataset is allowed before using it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f1027",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ab3341a",
   "metadata": {},
   "source": [
    "# Relevant metrics to look at the LinearRegression model: \n",
    "\n",
    "Below are four common regression metrics. Each compares the **true target values** to the model’s **predicted target values**.\n",
    "\n",
    "### Notation\n",
    "- $y_i$: true target value for sample $i$\n",
    "- $\\hat{y}_i$: predicted target value for sample $i$\n",
    "- $\\bar{y}$: mean of true targets in the evaluated set (typically the test set)\n",
    "- $n$: number of samples in the evaluated set\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Mean Squared Error (MSE)\n",
    "**What it does (short):** Measures the average squared prediction error. Large errors are penalized more because of squaring.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test` (true values for the test set)\n",
    "- $\\hat{y}$ = `y_pred` (predictions for the test set)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Root Mean Squared Error (RMSE)\n",
    "**What it does (short):** Square root of MSE, giving an error measure in the **same unit** as the target variable.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Mean Absolute Error (MAE)\n",
    "**What it does (short):** Measures the average absolute prediction error. Less sensitive to outliers than MSE/RMSE.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## 4) $R^2$ Score (Coefficient of Determination)\n",
    "**What it does (short):** Measures how much of the variance in $y$ is explained by the model. \n",
    "- $R^2 = 1$ is perfect fit\n",
    "- $R^2 = 0$ is no better than predicting the mean\n",
    "- $R^2 < 0$ is worse than predicting the mean\n",
    "\n",
    "**Formula:**\n",
    "$ R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "- $\\bar{y}$ is the mean of `y_test`\n",
    "\n",
    "---\n",
    "\n",
    "# Python code to compute all metrics (scikit-learn)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume you already have a train/test split:\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# 1) Train the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2) Create predictions on the test data\n",
    "# y_pred is \"y_hat\" (predicted y). It comes from calling model.predict(...) on X_test.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 3) Compute metrics comparing y_test (true) vs y_pred (predicted)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE  = {mse:.6f}\")\n",
    "print(f\"RMSE = {rmse:.6f}\")\n",
    "print(f\"MAE  = {mae:.6f}\")\n",
    "print(f\"R^2  = {r2:.6f}\")\n",
    "```\n",
    "\n",
    "Summary of which variables are used\n",
    "* **y_test**: the true target values for the test set (ground truth).\n",
    "* **y_pred**: the predicted target values for the test set, computed as:\n",
    "* **y_pred** = model.predict(X_test)\n",
    "* All four metrics above should typically be reported on the test set:\n",
    "    * Compare y_test vs y_pred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e60e4",
   "metadata": {},
   "source": [
    "## Interpreting regression metric values and how to compute them in practice\n",
    "\n",
    "### What does “closer to the true value” mean?\n",
    "In regression, each data point has:\n",
    "- a **true value** $y_i$ (the actual observed target from the dataset), and\n",
    "- a **predicted value** $\\hat{y}_i$ (the value predicted by the model).\n",
    "\n",
    "The difference between them is called the **residual**:\n",
    "$ \\text{residual}_i = y_i - \\hat{y}_i $\n",
    "\n",
    "A model is considered better when these residuals are small, meaning the predictions are numerically close to the true values.\n",
    "\n",
    "---\n",
    "\n",
    "## How do you get the actual (true) values?\n",
    "The **true values** come directly from your dataset.\n",
    "\n",
    "After splitting the data:\n",
    "- `y_train`: true target values used to train the model\n",
    "- `y_test`: true target values used to evaluate the model\n",
    "\n",
    "All regression metrics should be computed using **`y_test`**, because these values were not seen during training.\n",
    "\n",
    "---\n",
    "\n",
    "## How do you get the predicted values?\n",
    "Predicted values are produced by the trained model:\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "- `X_test`: input features for the test set\n",
    "- `y_pred`: predicted target values ($\\hat{y}$)\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Average squared difference between true and predicted values\n",
    "- **Interpretation:** Strongly penalizes large errors\n",
    "\n",
    "$ \\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` (true values) vs `y_pred` (predicted values)\n",
    "\n",
    "---\n",
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Square root of MSE, in the same unit as the target\n",
    "- **Interpretation:** Typical size of prediction error\n",
    "\n",
    "$ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    import numpy as np\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Average absolute difference between true and predicted values\n",
    "- **Interpretation:** Less sensitive to outliers than MSE/RMSE\n",
    "\n",
    "$ \\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## $R^2$ Score (Coefficient of Determination)\n",
    "- **Goal:** Higher is better\n",
    "- **What it measures:** Fraction of variance in the true values explained by the model\n",
    "- **Interpretation:**\n",
    "  - $R^2 = 1$: perfect predictions\n",
    "  - $R^2 = 0$: no better than predicting the mean\n",
    "  - $R^2 < 0$: worse than predicting the mean\n",
    "\n",
    "$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`  \n",
    "$\\bar{y}$ is the mean of `y_test`\n",
    "\n",
    "---\n",
    "\n",
    "## Complete minimal example (context)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Train the model on true training values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict target values for unseen data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **True (actual) values:** `y_test` (ground truth from the dataset)\n",
    "- **Predicted values:** `y_pred`, obtained using `model.predict(X_test)`\n",
    "- **Each metric compares:** `y_test` vs `y_pred`\n",
    "- **Lower is better:** MSE, RMSE, MAE\n",
    "- **Higher is better:** $R^2$\n",
    "- **Key idea:** Better models produce predictions $\\hat{y}$ that are numerically closer to the true values $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b50549",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1493",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a18dd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a636f",
   "metadata": {},
   "source": [
    "-----\n",
    "# Below is General functions that can be used in this course, with their description of what they actually do and when they are to be used:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb36415",
   "metadata": {},
   "source": [
    "## This is the General Info from the 2025 January Exam: \n",
    "\n",
    "# SVD & Anomaly Detection — Exam-Oriented Summary (Part 1)\n",
    "\n",
    "This section summarizes the **key functions and concepts** used in Part 1 of the SVD exercise.  \n",
    "The goal is to understand **what each concept does, when it is used, and why it is used**, without focusing on mathematical derivations.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "### `pd.read_csv(\"data/SVD.csv\", header=None)`\n",
    "\n",
    "**What it does**  \n",
    "Loads the CSV file and treats **every row as data**, not as column names.\n",
    "\n",
    "**When it is used**  \n",
    "Used when the dataset contains **only numerical values**, which is common in linear algebra and machine learning problems.\n",
    "\n",
    "**Why it is used**  \n",
    "SVD requires a **pure numeric matrix**. If a header were assumed, the first data row could be dropped or interpreted incorrectly.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`header=None` means *“this CSV has no column names; everything is data.”*\n",
    "\n",
    "---\n",
    "\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "### `np.linalg.svd(X, full_matrices=False)`\n",
    "\n",
    "**What it does**  \n",
    "Decomposes the data matrix $X$ into three components:\n",
    "\\[\n",
    "X \\approx U D V^T\n",
    "\\]\n",
    "\n",
    "**When it is used**  \n",
    "- Dimensionality reduction  \n",
    "- Anomaly detection  \n",
    "- Noise reduction  \n",
    "- Data compression  \n",
    "- Feature extraction  \n",
    "\n",
    "**Why it is used**  \n",
    "It separates the most important patterns in the data from less important ones, making it easier to approximate and analyze the dataset.\n",
    "\n",
    "**Exam takeaway**  \n",
    "SVD breaks a matrix into **patterns** and **how important those patterns are**.\n",
    "\n",
    "---\n",
    "\n",
    "## Meaning of $U$, $D$, and $V$\n",
    "\n",
    "### $U$ — Left singular vectors  \n",
    "- A matrix whose **columns** describe patterns across **samples (rows)**.  \n",
    "- Shape (with `full_matrices=False`):\n",
    "\\[\n",
    "(n\\_samples, r)\n",
    "\\]\n",
    "- Each column shows how strongly each sample is associated with a component.\n",
    "\n",
    "**Think of $U$ as:**  \n",
    "“How each data point uses the components.”\n",
    "\n",
    "---\n",
    "\n",
    "### $D$ — Singular values (diagonal matrix)  \n",
    "- A **diagonal matrix** containing the singular values.  \n",
    "- Shape:\n",
    "\\[\n",
    "(r, r)\n",
    "\\]\n",
    "- Larger values indicate more important components.\n",
    "\n",
    "**Why diagonal**  \n",
    "Each component scales independently, without mixing with others.\n",
    "\n",
    "---\n",
    "\n",
    "### $V$ — Right singular vectors  \n",
    "- A matrix whose **columns** describe patterns across **features (dimensions)**.  \n",
    "- Shape:\n",
    "\\[\n",
    "(n\\_dimensions, r)\n",
    "\\]\n",
    "\n",
    "**Think of $V$ as:**  \n",
    "“What the components look like in feature space.”\n",
    "\n",
    "---\n",
    "\n",
    "## Why `np.diag(s)` is used\n",
    "\n",
    "### `np.diag(s)`\n",
    "\n",
    "**What it does**  \n",
    "Converts the 1D vector of singular values `s` into a **diagonal matrix** $D$.\n",
    "\n",
    "**When it is used**  \n",
    "When explicitly reconstructing the matrix using:\n",
    "\\[\n",
    "X \\approx U D V^T\n",
    "\\]\n",
    "\n",
    "**Why it is used**  \n",
    "NumPy returns singular values as a vector for efficiency, but the SVD definition requires $D$ to be a matrix.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`s` contains the values, `np.diag(s)` builds the proper $D$ matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## `full_matrices=False` (important)\n",
    "\n",
    "**What it controls**  \n",
    "The **size** of the matrices returned by SVD.\n",
    "\n",
    "**With `full_matrices=True`**  \n",
    "- Produces large square matrices.\n",
    "- Often unnecessary and inefficient.\n",
    "\n",
    "**With `full_matrices=False`**  \n",
    "- Produces the **compact (economy) SVD**.\n",
    "- Keeps only the components needed to reconstruct and approximate the data.\n",
    "- More memory-efficient and faster.\n",
    "\n",
    "**Why it is used here**  \n",
    "This is the standard choice in data science and machine learning because it keeps only the meaningful information.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`full_matrices=False` means *“give me the smallest useful SVD.”*\n",
    "\n",
    "---\n",
    "\n",
    "## NumPy slicing for singular vectors\n",
    "\n",
    "### `problem1_first_right_singular_vector = problem1_V[:, 0]`\n",
    "\n",
    "**What it does**  \n",
    "Extracts the **first column** of the matrix $V$ and returns it as a 1D array.\n",
    "\n",
    "**How slicing works**  \n",
    "In NumPy:\n",
    "* array[rows, columns]\n",
    "\n",
    "- `:` means all rows  \n",
    "- `0` means column index 0  \n",
    "\n",
    "So `[:, 0]` means *all rows from column 0*.\n",
    "\n",
    "**Why it is used**  \n",
    "- Columns of $V$ are **right singular vectors**.\n",
    "- Column 0 corresponds to the **most important component**.\n",
    "\n",
    "**Common mistake**  \n",
    "This does **not** select a row — it selects a column.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`[:, 0]` extracts the **first (most important) singular vector**.\n",
    "\n",
    "---\n",
    "\n",
    "## One-line exam summary\n",
    "\n",
    "- `header=None` → CSV has no column names  \n",
    "- `svd(X)` → decomposes data into patterns and their importance  \n",
    "- $U$ → how samples relate to components  \n",
    "- $D$ → importance of each component (diagonal matrix)  \n",
    "- $V$ → what components look like in feature space  \n",
    "- `full_matrices=False` → compact, efficient SVD  \n",
    "- `np.diag(s)` → build the diagonal $D$ matrix  \n",
    "- `[:, 0]` → extract the first singular vector  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e980d48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11014eb",
   "metadata": {},
   "source": [
    "# SVD — Explained Variance (Part 2) Exam-Oriented Summary\n",
    "\n",
    "This section explains **explained variance**, how it is calculated in the context of SVD, and the roles of `cumsum()` and `argmax()`. The focus is on **what these concepts mean and why they are used**, not on mathematical derivations.\n",
    "\n",
    "---\n",
    "\n",
    "## What is explained variance?\n",
    "\n",
    "**Explained variance** measures **how much of the total information (variation)** in the original dataset is captured when keeping only the first $k$ singular components from the SVD.\n",
    "\n",
    "In plain terms:\n",
    "- It tells you **how well a reduced version of the data represents the original data**\n",
    "- Higher explained variance means **less information loss**\n",
    "- It helps decide **how many components are enough**\n",
    "\n",
    "For example:\n",
    "- 60% explained variance → much structure is lost  \n",
    "- 95% explained variance → almost all important structure is retained  \n",
    "\n",
    "**Exam intuition**  \n",
    "Explained variance answers the question:  \n",
    "> “If I keep only the first $k$ components, how much of the original data do I still explain?”\n",
    "\n",
    "---\n",
    "\n",
    "## How is explained variance calculated?\n",
    "\n",
    "From the SVD, each singular value $\\sigma_j$ represents how important a component is.\n",
    "\n",
    "The calculation follows this logic:\n",
    "\n",
    "1. **Square each singular value**  \n",
    "   - $\\sigma_j^2$ represents the variance contribution of component $j$\n",
    "\n",
    "2. **Sum all squared singular values**  \n",
    "   - This gives the **total variance** in the data\n",
    "\n",
    "3. **Compute the cumulative fraction**\n",
    "$$\n",
    "\\text{ExplainedVariance}(k) =\n",
    "\\frac{\\sum_{j=1}^{k} \\sigma_j^2}{\\sum_{j=1}^{r} \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "This produces values between $0$ and $1$ (or $0\\%$ to $100\\%$).\n",
    "\n",
    "**Why this works conceptually**  \n",
    "- Larger singular values correspond to more important directions\n",
    "- By summing them in order, we see how information accumulates as we keep more components\n",
    "\n",
    "---\n",
    "\n",
    "## What does `np.cumsum()` do?\n",
    "\n",
    "### `np.cumsum(array)`\n",
    "\n",
    "**What it does**  \n",
    "Computes the **cumulative sum** of an array.\n",
    "\n",
    "Example:\n",
    "Input: [a, b, c, d]\n",
    "Output: [a, a+b, a+b+c, a+b+c+d]\n",
    "\n",
    "\n",
    "**Why it is used here**\n",
    "- Singular values are ordered from **most important to least important**\n",
    "- `cumsum()` allows us to track:\n",
    "  - variance explained by 1 component\n",
    "  - variance explained by 2 components\n",
    "  - variance explained by 3 components\n",
    "  - and so on\n",
    "\n",
    "**In this exercise**\n",
    "np.cumsum(singular_values_squared) / total_variance\n",
    "\n",
    "produces a vector where each entry tells:\n",
    "“How much of the total variance is explained if we keep components up to this point”\n",
    "\n",
    "**Exam takeaway**  \n",
    "`cumsum()` builds the running total so we can see how variance accumulates as components are added.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `argmax()` and what does it do?\n",
    "\n",
    "### `np.argmax(array)`\n",
    "\n",
    "**What it does**\n",
    "- Returns the **index of the first maximum value** in an array.\n",
    "\n",
    "When used on a boolean array:\n",
    "- `False` is treated as 0  \n",
    "- `True` is treated as 1  \n",
    "- The first `True` is the maximum\n",
    "\n",
    "---\n",
    "\n",
    "## Why `argmax()` is used here\n",
    "\n",
    "This line:\n",
    "```python\n",
    "np.argmax(problem1_explained_variance >= 0.95)\n",
    "```\n",
    "\n",
    "Works conceptually as follows:\n",
    "problem1_explained_variance >= 0.95 → creates a boolean array like:\n",
    "[False, False, False, True, True, ...]\n",
    "\n",
    "argmax() → returns the index of the first True value\n",
    "\n",
    "+ 1→ converts from zero-based indexing to a component count starting at 1\n",
    "\n",
    "What this gives\n",
    "* The smallest number of components needed to explain at least 95% of the variance\n",
    "\n",
    "Exam intuition\n",
    "argmax() is used to answer:\n",
    "* “When does this condition become true for the first time?”\n",
    "\n",
    "One-line exam summary\n",
    "* Explained variance → how much information is retained using $k$ components\n",
    "* Squared singular values → variance contributions\n",
    "* Total variance → sum of all squared singular values\n",
    "* cumsum() → builds cumulative explained variance\n",
    "* argmax() → finds the first index where a condition is satisfied\n",
    "* Combined → select the smallest number of components that reaches a target variance level (e.g. 95%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4242909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343c2ff2",
   "metadata": {},
   "source": [
    "# This is an example from the Exam 230815 of how the plotting of the empirical distribution function of the residual with confidence bands (i.e. using the DKW inequality and 95% confidence) works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dea0e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. [3p] Empirical CDF of residuals with DKW 95% confidence band\n",
    "\n",
    "We want to study the **distribution of the residuals on the test set** and add a **uniform confidence band** using the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define residuals on the test set\n",
    "\n",
    "For each test point $(i = 1,\\dots,n)$, we have:\n",
    "\n",
    "- True salary $(y_i)$ (from $(\\text{problem2\\_y\\_test})$)\n",
    "- Predicted salary $(\\hat{y}_i)$ (from $(\\text{problem2\\_model.predict})$)\n",
    "\n",
    "The **residual** is\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i.\n",
    "$$\n",
    "\n",
    "Collect all test residuals in a vector\n",
    "\n",
    "$$\n",
    "e_1, e_2, \\dots, e_n.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Empirical distribution function (EDF) of the residuals\n",
    "\n",
    "The **empirical distribution function** (EDF) of the residuals is defined as\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(t)\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{ e_i \\le t \\},\n",
    "$$\n",
    "\n",
    "where $(\\mathbf{1}\\{\\cdot\\})$ is the indicator function.\n",
    "\n",
    "In practice, we:\n",
    "\n",
    "1. Sort the residuals:\n",
    "\n",
    "   $$\n",
    "   e_{(1)} \\le e_{(2)} \\le \\dots \\le e_{(n)},\n",
    "   $$\n",
    "\n",
    "2. At each sorted residual $(e_{(k)})$, the EDF jumps to\n",
    "\n",
    "   $$ \n",
    "   \\hat{F}_n\\bigl(e_{(k)}\\bigr) = \\frac{k}{n}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: DKW inequality and 95% confidence band\n",
    "\n",
    "Let $(F(t))$ be the **true CDF** of the residuals (unknown).  \n",
    "The DKW inequality states that for any $(\\varepsilon > 0)$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left(\n",
    "\\sup_{t} \\left| \\hat{F}_n(t) - F(t) \\right| > \\varepsilon\n",
    "\\right)\n",
    "\\le\n",
    "2 e^{-2 n \\varepsilon^2}.\n",
    "$$\n",
    "\n",
    "To get a **\\(95\\%\\)** (i.e. $(1-\\alpha = 0.95)$) **uniform confidence band**, we set $(\\alpha = 0.05)$ and solve\n",
    "\n",
    "$$\n",
    "2 e^{- 2 n \\varepsilon^2 } = \\alpha.\n",
    "$$\n",
    "\n",
    "Taking logarithms:\n",
    "\n",
    "$$\n",
    "e^{- 2 n \\varepsilon^2 } = \\frac{\\alpha}{2}\n",
    "\\quad\\Rightarrow\\quad\n",
    "-2 n \\varepsilon^2 = \\log \\frac{\\alpha}{2}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\varepsilon^2 = -\\frac{1}{2n} \\log \\frac{\\alpha}{2}.\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\varepsilon_n\n",
    "=\n",
    "\\sqrt{\n",
    "-\\frac{1}{2n} \\log \\frac{\\alpha}{2}\n",
    "}\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log\\!\\left( \\frac{2}{\\alpha} \\right)\n",
    "}.\n",
    "$$\n",
    "\n",
    "For $(\\alpha = 0.05)$,\n",
    "\n",
    "$$\n",
    "\\varepsilon_n\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log\\!\\left( \\frac{2}{0.05} \\right)\n",
    "}\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log(40)\n",
    "}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Constructing the confidence band\n",
    "\n",
    "For each $(t)$, the **95% confidence band** is\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(t) - \\varepsilon_n\n",
    "\\le\n",
    "F(t)\n",
    "\\le\n",
    "\\hat{F}_n(t) + \\varepsilon_n,\n",
    "$$\n",
    "\n",
    "or in terms of lower and upper band functions:\n",
    "\n",
    "$$\n",
    "F^{-}(t) = \\max\\bigl( \\hat{F}_n(t) - \\varepsilon_n,\\ 0 \\bigr),\n",
    "\\qquad\n",
    "F^{+}(t) = \\min\\bigl( \\hat{F}_n(t) + \\varepsilon_n,\\ 1 \\bigr).\n",
    "$$\n",
    "\n",
    "In the plot, we:\n",
    "\n",
    "- Plot the EDF $(\\hat{F}_n(t))$ as a step curve,\n",
    "- Plot the lower band $(F^{-}(t))$,\n",
    "- Plot the upper band $(F^{+}(t))$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Interpretation — what does the band tell us?\n",
    "\n",
    "The DKW band has the property that, with probability at least $(95\\%)$,\n",
    "\n",
    "$$\n",
    "F(t) \\in [F^{-}(t), F^{+}(t)] \\quad \\text{for all } t.\n",
    "$$\n",
    "\n",
    "In words:\n",
    "\n",
    "- The band tells us how much **uncertainty** there is in the empirical CDF as an estimate of the true residual distribution.\n",
    "- It is **uniform in \\(t\\)**: the guarantee holds simultaneously for all thresholds $(t)$.\n",
    "\n",
    "**What can we use it for?**\n",
    "\n",
    "- To assess how precisely we have estimated the distribution of residuals from a finite test sample.\n",
    "- To check whether a **candidate theoretical distribution** for residuals (e.g. normal distribution) lies mostly within this band; if the theoretical CDF goes outside the band, this suggests a poor fit.\n",
    "- More generally, to quantify uncertainty in distributional features of the residuals (e.g. quantiles, tail behavior) in a nonparametric way.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code from the above:\n",
    "\n",
    "# Part 6\n",
    "# Put the code for part 6 below this line\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Recompute predictions and residuals on the test set (for clarity)\n",
    "y_test_true = problem2_y_test\n",
    "y_test_pred = problem2_model.predict(problem2_X_test)\n",
    "residuals_test = y_test_true - y_test_pred\n",
    "\n",
    "# Number of test samples\n",
    "n = len(residuals_test)\n",
    "\n",
    "# 2. Sort residuals and build empirical CDF values\n",
    "residuals_sorted = np.sort(residuals_test)\n",
    "ecdf_values = np.arange(1, n + 1) / n  # k/n for k = 1,...,n\n",
    "\n",
    "# 3. Compute epsilon using the DKW inequality for 95% confidence\n",
    "alpha = 0.05\n",
    "epsilon = np.sqrt((1.0 / (2.0 * n)) * np.log(2.0 / alpha))\n",
    "\n",
    "# 4. Compute lower and upper confidence bands (clipped to [0, 1])\n",
    "lower_band = np.clip(ecdf_values - epsilon, 0.0, 1.0)\n",
    "upper_band = np.clip(ecdf_values + epsilon, 0.0, 1.0)\n",
    "\n",
    "print(f\"Number of test samples n = {n}\")\n",
    "print(f\"DKW epsilon (95% band)  = {epsilon:.4f}\")\n",
    "\n",
    "# 5. Plot the empirical CDF and the confidence band\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Empirical CDF as a step function\n",
    "plt.step(residuals_sorted, ecdf_values, where=\"post\", label=\"Empirical CDF of residuals\")\n",
    "\n",
    "# Confidence band as two lines\n",
    "plt.step(residuals_sorted, lower_band, where=\"post\", linestyle=\"--\", label=\"Lower 95% band\")\n",
    "plt.step(residuals_sorted, upper_band, where=\"post\", linestyle=\"--\", label=\"Upper 95% band\")\n",
    "\n",
    "plt.xlabel(\"Residual (true - predicted salary)\")\n",
    "plt.ylabel(\"Empirical CDF\")\n",
    "plt.title(\"Empirical CDF of Test Residuals with DKW 95% Confidence Band\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9105d9",
   "metadata": {},
   "source": [
    "### THIS IS ANOTHER WAY TO CALCULATE AND PLOT FOR THE EMPIRICAL DISTRIBUTION AND DOING THE DKW BAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "\n",
    "# Write the code to plot the empirical distribution function of the residual\n",
    "# with confidence bands with 95% confidence in this cell\n",
    "\n",
    "# THIS IS THE OPTIMAL WAY TO PLOT THE EMPIRICAL DISTRIBUTION FUNCTION WITH 95% CONFIDENCE BANDS:\n",
    "\n",
    "# from Utils import makeEDF,plotEDF\n",
    "from Utils import makeEDF, plotEDF\n",
    "\n",
    "y_pred = problem2_model.predict(problem2_X_test)\n",
    "\n",
    "y_true = problem2_y_test\n",
    "\n",
    "residuals = (y_true - y_pred).to_numpy()\n",
    "\n",
    "# 4) Build EDF using Utils.py\n",
    "edf_residuals = makeEDF(residuals)\n",
    "\n",
    "# 5) Plot EDF with DKW confidence band (95% confidence => alpha=0.95)\n",
    "plotEDF(\n",
    "    edf_residuals,\n",
    "    confidence_band=True,\n",
    "    alpha=0.95,\n",
    "    title=\"Empirical Distribution Function of residuals (DKW 95% band)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678c3c8",
   "metadata": {},
   "source": [
    "# This Section describes the difference between Inversion sampling and using simple Reject Accept:\n",
    "\n",
    "\n",
    "## PROBLEM 3 — Random variable generation from a given CDF\n",
    "\n",
    "We are given the CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x\\le 0\\\\\n",
    "e^x-1, & 0<x<\\ln(2)\\\\\n",
    "1, & x\\ge \\ln(2)\n",
    "\\end{cases}\n",
    "$$\n",
    "The support is the interval $(x\\in(0,\\ln 2))$.\n",
    "\n",
    "---\n",
    "\n",
    "# A) Step-by-step: Inversion sampling (construct 1000 samples)\n",
    "\n",
    "### Step 1: Identify the part of the CDF we can invert\n",
    "For $(0<x<\\ln 2)$,\n",
    "$$\n",
    "F(x)=e^x-1.\n",
    "$$\n",
    "Also note $(F(0)=0)$ and $(F(\\ln 2)=e^{\\ln 2}-1=2-1=1)$, so this maps exactly to $((0,1))$.\n",
    "\n",
    "### Step 2: Set $(U \\sim \\text{Unif}(0,1))$ and solve $(U = F(X))$\n",
    "Let $(U\\in(0,1))$. Set:\n",
    "$$\n",
    "U = e^X - 1.\n",
    "$$\n",
    "Solve for (X)$:\n",
    "$$\n",
    "U+1 = e^X \\quad\\Rightarrow\\quad X=\\ln(1+U).\n",
    "$$\n",
    "\n",
    "### Step 3: Sampling rule (the inverse CDF)\n",
    "$$\n",
    "X = F^{-1}(U) = \\ln(1+U), \\quad U\\sim \\text{Unif}(0,1).\n",
    "$$\n",
    "\n",
    "### Step 4: Generate 1000 samples\n",
    "1. Draw $(U_1,\\dots,U_{1000}\\stackrel{iid}{\\sim}\\text{Unif}(0,1))$.\n",
    "2. Compute $(X_i=\\ln(1+U_i))$.\n",
    "\n",
    "**Python (minimal):**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "U = np.random.rand(n)         # U_i ~ Uniform(0,1)\n",
    "X_inv = np.log(1 + U)         # X_i = ln(1+U_i)\n",
    "```\n",
    "\n",
    "## Step 5: Estimate mean and variance from the 1000 samples\n",
    "\n",
    "The sample mean is estimated as\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i,$\n",
    "\n",
    "and the sample variance is estimated as\n",
    "\n",
    "$\\hat{\\sigma}^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\hat{\\mu})^2.$\n",
    "\n",
    "```python\n",
    "    mean_hat = X_inv.mean()\n",
    "    var_hat  = X_inv.var(ddof=1)  # sample variance (divide by n-1)\n",
    "```\n",
    "\n",
    "## B) Step-by-step: Accept–Reject sampling (construct 1000 samples)\n",
    "\n",
    "### Step 1: Compute the target density $f(x)$\n",
    "\n",
    "Differentiate the CDF on the continuous part:\n",
    "\n",
    "$$\n",
    "f(x) = F'(x) = e^x, \\qquad 0 < x < \\ln 2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "f(x) = 0 \\quad \\text{otherwise}.\n",
    "$$\n",
    "\n",
    "Check normalization:\n",
    "\n",
    "$$\n",
    "\\int_{0}^{\\ln 2} e^x \\, dx\n",
    "= e^{\\ln 2} - 1\n",
    "= 2 - 1\n",
    "= 1,\n",
    "$$\n",
    "\n",
    "so $f$ is already a valid PDF.\n",
    "\n",
    "### Step 2: Choose an easy proposal density $g(x)$\n",
    "\n",
    "A simple choice is uniform on the same support:\n",
    "\n",
    "$$\n",
    "g(x) = \\text{Unif}(0, \\ln 2)\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "g(x) = \\frac{1}{\\ln 2}, \\quad 0 < x < \\ln 2.\n",
    "$$\n",
    "\n",
    "**Why this proposal?**\n",
    "\n",
    "- Very easy to sample from.\n",
    "- Same support as $f$.\n",
    "- Makes $f(x) / g(x)$ simple, giving a good acceptance rate.\n",
    "\n",
    "### Step 3: Find a constant $M$ so that $f(x) \\le M g(x)$\n",
    "\n",
    "Compute the ratio on $[0, \\ln 2]$:\n",
    "\n",
    "$$\n",
    "\\frac{f(x)}{g(x)} = \\frac{e^x}{1/\\ln 2} = e^x \\ln 2.\n",
    "$$\n",
    "\n",
    "This is maximized at $x = \\ln 2$, where $e^{\\ln 2} = 2$. Hence:\n",
    "\n",
    "$$\n",
    "\\max_{x \\in [0, \\ln 2]} \\frac{f(x)}{g(x)} = 2 \\ln 2.\n",
    "$$\n",
    "\n",
    "So we can choose\n",
    "\n",
    "$$\n",
    "M = 2 \\ln 2.\n",
    "$$\n",
    "\n",
    "### Step 4: Write the acceptance probability\n",
    "\n",
    "Accept–Reject accepts a proposal $Y \\sim g$ with probability\n",
    "\n",
    "$$\n",
    "\\frac{f(Y)}{M g(Y)}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\n",
    "\\frac{f(y)}{M g(y)}\n",
    "= \\frac{e^y}{(2 \\ln 2)\\cdot (1/\\ln 2)}\n",
    "= \\frac{e^y}{2}.\n",
    "$$\n",
    "\n",
    "So the acceptance test is:\n",
    "\n",
    "* Draw $U \\sim \\text{Unif}(0,1)$. Accept $Y$ if $U \\le e^Y/2$.\n",
    "\n",
    "### Step 5: Generate 1000 accepted samples and compute acceptance proportion\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Propose $Y \\sim \\text{Unif}(0, \\ln 2)$.\n",
    "2. Draw $U \\sim \\text{Unif}(0, 1)$.\n",
    "3. If $U \\le e^Y / 2$, accept $Y$ as a sample; otherwise reject and repeat.\n",
    "4. Keep going until you have 1000 accepted samples.\n",
    "5. Acceptance proportion = (number accepted) / (number proposed).\n",
    "\n",
    "**Expected acceptance proportion:**\n",
    "\n",
    "$$\n",
    "\\text{AccRate} = \\frac{1}{M} = \\frac{1}{2 \\ln 2} \\approx 0.721.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This is the final and complete code for both of the different approaches:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3 — Random variable generation from a given CDF\n",
    "# Target CDF:\n",
    "#   F(x)=0                  for x<=0\n",
    "#   F(x)=e^x - 1            for 0<x<ln(2)\n",
    "#   F(x)=1                  for x>=ln(2)\n",
    "#\n",
    "# Therefore the PDF on (0, ln(2)) is:\n",
    "#   f(x) = d/dx (e^x - 1) = e^x,   for 0<x<ln(2)\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "n = 1000\n",
    "# NOTE: Using np.random.* directly (no rng / default_rng).\n",
    "# If you want reproducibility, uncomment the next line:\n",
    "# np.random.seed(0)\n",
    "\n",
    "ln2 = np.log(2.0)\n",
    "\n",
    "# ============================================================\n",
    "# A) Inversion sampling\n",
    "#   U ~ Uniform(0,1)\n",
    "#   X = F^{-1}(U) = ln(1+U)\n",
    "#\n",
    "# Implemented as a loop (as in your example).\n",
    "# ============================================================\n",
    "\n",
    "samples_inv = []\n",
    "\n",
    "while len(samples_inv) < n:\n",
    "    U = np.random.uniform(0.0, 1.0)  # U ~ Uniform(0,1)\n",
    "    x = np.log(U + 1.0)              # X = ln(1+U)\n",
    "    samples_inv.append(x)\n",
    "\n",
    "# Convert to NumPy array for easy stats\n",
    "X_inv = np.array(samples_inv, dtype=float)\n",
    "\n",
    "# ---- Estimate mean and variance (sample variance uses ddof=1) ----\n",
    "mean_inv = X_inv.mean()\n",
    "var_inv  = X_inv.var(ddof=1)\n",
    "\n",
    "print(\"=== Inversion sampling ===\")\n",
    "print(\"n =\", n)\n",
    "print(\"Length of samples:\", len(samples_inv))\n",
    "print(\"sample mean =\", mean_inv)\n",
    "print(\"sample var  =\", var_inv)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# B) Accept–Reject sampling (written in the same style as your example)\n",
    "#\n",
    "# Target: f(x) = e^x on (0, ln2)\n",
    "#\n",
    "# Proposal: g(x) = Uniform(0, ln2)\n",
    "#   g(x) = 1/ln2 on (0, ln2)\n",
    "#\n",
    "# Bound:\n",
    "#   f(x)/g(x) = e^x ln2, maximized at x=ln2 -> 2 ln2\n",
    "#   so we can choose M = 2 ln2\n",
    "#\n",
    "# Acceptance probability:\n",
    "#   f(y)/(M g(y)) = e^y / 2\n",
    "# ============================================================\n",
    "\n",
    "def problem3_rejection(n_samples=1):\n",
    "    \"\"\"\n",
    "    Return a numpy array of length n_samples with samples from\n",
    "    f(x)=e^x on (0, ln(2)) using accept-reject with proposal Unif(0, ln(2)).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # 1) Sample from the proposal: Uniform(0, ln2)\n",
    "        y = np.random.uniform(0.0, ln2)\n",
    "\n",
    "        # 2) Compute acceptance probability w = f(y)/(M g(y)) = e^y / 2\n",
    "        w = np.exp(y) / 2.0\n",
    "\n",
    "        # 3) Sample U ~ Uniform(0,1) for acceptance\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "\n",
    "        # 4) Accept if u <= w\n",
    "        if u <= w:\n",
    "            samples.append(y)\n",
    "\n",
    "    return np.array(samples, dtype=float)\n",
    "\n",
    "# Generate n samples with accept-reject\n",
    "X_ar = problem3_rejection(n)\n",
    "\n",
    "# ---- Estimate mean and variance (sample variance uses ddof=1) ----\n",
    "mean_ar = X_ar.mean()\n",
    "var_ar  = X_ar.var(ddof=1)\n",
    "\n",
    "print(\"=== Accept–Reject sampling ===\")\n",
    "print(\"n =\", n)\n",
    "print(\"Length of samples:\", len(X_ar))\n",
    "print(\"sample mean =\", mean_ar)\n",
    "print(\"sample var  =\", var_ar)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# (Optional) Quick sanity checks\n",
    "# ============================================================\n",
    "\n",
    "print(\"Sanity checks:\")\n",
    "print(\"Inversion: min/max =\", X_inv.min(), X_inv.max(), \" (should be within (0, ln2) )\")\n",
    "print(\"A-R      : min/max =\", X_ar.min(),  X_ar.max(),  \" (should be within (0, ln2) )\")\n",
    "\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d18f3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c1e304",
   "metadata": {},
   "source": [
    "# This is information about the Accept Reject problems:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b329221",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2163b51",
   "metadata": {},
   "source": [
    "# This explains how Recall and Precision works for both class 0 and 1:\n",
    "\n",
    "\n",
    "### Precision and Recall (per class)\n",
    "\n",
    "Let the confusion matrix be:\n",
    "\n",
    "- **tn**: true class 0 predicted as 0  \n",
    "- **fp**: true class 0 predicted as 1  \n",
    "- **fn**: true class 1 predicted as 0  \n",
    "- **tp**: true class 1 predicted as 1  \n",
    "\n",
    "---\n",
    "\n",
    "### Class 1 (treat class `1` as the positive class)\n",
    "\n",
    "**Precision (class 1)**  \n",
    "Fraction of samples predicted as class 1 that are truly class 1:\n",
    "$$\n",
    "\\text{Precision}_1 = \\frac{tp}{tp + fp}\n",
    "$$\n",
    "\n",
    "**Recall (class 1)**  \n",
    "Fraction of true class 1 samples that are correctly predicted:\n",
    "$$\n",
    "\\text{Recall}_1 = \\frac{tp}{tp + fn}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Class 0 (treat class `0` as the positive class, one-vs-rest)\n",
    "\n",
    "When evaluating class 0, we treat class `0` as the “positive” class.\n",
    "\n",
    "- True positives for class 0: **tn**\n",
    "- Predicted positives for class 0: **tn + fn**\n",
    "- Actual positives for class 0: **tn + fp**\n",
    "\n",
    "**Precision (class 0)**  \n",
    "Fraction of samples predicted as class 0 that are truly class 0:\n",
    "$$\n",
    "\\text{Precision}_0 = \\frac{tn}{tn + fn}\n",
    "$$\n",
    "\n",
    "**Recall (class 0)**  \n",
    "Fraction of true class 0 samples that are correctly predicted:\n",
    "$$\n",
    "\\text{Recall}_0 = \\frac{tn}{tn + fp}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Class | Precision | Recall |\n",
    "|------|-----------|--------|\n",
    "| 0 | $( \\frac{tn}{tn + fn} $) | $( \\frac{tn}{tn + fp} $) |\n",
    "| 1 | $( \\frac{tp}{tp + fp} $) | $( \\frac{tp}{tp + fn} $) |\n",
    "\n",
    "---\n",
    "\n",
    "**Key intuition:**  \n",
    "- **Precision** conditions on what the model *predicted*  \n",
    "- **Recall** conditions on what the class *actually was*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb525ad6",
   "metadata": {},
   "source": [
    "### Choice of $( n )$ when using Hoeffding’s inequality\n",
    "\n",
    "Hoeffding’s inequality bounds the deviation of an empirical mean from its true\n",
    "expectation. Therefore, $( n )$ must be the **number of independent Bernoulli trials**\n",
    "used to compute that empirical mean.\n",
    "\n",
    "For precision and recall, each metric is an average of indicator variables\n",
    "(correct or incorrect classification) over a specific subset of samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is defined as:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{number of correct positive predictions}}{\\text{number of predicted positives}}\n",
    "$$\n",
    "\n",
    "Each predicted positive contributes one Bernoulli trial (correct or not). Therefore:\n",
    "$$\n",
    "n_{\\text{precision}} = \\text{number of predicted positives}\n",
    "$$\n",
    "\n",
    "- Class 1: $( n = tp + fp )$  \n",
    "- Class 0: $( n = tn + fn )$\n",
    "\n",
    "---\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall is defined as:\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{number of correctly classified positives}}{\\text{number of actual positives}}\n",
    "$$\n",
    "\n",
    "Each actual positive contributes one Bernoulli trial. Therefore:\n",
    "$$\n",
    "n_{\\text{recall}} = \\text{number of actual positives}\n",
    "$$\n",
    "\n",
    "- Class 1: $( n = tp + fn )$  \n",
    "- Class 0: $( n = tn + fp )$\n",
    "\n",
    "---\n",
    "\n",
    "### Key idea\n",
    "\n",
    "The denominator of precision or recall is exactly the number of samples over which\n",
    "the empirical average is computed. This is why it is the correct choice of $( n )$\n",
    "in Hoeffding’s inequality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecbc8c",
   "metadata": {},
   "source": [
    "### This is how you get the precision and recall using the confusion matrix:\n",
    "\n",
    "```python\n",
    "# === 2. Build the confusion matrix ===\n",
    "# We specify labels=[0, 1] to ensure the order is TN, FP, FN, TP when we ravel()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
    "print(\"tn:\", tn)\n",
    "print(\"fp:\", fp)\n",
    "print(\"fn:\", fn)\n",
    "print(\"tp:\", tp)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf8436",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0c8b69",
   "metadata": {},
   "source": [
    "# When to use C=_ or max_iter in LogisticRegression:\n",
    "\n",
    "## Logistic Regression: `C` vs `max_iter` — When and Why to Use Each\n",
    "\n",
    "### `max_iter` (controls convergence)\n",
    "`max_iter` specifies how many optimization steps the solver is allowed to take when fitting the logistic regression model.\n",
    "\n",
    "You should change `max_iter` when:\n",
    "- You get a `ConvergenceWarning`\n",
    "- You see messages like: \"TOTAL NO. OF ITERATIONS REACHED LIMIT\"\n",
    "- You are working with high-dimensional data such as text features from `CountVectorizer` or `TfidfVectorizer`\n",
    "\n",
    "Increasing `max_iter`:\n",
    "- Allows the optimizer to finish training\n",
    "- Does NOT change the model’s complexity\n",
    "- Does NOT affect regularization or learned coefficients\n",
    "\n",
    "Typical values:\n",
    "- 300–1000 for text classification\n",
    "\n",
    "Example:\n",
    "```python\n",
    "LogisticRegression(max_iter=1000)\n",
    "```\n",
    "\n",
    "\n",
    "**C (controls regularization strength)**\n",
    "\n",
    "**C** is the inverse of the regularization strength in logistic regression.\n",
    "\n",
    "**What C does:**\n",
    "\n",
    "- Smaller **C** → stronger regularization → simpler model  \n",
    "- Larger **C** → weaker regularization → more complex model  \n",
    "\n",
    "**You should change C when:**\n",
    "\n",
    "- The model is overfitting  \n",
    "- You want to control model complexity  \n",
    "- You are tuning performance, not fixing convergence issues  \n",
    "\n",
    "**Changing C:**\n",
    "\n",
    "- Directly affects learned coefficients  \n",
    "- Too small **C** can lead to underfitting  \n",
    "\n",
    "**Typical values:**\n",
    "\n",
    "- 0.1, 1.0, 10\n",
    "\n",
    "Example:\n",
    "```python\n",
    "LogisticRegression(C=0.1)\n",
    "```\n",
    "\n",
    "### Important rule of thumb\n",
    "* If you see convergence warnings, increase max_iter or change the solver\n",
    "* Do NOT change C just to silence convergence warnings\n",
    "* Use C only for regularization and performance tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1a2cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53083ba7",
   "metadata": {},
   "source": [
    "# This is the correct way to solve with Expected hitting time:\n",
    "### If we want to check that the answer is correct, then use the expected_hitting_time function above:\n",
    "\n",
    "## Goal\n",
    "We want the **expected number of steps until the first time we enter Suburbs (S)**, given we **start in Downtown (D)**.\n",
    "\n",
    "Let the states be:\n",
    "- **D** = Downtown\n",
    "- **S** = Suburbs (the *target*)\n",
    "- **C** = Countryside\n",
    "\n",
    "Transition matrix (row = current, column = next):\n",
    "$$\n",
    "P=\\begin{pmatrix}\n",
    "0.3 & 0.4 & 0.3\\\\\n",
    "0.2 & 0.5 & 0.3\\\\\n",
    "0.4 & 0.3 & 0.3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Define the **hitting time**\n",
    "$$\n",
    "T = \\inf\\{t\\ge 1 : X_t = S\\}.\n",
    "$$\n",
    "We want $(\\mathbb{E}[T \\mid X_0=D]$).\n",
    "\n",
    "---\n",
    "\n",
    "## Method 1 (clean “by hand” solution): first-step equations\n",
    "Let\n",
    "- $(h_D = \\mathbb{E}[T \\mid X_0=D])$\n",
    "- $(h_C = \\mathbb{E}[T \\mid X_0=C])$\n",
    "- $(h_S = \\mathbb{E}[T \\mid X_0=S])$\n",
    "\n",
    "Once we are in Suburbs, the hitting time is done, so:\n",
    "$$\n",
    "h_S = 0.\n",
    "$$\n",
    "\n",
    "### Equation for $(h_D)$\n",
    "From Downtown, in **one step** we go to:\n",
    "- D with prob 0.3\n",
    "- S with prob 0.4 (then we're done)\n",
    "- C with prob 0.3\n",
    "\n",
    "So by conditioning on the first step:\n",
    "$$\n",
    "h_D = 1 + 0.3\\,h_D + 0.4\\,h_S + 0.3\\,h_C.\n",
    "$$\n",
    "Since $(h_S=0)$:\n",
    "$$\n",
    "h_D = 1 + 0.3\\,h_D + 0.3\\,h_C.\n",
    "$$\n",
    "\n",
    "### Equation for $(h_C)$\n",
    "From Countryside, in one step we go to:\n",
    "- D with prob 0.4\n",
    "- S with prob 0.3 (done)\n",
    "- C with prob 0.3\n",
    "\n",
    "So:\n",
    "$$\n",
    "h_C = 1 + 0.4\\,h_D + 0.3\\,h_S + 0.3\\,h_C\n",
    "     = 1 + 0.4\\,h_D + 0.3\\,h_C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Solve the 2×2 system\n",
    "\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "From $(h_D = 1 + 0.3h_D + 0.3h_C)$:\n",
    "$$\n",
    "0.7h_D - 0.3h_C = 1.\n",
    "$$\n",
    "\n",
    "From $(h_C = 1 + 0.4h_D + 0.3h_C)$:\n",
    "$$\n",
    "-0.4h_D + 0.7h_C = 1.\n",
    "$$\n",
    "\n",
    "To avoid decimals, multiply both equations by 10:\n",
    "$$\n",
    "7h_D - 3h_C = 10,\n",
    "$$\n",
    "\n",
    "$$\n",
    "-4h_D + 7h_C = 10.\n",
    "$$\n",
    "\n",
    "### This can also be done using code: \n",
    "```python\n",
    "     # Coefficients for the linear system:\n",
    "     # From the equations:\n",
    "     #   7*h_D - 3*h_C = 10\n",
    "     #  -4*h_D + 7*h_C = 10\n",
    "\n",
    "     A = np.array([\n",
    "     [7, -3],\n",
    "     [-4, 7]\n",
    "     ])\n",
    "\n",
    "     b = np.array([10, 10])\n",
    "\n",
    "     # Solve A * x = b\n",
    "     solution = np.linalg.solve(A, b)\n",
    "\n",
    "     h_D, h_C = solution\n",
    "\n",
    "     problem1_ET = round(h_D, 1)\n",
    "     print(\"Expected steps to reach Suburbs starting from Downtown:\", problem1_ET)\n",
    "```\n",
    "\n",
    "Solve above equation by hand instead of code:\n",
    "- Multiply the first by 4: $(\\;28h_D - 12h_C = 40)$\n",
    "- Multiply the second by 7: $(\\;-28h_D + 49h_C = 70)$\n",
    "\n",
    "Add them:\n",
    "$$\n",
    "37h_C = 110 \\quad\\Rightarrow\\quad h_C = \\frac{110}{37}\\approx 2.973.\n",
    "$$\n",
    "\n",
    "Plug into $(7h_D - 3h_C = 10)$:\n",
    "$$\n",
    "7h_D = 10 + 3\\cdot \\frac{110}{37} = 10 + \\frac{330}{37} = \\frac{700}{37}\n",
    "\\Rightarrow\n",
    "h_D = \\frac{100}{37}\\approx 2.703.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Final answer (to 1 decimal)\n",
    "$$\n",
    "\\boxed{\\mathbb{E}[T \\mid X_0=D] = h_D = \\frac{100}{37}\\approx 2.7\\text{ steps}.}\n",
    "$$\n",
    "\n",
    "So, starting in Downtown, you expect to enter Suburbs for the first time after about **2.7 steps**.\n",
    "\n",
    "---\n",
    "\n",
    "## How this connects to the “truncate at 30” hint (motivation)\n",
    "A standard identity for nonnegative integer-valued $(T)$ is:\n",
    "$$\n",
    "\\mathbb{E}[T] = \\sum_{k\\ge 1}\\mathbb{P}(T\\ge k).\n",
    "$$\n",
    "\n",
    "If we “kill” the chain when it hits $(S)$, we only track the non-target states $(\\{D,C\\})$.\n",
    "The submatrix $(Q)$ of transitions **within $(\\{D,C\\})$** is:\n",
    "$$\n",
    "Q=\n",
    "\\begin{pmatrix}\n",
    "P(D\\to D) & P(D\\to C)\\\\\n",
    "P(C\\to D) & P(C\\to C)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0.3 & 0.3\\\\\n",
    "0.4 & 0.3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Starting from $(D)$, the probability we have **not** hit $(S)$ after $(k-1)$ steps is:\n",
    "$$\n",
    "\\mathbb{P}(T\\ge k) = e_D^\\top Q^{\\,k-1}\\mathbf{1},\n",
    "$$\n",
    "where $(e_D=(1,0)^\\top)$ and $(\\mathbf{1}=(1,1)^\\top)$.\n",
    "\n",
    "Then the truncated approximation is:\n",
    "$$\n",
    "\\mathbb{E}[T]\\approx \\sum_{k=1}^{30} \\mathbb{P}(T\\ge k).\n",
    "$$\n",
    "\n",
    "In this specific problem, the *exact* value is already $(\\frac{100}{37}\\approx 2.7)$, and the tail beyond 30 is negligible because repeatedly avoiding $(S)$ has exponentially decreasing probability.\n",
    "(That’s why computing up to 30 terms is enough for 1 decimal.)\n",
    "\n",
    "**But for full points, the cleanest is the exact first-step equation solution above.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492dff2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9855a558",
   "metadata": {},
   "source": [
    "# This is a similar exercise as the one above, but with 4 states instead:\n",
    "\n",
    "# Expected time to reach the Workshop (hand solution)\n",
    "\n",
    "We consider the Markov chain with states\n",
    "\n",
    "- **D** = Downtown  \n",
    "- **S** = Suburbs  \n",
    "- **C** = Countryside  \n",
    "- **W** = Workshop (target, absorbing)\n",
    "\n",
    "and transition matrix\n",
    "$$\n",
    "P=\\begin{pmatrix}\n",
    "0.3 & 0.7 & 0 & 0\\\\\n",
    "0.2 & 0.5 & 0.3 & 0\\\\\n",
    "0 & 0 & 0.5 & 0.5\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Goal\n",
    "Compute the expected number of steps until the first time the chain enters the **Workshop**, starting from **Downtown**.\n",
    "\n",
    "Define the hitting time\n",
    "$$\n",
    "T = \\inf\\{t \\ge 1 : X_t = W\\}.\n",
    "$$\n",
    "We want $\\mathbb{E}[T \\mid X_0 = D]$.\n",
    "\n",
    "---\n",
    "\n",
    "## First-step equations\n",
    "\n",
    "Let\n",
    "$$\n",
    "h_D = \\mathbb{E}[T \\mid X_0=D], \\quad\n",
    "h_S = \\mathbb{E}[T \\mid X_0=S], \\quad\n",
    "h_C = \\mathbb{E}[T \\mid X_0=C], \\quad\n",
    "h_W = \\mathbb{E}[T \\mid X_0=W].\n",
    "$$\n",
    "\n",
    "Since $W$ is the target state,\n",
    "$$\n",
    "h_W = 0.\n",
    "$$\n",
    "\n",
    "### Equation for $h_D$\n",
    "From Downtown, the chain goes to:\n",
    "- $D$ with probability $0.3$\n",
    "- $S$ with probability $0.7$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "h_D = 1 + 0.3\\,h_D + 0.7\\,h_S.\n",
    "$$\n",
    "\n",
    "### Equation for $h_S$\n",
    "From Suburbs, the chain goes to:\n",
    "- $D$ with probability $0.2$\n",
    "- $S$ with probability $0.5$\n",
    "- $C$ with probability $0.3$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "h_S = 1 + 0.2\\,h_D + 0.5\\,h_S + 0.3\\,h_C.\n",
    "$$\n",
    "\n",
    "### Equation for $h_C$\n",
    "From Countryside, the chain goes to:\n",
    "- $C$ with probability $0.5$\n",
    "- $W$ with probability $0.5$\n",
    "\n",
    "Thus,\n",
    "$$\n",
    "h_C = 1 + 0.5\\,h_C + 0.5\\,h_W\n",
    "     = 1 + 0.5\\,h_C.\n",
    "$$\n",
    "Rearranging,\n",
    "$$\n",
    "0.5h_C = 1 \\quad\\Rightarrow\\quad h_C = 2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Solving the system\n",
    "\n",
    "Substitute $h_C = 2$ into the equation for $h_S$:\n",
    "$$\n",
    "h_S = 1 + 0.2h_D + 0.5h_S + 0.3\\cdot 2\n",
    "     = 1.6 + 0.2h_D + 0.5h_S.\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "0.5h_S = 1.6 + 0.2h_D\n",
    "\\quad\\Rightarrow\\quad\n",
    "h_S = 3.2 + 0.4h_D.\n",
    "$$\n",
    "\n",
    "Now rearrange the equation for $h_D$:\n",
    "$$\n",
    "h_D = 1 + 0.3h_D + 0.7h_S\n",
    "\\quad\\Rightarrow\\quad\n",
    "0.7h_D = 1 + 0.7h_S\n",
    "\\quad\\Rightarrow\\quad\n",
    "h_D - h_S = \\frac{10}{7}.\n",
    "$$\n",
    "\n",
    "Substitute $h_S = 3.2 + 0.4h_D$:\n",
    "$$\n",
    "h_D - (3.2 + 0.4h_D) = \\frac{10}{7}\n",
    "$$\n",
    "$$\n",
    "0.6h_D - 3.2 = \\frac{10}{7}\n",
    "$$\n",
    "$$\n",
    "0.6h_D = 3.2 + \\frac{10}{7}\n",
    "       = \\frac{162}{35}\n",
    "$$\n",
    "$$\n",
    "h_D = \\frac{162}{35} \\cdot \\frac{1}{0.6}\n",
    "     = \\frac{54}{7}\n",
    "     \\approx 7.714.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Final answer (to one decimal)\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbb{E}[T \\mid X_0=D] = \\frac{54}{7} \\approx 7.7 \\text{ steps}.}\n",
    "$$\n",
    "\n",
    "Thus, starting from Downtown, the expected number of steps until the truck first reaches the Workshop is approximately **7.7 steps**.\n",
    "\n",
    "---\n",
    "\n",
    "## Connection to the truncation hint\n",
    "\n",
    "For a nonnegative integer-valued hitting time $T$,\n",
    "$$\n",
    "\\mathbb{E}[T] = \\sum_{k \\ge 1} \\mathbb{P}(T \\ge k).\n",
    "$$\n",
    "\n",
    "If we kill the chain upon reaching $W$, the submatrix $Q$ of transitions among the non-target states $\\{D,S,C\\}$ is\n",
    "$$\n",
    "Q=\n",
    "\\begin{pmatrix}\n",
    "0.3 & 0.7 & 0\\\\\n",
    "0.2 & 0.5 & 0.3\\\\\n",
    "0 & 0 & 0.5\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Starting from $D$,\n",
    "$$\n",
    "\\mathbb{P}(T \\ge k) = e_D^\\top Q^{\\,k-1}\\mathbf{1},\n",
    "$$\n",
    "where $e_D=(1,0,0)^\\top$ and $\\mathbf{1}=(1,1,1)^\\top$.\n",
    "\n",
    "Since the spectral radius of $Q$ is strictly less than $1$, these probabilities decay exponentially, so truncating the sum at $k=50$ is sufficient to obtain accuracy to one decimal place. However, in this case the exact value is already given by the first-step equation solution above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24674e62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f4e5870",
   "metadata": {},
   "source": [
    "# This is how you use Precision and Recall with Hoeffding confidence intervals:\n",
    "\n",
    "```python\n",
    "\n",
    "# This is the other way to simply use precision and recall score functions:\n",
    "# We can only have values between 0 and 1, so a = 0, and b=1\n",
    "\n",
    "a,b = 0,1\n",
    "alpha = 0.05 # For 95% confidence interval\n",
    "\n",
    "def hoeffding(p, n):\n",
    "    epsilon = (b-a)*np.sqrt(np.log(alpha/2) / (-2*n))\n",
    "    lower = max(0, p - epsilon)\n",
    "    upper = min(1, p + epsilon)\n",
    "    return (lower, upper)\n",
    "\n",
    "# To be able to calculate the precision and recall we need to extrace the TP, TN... variables.\n",
    "# This can be done by using a covariance matrix:\n",
    "y_true = problem3_y_test\n",
    "y_pred = problem3_model.predict(problem3_X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "precision1 = precision_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "recall0 = recall_score(y_true, y_pred, pos_label=0)\n",
    "recall1 = recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "precision0_n = np.sum(y_pred == 0)\n",
    "precision1_n = np.sum(y_pred == 1)\n",
    "\n",
    "recall0_n = np.sum(y_true == 0)\n",
    "recall1_n = np.sum(y_true == 1)\n",
    "\n",
    "\n",
    "problem3_precision_0_score = hoeffding(precision0, precision0_n)\n",
    "problem3_precision_1_score = hoeffding(precision1, precision1_n)\n",
    "\n",
    "problem3_recall_0_score = hoeffding(recall0, recall0_n)\n",
    "problem3_recall_1_score = hoeffding(recall1, recall1_n)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(f\"This is the intervals: \\nPrecision 0: {problem3_precision_0_score} \\nRecall 0: {problem3_recall_0_score} \\nPrecision 1: {problem3_precision_1_score} \\nRecall 1: {problem3_recall_1_score}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6c82d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f58fbaaa",
   "metadata": {},
   "source": [
    "# This is about Inversion sampling, Accept Reject as well as choosing the best g(x):\n",
    "\n",
    "# How to think about **Inversion sampling** (given $F$)\n",
    "\n",
    "## Core idea (Theorem 6.14)\n",
    "If $U \\sim \\mathrm{Uniform}([0,1])$ and $F$ is a CDF, then\n",
    "$X = F^{-1}(U)$ has CDF $F$. \n",
    "\n",
    "### Step-by-step mindset\n",
    "1. **Check $F$ is a valid CDF**  \n",
    "   Non-decreasing, right-continuous, $\\lim_{x\\to -\\infty}F(x)=0$, $\\lim_{x\\to\\infty}F(x)=1$.\n",
    "\n",
    "2. **Find the quantile function**  \n",
    "   Use the *quantile* definition\n",
    "   $F^{-1}(u) = \\inf\\{x : F(x)\\ge u\\}$, $u\\in(0,1)$. \n",
    "   If $F$ is strictly increasing on its support, this is just the usual inverse.\n",
    "\n",
    "3. **Generate uniforms**  \n",
    "   Draw $U_1,\\dots,U_n \\overset{iid}{\\sim}\\mathrm{Uniform}([0,1])$.\n",
    "\n",
    "4. **Transform**  \n",
    "   Set $X_i = F^{-1}(U_i)$.\n",
    "\n",
    "5. **(Sanity check)**  \n",
    "   The samples should live on the support of $F$, and hist/EDF should look right.\n",
    "\n",
    "---\n",
    "\n",
    "## Example A (your exercise): Inversion for the given piecewise $F$\n",
    "You are given\n",
    "$F(x)=\n",
    "\\begin{cases}\n",
    "0, & x\\le 0\\\\\n",
    "e^x-1, & 0<x<\\ln(2)\\\\\n",
    "1, & x\\ge \\ln(2)\n",
    "\\end{cases}$\n",
    "\n",
    "### 1) Compute $F^{-1}(u)$\n",
    "For $0<u<1$, we are in the middle branch:\n",
    "$u = e^x - 1$  \n",
    "$u+1 = e^x$  \n",
    "$x = \\ln(1+u)$\n",
    "\n",
    "So the quantile is\n",
    "$F^{-1}(u)=\\ln(1+u)$ for $u\\in(0,1)$.\n",
    "\n",
    "### 2) Sampling rule (1000 samples)\n",
    "- Draw $U_1,\\dots,U_{1000}\\sim \\mathrm{Uniform}([0,1])$\n",
    "- Output $X_i=\\ln(1+U_i)$\n",
    "\n",
    "This automatically produces values in $[0,\\ln(2)]$.\n",
    "\n",
    "---\n",
    "\n",
    "## Example B (classic, from notes): Exponential inversion\n",
    "If $F(x)=1-e^{-\\lambda x}$ for $x\\ge 0$, then\n",
    "- $u = 1-e^{-\\lambda x}$\n",
    "- $e^{-\\lambda x}=1-u$\n",
    "- $x = -\\frac{1}{\\lambda}\\ln(1-u)$\n",
    "\n",
    "so $X=-\\frac{1}{\\lambda}\\ln(1-U)$ has the exponential distribution. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# How to think about **Accept–Reject sampling** (given $f$)\n",
    "\n",
    "## Core idea (Algorithm 1 + condition)\n",
    "You want samples from a **target density** $f(x)$. Choose an easy **proposal density** $g(x)$ and a constant $M>0$ such that\n",
    "$f(x)\\le M g(x)\\ \\ \\text{for all }x.$ \n",
    "\n",
    "Then the algorithm:\n",
    "1. Sample $X\\sim g$\n",
    "2. Sample $U\\sim \\mathrm{Uniform}([0,1])$\n",
    "3. Accept $X$ if $U \\le r(X)$ where\n",
    "   $r(x)=\\frac{f(x)}{M g(x)}\\in[0,1]$ \n",
    "\n",
    "The accepted sample has density $f$. \n",
    "\n",
    "### Step-by-step mindset\n",
    "1. **Write down the target density $f(x)$** (must integrate to $1$).\n",
    "2. **Pick a proposal $g(x)$** that is:\n",
    "   - easy to sample from\n",
    "   - has support covering where $f(x)>0$\n",
    "   - “similar shape” to $f$ to avoid huge rejection\n",
    "3. **Find a valid $M$** by bounding the ratio:\n",
    "   $M \\ge \\sup_x \\frac{f(x)}{g(x)}$\n",
    "4. **Compute acceptance function**:\n",
    "   $r(x)=\\frac{f(x)}{Mg(x)}$\n",
    "5. **Run accept/reject until you collect 1000 accepted samples**.\n",
    "6. **Acceptance rate**\n",
    "   The acceptance probability is:\n",
    "   \n",
    "   $P(\\text{accept})=\\int r(x)g(x)\\,dx = \\frac{1}{M}\\int f(x)\\,dx = \\frac{1}{M}$ \n",
    "   \n",
    "   (since $\\int f=1$), so the expected acceptance proportion is about $1/M$.\n",
    "\n",
    "---\n",
    "\n",
    "## Example C (from notes): Accept–Reject for $f(x)=\\frac{1}{2}\\cos(x)$ on $(-\\pi/2,\\pi/2)$\n",
    "Target:\n",
    "$f(x)=\\frac{1}{2}\\cos(x)$ for $-\\pi/2 < x < \\pi/2$, and $0$ otherwise. \n",
    "\n",
    "### 1) Choose proposal\n",
    "A natural choice: $g$ uniform on the same interval:\n",
    "$g(x)=\\frac{1}{\\pi}$ for $-\\pi/2<x<\\pi/2$.\n",
    "\n",
    "Why? Same support, trivial to sample.\n",
    "\n",
    "### 2) Find $M$\n",
    "Compute the ratio:\n",
    "$\\frac{f(x)}{g(x)}=\\frac{\\frac{1}{2}\\cos(x)}{1/\\pi}=\\frac{\\pi}{2}\\cos(x)$\n",
    "The maximum of $\\cos(x)$ on that interval is $1$ at $x=0$, so\n",
    "$M=\\frac{\\pi}{2}$.\n",
    "\n",
    "### 3) Acceptance rule\n",
    "$r(x)=\\frac{f(x)}{Mg(x)}=\n",
    "\\frac{\\frac{1}{2}\\cos(x)}{(\\pi/2)(1/\\pi)}=\\cos(x)$\n",
    "So:\n",
    "- sample $X\\sim \\mathrm{Uniform}([-\\pi/2,\\pi/2])$\n",
    "- sample $U\\sim \\mathrm{Uniform}([0,1])$\n",
    "- accept if $U\\le \\cos(X)$ or mainly if $U\\le r(x)$\n",
    "\n",
    "### 4) Expected acceptance proportion\n",
    "$1/M = 2/\\pi \\approx 0.6366$.\n",
    "\n",
    "---\n",
    "\n",
    "# Accept–Reject when you are given **$F$ instead of $f$**\n",
    "Accept–Reject (Algorithm 1) is stated for **densities**. \n",
    "So if you are given $F$, your first move is usually:\n",
    "\n",
    "## Step-by-step mindset (given $F$)\n",
    "1. **Differentiate** where possible to get the density:\n",
    "   $f(x)=F'(x)$ on intervals where $F$ is smooth.\n",
    "2. **Identify the support** (where $f(x)>0$).\n",
    "3. Then do the usual accept–reject steps: choose $g$, find $M$, compute $r(x)$, run the sampler.\n",
    "\n",
    "---\n",
    "\n",
    "## Example D (your exercise again, but now Accept–Reject using $F$)\n",
    "Given the same $F$:\n",
    "$F(x)=\n",
    "\\begin{cases}\n",
    "0, & x\\le 0\\\\\n",
    "e^x-1, & 0<x<\\ln(2)\\\\\n",
    "1, & x\\ge \\ln(2)\n",
    "\\end{cases}$\n",
    "\n",
    "### 1) Convert $F$ to $f$\n",
    "On $0<x<\\ln(2)$:\n",
    "\n",
    "$f(x)=F'(x)=e^x$\n",
    "\n",
    "Outside: $f(x)=0$.\n",
    "\n",
    "So the target density is:\n",
    "$$\n",
    "f(x)= e^x \\text{ on the interval } {(0,\\ln(2))}\n",
    "$$.\n",
    "\n",
    "### 2) Choose a proposal $g$\n",
    "A simple choice:\n",
    "$g(x)=\\frac{1}{\\ln(2)} \\text{ for the interval } {(0,\\ln(2))}$ (uniform on the support).\n",
    "\n",
    "Why? Easy to sample, same support.\n",
    "\n",
    "### 3) Find $M$\n",
    "We need $f(x)\\le Mg(x)$, i.e.\n",
    "\n",
    "$e^x \\le M\\cdot \\frac{1}{\\ln(2)}$\n",
    "\n",
    "So\n",
    "\n",
    "$M \\ge e^x \\ln(2)$ for all $x\\in(0,\\ln(2))$.\n",
    "\n",
    "\n",
    "The maximum is at $x=\\ln(2)$:\n",
    "\n",
    "$\\sup e^x \\ln(2) = e^{\\ln(2)} * \\ln(2) = 2 * \\ln(2)$\n",
    "\n",
    "So take\n",
    "$M = 2\\ln(2)$.\n",
    "\n",
    "### 4) Acceptance function\n",
    "$r(x)=\\frac{f(x)}{Mg(x)}\n",
    "=\\frac{e^x}{(2\\ln(2))\\cdot (1/\\ln(2))}\n",
    "=\\frac{e^x}{2}$\n",
    "\n",
    "So:\n",
    "- sample $X\\sim \\mathrm{Uniform}([0,\\ln(2)])$\n",
    "- sample $U\\sim \\mathrm{Uniform}([0,1])$\n",
    "- accept if $U \\le \\frac{e^X}{2}$\n",
    "\n",
    "### 5) Expected acceptance proportion\n",
    "$1/M = \\frac{1}{2\\ln(2)} \\approx 0.7213$ \n",
    "\n",
    "---\n",
    "\n",
    "# Quick “which method should I use?”\n",
    "- If you have a nice closed-form $F^{-1}$: **Inversion is usually best** (simple + no rejection). \n",
    "- If $F^{-1}$ is ugly / expensive but you can bound a density: **Accept–Reject**. \n",
    "- If you’re given only $F$ and not $f$: try to get $f=F'$ first, then Accept–Reject. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b70c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11bf4119",
   "metadata": {},
   "source": [
    "# We keep going:\n",
    "\n",
    "## Deciding the best \\( g(x) \\)\n",
    "\n",
    "When using Accept–Reject sampling, the **runtime is dominated by how many samples get rejected**.  \n",
    "To make the algorithm fast (for example, when the exercise says it must run in under 3 seconds),  \n",
    "you must choose \\( g(x) \\) so that the rejection rate is as small as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What determines the speed\n",
    "In Accept–Reject sampling, the acceptance probability is\n",
    "$P(\\text{accept}) = 1/M$\n",
    "where\n",
    "$M = \\sup_x \\frac{f(x)}{g(x)}$\n",
    "\n",
    "This means:\n",
    "- Expected number of proposals per accepted sample is $M$\n",
    "- Smaller $M$ gives faster sampling\n",
    "- Choosing $g(x)$ is really about **minimizing $M$**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why normalization alone is not enough\n",
    "Making sure $g(x)$ integrates to 1 is **necessary**, but it does **not** make it efficient.\n",
    "\n",
    "A uniform proposal may be valid:\n",
    "$g(x) = c$\n",
    "with\n",
    "$c \\cdot (\\text{length of support}) = 1$\n",
    "\n",
    "But if $f(x)$ is highly peaked, this choice gives a large $M$, which causes many rejections and slow runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Principle for choosing a fast proposal\n",
    "To make Accept–Reject fast:\n",
    "\n",
    "> Choose $g(x)$ to have the **same support and a similar shape as $f(x)$**, while still being easy to sample from.\n",
    "\n",
    "This reduces the maximum ratio $f(x)/g(x)$ and therefore reduces $M$.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Step-by-step strategy to decide \\( g(x) \\)\n",
    "\n",
    "#### Step 1: Analyze the shape of \\( f(x) \\)\n",
    "Ask:\n",
    "- Is $f(x)$ increasing or decreasing?\n",
    "- Is it symmetric?\n",
    "- Where is its maximum?\n",
    "- Is it flat or sharply peaked?\n",
    "\n",
    "You should sketch $f(x)$ mentally.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Choose a proposal with matching shape\n",
    "Good choices:\n",
    "- Increasing $f(x)$ → increasing $g(x)$\n",
    "- Symmetric $f(x)$ → symmetric $g(x)$\n",
    "- Unimodal $f(x)$ → unimodal $g(x)$\n",
    "\n",
    "Avoid flat $g(x)$ for sharply peaked $f(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Ensure domination\n",
    "You must guarantee:\n",
    "$f(x) \\le M g(x)$ for all $x$\n",
    "\n",
    "This means $M g(x)$ must lie above $f(x)$ everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Minimize \\( M \\)\n",
    "Compute or tightly bound:\n",
    "$M = \\sup_x \\frac{f(x)}{g(x)}$\n",
    "\n",
    "This is the **most important step** for speed.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example: improving a slow proposal\n",
    "\n",
    "Suppose the target density is\n",
    "$f(x) = e^x$ on $0 < x < \\ln(2)$\n",
    "\n",
    "---\n",
    "\n",
    "#### Poor choice (slow)\n",
    "Choose a uniform proposal:\n",
    "$g_1(x) = \\frac{1}{\\ln(2)}$ on $0 < x < \\ln(2)$\n",
    "\n",
    "Then:\n",
    "$\\sup_x \\frac{f(x)}{g_1(x)} = 2 \\ln(2)$\n",
    "\n",
    "So:\n",
    "$M_1 = 2 \\ln(2)$  \n",
    "Acceptance rate $\\approx 1/(2 \\ln(2)) \\approx 0.72$\n",
    "\n",
    "---\n",
    "\n",
    "#### Better choice (faster)\n",
    "Choose a proposal with the same shape:\n",
    "$g_2(x) = \\frac{e^x}{\\int_0^{\\ln(2)} e^t dt}\n",
    "       = \\frac{e^x}{2 - 1}\n",
    "       = e^x$\n",
    "\n",
    "Here:\n",
    "$\\frac{f(x)}{g_2(x)} = 1$ for all $x$\n",
    "\n",
    "So:\n",
    "$M_2 = 1$  \n",
    "Acceptance rate $= 1$\n",
    "\n",
    "This gives **no rejections at all** and is maximally fast.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Why this matters in time-limited exercises\n",
    "If an exercise says *“must not take longer than 3 seconds”*, the expected reasoning is:\n",
    "\n",
    "- Identify the shape of $f(x)$\n",
    "- Avoid uniform proposals if $f(x)$ is not flat\n",
    "- Choose $g(x)$ to closely match $f(x)$\n",
    "- Argue that this minimizes $M$ and speeds up sampling\n",
    "\n",
    "You are **not expected to find the mathematically optimal proposal**, only one that is clearly efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Final takeaway\n",
    "> The best and fastest $g(x)$ is **not** the one that is easiest to normalize,  \n",
    "> but the one that makes $\\sup_x f(x)/g(x)$ as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fe9c1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2398d637",
   "metadata": {},
   "source": [
    "# This text simply answers the question: \"explain what proposal distribution you chose and why you can choose it.\"\n",
    "\n",
    "\n",
    "\n",
    "### Choice of proposal distribution and acceptance rate\n",
    "\n",
    "From the given CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x \\le 0 \\\\\n",
    "e^x - 1, & 0 < x < \\ln(2) \\\\\n",
    "1, & x \\ge \\ln(2),\n",
    "\\end{cases}\n",
    "$$\n",
    "the corresponding density is\n",
    "$$\n",
    "f(x)=F'(x)=e^x \\quad \\text{for } 0<x<\\ln(2),\n",
    "$$\n",
    "and $f(x)=0$ otherwise.\n",
    "\n",
    "I chose the proposal distribution to be **uniform on the support of $f$**:\n",
    "$$\n",
    "g(x)=\\frac{1}{\\ln(2)} \\quad \\text{for } 0<x<\\ln(2),\n",
    "$$\n",
    "and $g(x)=0$ otherwise.\n",
    "\n",
    "This choice is valid because $g(x)$ integrates to 1, has the same support as $f(x)$ (so no proposals are wasted where $f(x)=0$), and is very easy to sample from.\n",
    "\n",
    "To apply the accept–reject algorithm, we need $M$ such that $f(x)\\le M g(x)$ for all $x$ in the support. We compute\n",
    "$$\n",
    "\\frac{f(x)}{g(x)} = e^x \\ln(2),\n",
    "$$\n",
    "whose maximum on $(0,\\ln(2))$ is attained at $x=\\ln(2)$, giving\n",
    "$$\n",
    "M = 2\\ln(2).\n",
    "$$\n",
    "\n",
    "The acceptance probability is therefore\n",
    "$$\n",
    "\\alpha(x)=\\frac{f(x)}{M g(x)}=\\frac{e^x}{2}.\n",
    "$$\n",
    "\n",
    "The empirical proportion of accepted samples is computed as\n",
    "$$\n",
    "\\hat p=\\frac{1000}{\\text{number of proposals}},\n",
    "$$\n",
    "which should be close to the theoretical acceptance rate\n",
    "$$\n",
    "p=\\frac{1}{M}=\\frac{1}{2\\ln(2)}\\approx 0.72.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d087a89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13bffd93",
   "metadata": {},
   "source": [
    "-----\n",
    "## How to think when choosing a proposal distribution \\( g(x) \\) in rejection sampling\n",
    "\n",
    "When facing a difficult rejection sampling problem, do **not** start by guessing formulas. Instead, follow this reasoning process.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Locate where the probability mass is\n",
    "Ask: *Where does the distribution actually concentrate its mass?*\n",
    "\n",
    "Look for terms like:\n",
    "- $( e^{-1/x} )$, $( e^{x^2} )$, $( x^\\alpha )$\n",
    "- behavior near boundaries (0, infinity, endpoints)\n",
    "\n",
    "**Rule:**  \n",
    "Your proposal must put mass where the target puts mass.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Identify the dominant term\n",
    "Ignore constants and lower-order factors at first.\n",
    "\n",
    "Ask: *Which part of the density controls the shape?*\n",
    "\n",
    "Examples:\n",
    "- $( e^{-x} )$ → exponential\n",
    "- $( e^{-x^2} )$ → Gaussian-like\n",
    "- $( e^{-1/x} )$ → strong boundary concentration\n",
    "\n",
    "**Rule:**  \n",
    "Match the dominant term first; fix the rest using rejection.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Consider a change of variables\n",
    "If the density contains:\n",
    "- $( 1/x )$, $( \\log x )$, or sharp boundary behavior\n",
    "\n",
    "Ask: *Would this look simpler in another variable?*\n",
    "\n",
    "Common transformations:\n",
    "- $( Y = 1/X )$ for $( e^{-1/x} )$\n",
    "- $( Y = \\log X )$ for multiplicative scales\n",
    "\n",
    "**Rule:**  \n",
    "If the density is ugly in $( x )$, change coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Choose a proposal that is easy to sample from\n",
    "Good proposals:\n",
    "- Uniform (only if the target is fairly flat)\n",
    "- Exponential or shifted exponential\n",
    "- Gaussian\n",
    "\n",
    "Bad proposals:\n",
    "- hard-to-invert CDFs\n",
    "- complicated expressions\n",
    "\n",
    "**Rule:**  \n",
    "If sampling from $( g(x) )$ is hard, you chose the wrong proposal.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Immediately check the ratio $( f(x)/g(x) )$\n",
    "Before coding, compute:\n",
    "$$\n",
    "\\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "Ask:\n",
    "- Is it bounded?\n",
    "- Does it simplify?\n",
    "- Do exponentials cancel?\n",
    "\n",
    "**Rule:**  \n",
    "If exponentials cancel and the ratio is simple, the proposal is good.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Estimate the rejection constant $( M )$\n",
    "Check where the maximum of $( f(x)/g(x) )$ occurs:\n",
    "- often at boundaries\n",
    "- sometimes at symmetry points\n",
    "\n",
    "Good signs:\n",
    "- $( M \\approx 1 )$: very efficient\n",
    "- $( M \\gg 10 )$: rethink your proposal\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Key mindset\n",
    "Rejection sampling is not mechanical algebra — it is **distribution engineering**:\n",
    "- understand the shape\n",
    "- match it intelligently\n",
    "- use rejection only to correct small differences\n",
    "\n",
    "---\n",
    "\n",
    "### One-line checklist (exam-ready)\n",
    "1. Where is the mass?  \n",
    "2. What term dominates?  \n",
    "3. Should I change variables?  \n",
    "4. Can I sample from $( g )$ easily?  \n",
    "5. Is $( f/g )$ bounded?  \n",
    "6. Is $( M )$ small?\n",
    "\n",
    "If all answers are yes, your choice of $( g(x) )$ is good.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd1152",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e79cf3f",
   "metadata": {},
   "source": [
    "# This is how we estimate M from code:\n",
    "\n",
    "```python\n",
    "samples = []\n",
    "n_samples = 1000\n",
    "total_samples = 0\n",
    "\n",
    "x_vals = np.random.uniform(0, np.log(2), 10000)\n",
    "fx = np.exp(x_vals)\n",
    "gx = 1 / (np.log(2))\n",
    "\n",
    "M_est = max(fx / gx)\n",
    "M_true = 2*np.log(2)\n",
    "\n",
    "print(\"M estimate is: \", M_est)\n",
    "print(\"True M is: \", M_true)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7794f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ec8ddfc",
   "metadata": {},
   "source": [
    "# This is how you can always find a better g(x) for the Accept-Reject problems:\n",
    "\n",
    "## Problem 1.5 — how to choose a *better* proposal $g$\n",
    "\n",
    "We are given the CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x \\le 0, \\\\\n",
    "20x e^{20-1/x}, & 0 < x < \\frac{1}{20}, \\\\\n",
    "1, & x \\ge \\frac{1}{20}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 1) First convert the CDF to a PDF\n",
    "For $0 < x < \\frac{1}{20}$,\n",
    "$$\n",
    "f_X(x) = F'(x).\n",
    "$$\n",
    "Differentiate:\n",
    "$$\n",
    "F(x) = 20x e^{20-1/x}.\n",
    "$$\n",
    "Using the product rule,\n",
    "$$\n",
    "f_X(x)=20e^{20-1/x} + 20x e^{20-1/x}\\cdot \\frac{1}{x^2}\n",
    "      = 20 e^{20-1/x}\\left(1+\\frac{1}{x}\\right).\n",
    "$$\n",
    "\n",
    "### 2) Why Uniform $(0,1/20)$ is not great\n",
    "If we choose $g_X(x)=20$ on $(0,1/20)$ (Uniform proposal), then\n",
    "$$\n",
    "\\frac{f_X(x)}{g_X(x)} = e^{20-1/x}\\left(1+\\frac{1}{x}\\right).\n",
    "$$\n",
    "This ratio is maximized at $x=1/20$:\n",
    "$$\n",
    "e^{20-20}(1+20)=21,\n",
    "$$\n",
    "so we must take $M=21$, giving acceptance rate about $1/21 \\approx 4.8\\%$ (slow).\n",
    "\n",
    "### 3) The trick: transform variables to make the distribution easy\n",
    "Notice the term $e^{20-1/x}$ contains $1/x$, so let\n",
    "$$\n",
    "Y=\\frac{1}{X}.\n",
    "$$\n",
    "Then $Y \\in (20,\\infty)$ since $0<X<1/20$.\n",
    "\n",
    "Use change-of-variables:\n",
    "$$\n",
    "f_Y(y) = f_X(1/y)\\left|\\frac{d(1/y)}{dy}\\right| = f_X(1/y)\\cdot \\frac{1}{y^2}.\n",
    "$$\n",
    "Compute $f_X(1/y)$:\n",
    "- $e^{20-1/(1/y)} = e^{20-y}$\n",
    "- $1+\\frac{1}{1/y} = 1+y$\n",
    "\n",
    "So\n",
    "$$\n",
    "f_Y(y)=20 e^{20-y}(1+y)\\cdot\\frac{1}{y^2}\n",
    "      =20 e^{20-y}\\left(\\frac{1}{y}+\\frac{1}{y^2}\\right),\\quad y>20.\n",
    "$$\n",
    "Rewrite $e^{20-y}=e^{-(y-20)}$:\n",
    "$$\n",
    "f_Y(y)=20 e^{-(y-20)}\\left(\\frac{1}{y}+\\frac{1}{y^2}\\right),\\quad y>20.\n",
    "$$\n",
    "\n",
    "### 4) Choose a proposal $g_Y$ that matches the main shape\n",
    "The main shape is the exponential term $e^{-(y-20)}$, so choose:\n",
    "$$\n",
    "g_Y(y)=e^{-(y-20)},\\quad y\\ge 20,\n",
    "$$\n",
    "which is exactly a shifted exponential: $Y = 20 + Z$ where $Z\\sim \\mathrm{Exp}(1)$.\n",
    "\n",
    "Now the ratio is:\n",
    "$$\n",
    "\\frac{f_Y(y)}{g_Y(y)} = 20\\left(\\frac{1}{y}+\\frac{1}{y^2}\\right).\n",
    "$$\n",
    "This decreases as $y$ increases, so the supremum is at $y=20$:\n",
    "$$\n",
    "M = 20\\left(\\frac{1}{20}+\\frac{1}{20^2}\\right)\n",
    "  = 20(0.05 + 0.0025)\n",
    "  = 1.05.\n",
    "$$\n",
    "So the acceptance rate is about $1/1.05 \\approx 95\\%$ (fast).\n",
    "\n",
    "### 5) Sampling algorithm\n",
    "1. Propose $Y = 20 + \\mathrm{Exp}(1)$ (i.e., $Y = 20 - \\ln U$ for $U\\sim \\mathrm{Unif}(0,1)$).\n",
    "2. Accept with probability\n",
    "$$\n",
    "\\alpha(Y) = \\frac{f_Y(Y)}{M g_Y(Y)}\n",
    "          = \\frac{20\\left(\\frac{1}{Y}+\\frac{1}{Y^2}\\right)}{1.05}.\n",
    "$$\n",
    "3. If accepted, output $X = 1/Y$.\n",
    "\n",
    "---\n",
    "\n",
    "## How do I find a better $g$ in general?\n",
    "\n",
    "A practical method:\n",
    "\n",
    "1. **Convert to a PDF** (if you are given a CDF) so you know the true target density $f$.\n",
    "2. **Look at the “hard part” of $f$** (usually the tail behavior):\n",
    "   - If you see $e^{-y}$-type decay, try exponential/gamma proposals.\n",
    "   - If you see $e^{-y^2}$, try a normal proposal.\n",
    "   - If you see $1/x$, $\\log x$, or $e^{-1/x}$, try a substitution like $y=1/x$ or $y=-\\log x$.\n",
    "3. **Use a change of variables** to simplify the density into something recognizable.\n",
    "4. **Pick $g$ to match the dominant shape factor** of the transformed density.\n",
    "5. **Compute or upper-bound** $M=\\sup f/g$.\n",
    "   If $M$ is close to $1$, your proposal is excellent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Why does the factor $1/y^2$ appear in the change of variables?\n",
    "\n",
    "Let $X$ have density $f_X(x)$ and define the transformation\n",
    "$$\n",
    "Y = \\frac{1}{X}.\n",
    "$$\n",
    "This transformation is one-to-one for $x>0$. Solving for $x$ in terms of $y$ gives the inverse\n",
    "$$\n",
    "x = g^{-1}(y) = \\frac{1}{y}.\n",
    "$$\n",
    "\n",
    "The change-of-variables formula for densities states:\n",
    "$$\n",
    "f_Y(y) = f_X\\!\\big(g^{-1}(y)\\big)\\left|\\frac{d}{dy}g^{-1}(y)\\right|.\n",
    "$$\n",
    "\n",
    "Differentiating the inverse transformation,\n",
    "$$\n",
    "\\frac{d}{dy}\\left(\\frac{1}{y}\\right) = -\\frac{1}{y^2}.\n",
    "$$\n",
    "Taking the absolute value gives\n",
    "$$\n",
    "\\left|\\frac{d}{dy}\\left(\\frac{1}{y}\\right)\\right| = \\frac{1}{y^2}.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "f_Y(y) = f_X(1/y)\\cdot \\frac{1}{y^2}.\n",
    "$$\n",
    "\n",
    "This factor accounts for how the transformation stretches or compresses probability mass and ensures that the transformed density integrates to one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85da4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa0186bc",
   "metadata": {},
   "source": [
    "# This is example of one of those exercises from above text:\n",
    "\n",
    "\n",
    "### Exercise:\n",
    "5. [4p] Fill in the remaining part of the function `problem1_rejection_2` in order to produce samples from the below distribution using rejection sampling:\n",
    "$$\n",
    "    F[x] = \n",
    "    \\begin{cases}\n",
    "        0, & x \\leq 0 \\\\\n",
    "        20xe^{20-1/x}, & 0 < x < \\frac{1}{20} \\\\\n",
    "        1, & x \\geq \\frac{1}{20}\n",
    "    \\end{cases}\n",
    "$$\n",
    "Hint: this is tricky because if you choose the wrong sampling distribution you reject at least 9 times out of 10. You will get points based on how long your code takes to create a certain number of samples, if you choose the correct sampling distribution you can easily create 100000 samples within 2 seconds.\n",
    "\n",
    "\n",
    "\n",
    "### Code:\n",
    "\n",
    "```python\n",
    "# ------------------------------------------------------------\n",
    "# Problem 1.5\n",
    "# Rejection sampling for the second distribution (fast version)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection_2(n_samples):\n",
    "    # We sample Y = 1/X instead of X directly.\n",
    "    # Proposal: Y = 20 + Exp(1)  (i.e., shifted exponential)\n",
    "    # Then accept/reject in Y-space and transform back: X = 1/Y.\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    M = 1.05  # sup_y f_Y(y)/g_Y(y) occurs at y=20\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # propose y ~ 20 + Exp(1)\n",
    "        u1 = np.random.uniform(0.0, 1.0)\n",
    "        y = 20.0 - np.log(u1)\n",
    "\n",
    "        # acceptance probability alpha(y) = f_Y(y) / (M g_Y(y))\n",
    "        # with g_Y(y) = exp(-(y-20)) and f_Y(y) = 20 exp(-(y-20)) (1/y + 1/y^2)\n",
    "        alpha = (20.0 * (1.0 / y + 1.0 / (y**2))) / M\n",
    "\n",
    "        u2 = np.random.uniform(0.0, 1.0)\n",
    "        if u2 <= alpha:\n",
    "            x = 1.0 / y\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n",
    "\n",
    "n_samples = 100000\n",
    "samples = problem1_rejection_2(n_samples)\n",
    "print(len(samples))\n",
    "```\n",
    "\n",
    "\n",
    "### Why do we write `Y = 20 - \\log(U)` in code?\n",
    "\n",
    "Let $U \\sim \\mathrm{Unif}(0,1)$.\n",
    "\n",
    "An exponential random variable with rate $1$ has CDF\n",
    "$$\n",
    "F_Z(z) = 1 - e^{-z}, \\quad z \\ge 0.\n",
    "$$\n",
    "\n",
    "Using the inverse CDF method, set\n",
    "$$\n",
    "U = 1 - e^{-Z}.\n",
    "$$\n",
    "\n",
    "Solve for $Z$:\n",
    "$$\n",
    "e^{-Z} = 1 - U\n",
    "$$\n",
    "$$\n",
    "Z = -\\log(1 - U).\n",
    "$$\n",
    "\n",
    "Since $1 - U \\sim \\mathrm{Unif}(0,1)$, we can write\n",
    "$$\n",
    "Z = -\\log U \\sim \\mathrm{Exp}(1).\n",
    "$$\n",
    "\n",
    "Our proposal density is supported on $y \\ge 20$, so we shift the exponential:\n",
    "$$\n",
    "Y = 20 + Z.\n",
    "$$\n",
    "\n",
    "Substituting $Z$ gives\n",
    "$$\n",
    "Y = 20 - \\log U.\n",
    "$$\n",
    "\n",
    "Thus, the code `Y = 20 - np.log(u)` generates a sample from the proposal\n",
    "$$\n",
    "g_Y(y) = e^{-(y-20)}, \\quad y \\ge 20.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ae572",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e243ea",
   "metadata": {},
   "source": [
    "# This is another Accept Reject problem which finds a better g(x) distribution:\n",
    "\n",
    "## Problem 1 — Rejection sampling with a better proposal\n",
    "\n",
    "We want to sample from a distribution with CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x \\le 0,\\\\\n",
    "\\dfrac{e^{x^2}-x^2-1}{e-2}, & 0<x<1,\\\\\n",
    "1, & x \\ge 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Differentiate the CDF to get the PDF\n",
    "\n",
    "For $0<x<1$,\n",
    "$$\n",
    "f(x) = F'(x).\n",
    "$$\n",
    "\n",
    "Differentiate:\n",
    "$$\n",
    "F(x)=\\frac{e^{x^2}-x^2-1}{e-2}.\n",
    "$$\n",
    "\n",
    "Using $\\frac{d}{dx}e^{x^2}=2x e^{x^2}$ and $\\frac{d}{dx}(-x^2)=-2x$:\n",
    "$$\n",
    "f(x)=\\frac{2x e^{x^2}-2x}{e-2}\n",
    "=\\frac{2x\\big(e^{x^2}-1\\big)}{e-2},\\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "Outside $(0,1)$ the density is $0$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: A naive proposal and why it is not great\n",
    "\n",
    "If we choose a Uniform proposal on $(0,1)$, then\n",
    "$$\n",
    "g(x)=1,\\quad 0<x<1.\n",
    "$$\n",
    "\n",
    "Then $M=\\sup_{x\\in(0,1)}\\frac{f(x)}{g(x)}=\\sup f(x)$, and the acceptance rate becomes about $1/M$.\n",
    "This gives an acceptance rate around $21\\%$, which is somewhat low.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Choose a better proposal $g(x)=2x$\n",
    "\n",
    "A good rule is: choose $g$ to match the main shape of $f$.\n",
    "\n",
    "Here,\n",
    "$$\n",
    "f(x)=\\frac{2x\\big(e^{x^2}-1\\big)}{e-2}\n",
    "$$\n",
    "contains a factor $2x$ multiplying another term.\n",
    "So we choose\n",
    "$$\n",
    "g(x)=2x,\\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "Check that it is a valid density:\n",
    "$$\n",
    "\\int_0^1 2x\\,dx = \\left[x^2\\right]_0^1 = 1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Compute $M=\\sup \\frac{f(x)}{g(x)}$\n",
    "\n",
    "Compute the ratio:\n",
    "$$\n",
    "\\frac{f(x)}{g(x)}\n",
    "=\n",
    "\\frac{\\frac{2x(e^{x^2}-1)}{e-2}}{2x}\n",
    "=\n",
    "\\frac{e^{x^2}-1}{e-2}.\n",
    "$$\n",
    "\n",
    "This is increasing in $x$ on $(0,1)$, so the supremum is at $x=1$:\n",
    "$$\n",
    "M=\\frac{e^{1}-1}{e-2}=\\frac{e-1}{e-2}.\n",
    "$$\n",
    "\n",
    "Therefore the acceptance rate is approximately\n",
    "$$\n",
    "\\frac{1}{M}=\\frac{e-2}{e-1}\\approx 0.418 \\approx 42\\%.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Acceptance probability $\\alpha(x)$\n",
    "\n",
    "In rejection sampling, accept with probability\n",
    "$$\n",
    "\\alpha(x)=\\frac{f(x)}{M g(x)}.\n",
    "$$\n",
    "\n",
    "Using the ratio above:\n",
    "$$\n",
    "\\alpha(x)\n",
    "=\n",
    "\\frac{\\frac{e^{x^2}-1}{e-2}}{\\frac{e-1}{e-2}}\n",
    "=\n",
    "\\frac{e^{x^2}-1}{e-1}.\n",
    "$$\n",
    "\n",
    "Note that $\\alpha(x)\\in[0,1]$ since $\\alpha(0)=0$ and $\\alpha(1)=1$.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Why we must sample $x$ using $x=\\sqrt{U}$\n",
    "\n",
    "Rejection sampling requires that the proposal value $X$ is actually drawn from the proposal density $g$.\n",
    "\n",
    "If we choose $g(x)=2x$, we must generate $X\\sim g$, not $X\\sim \\text{Uniform}(0,1)$.\n",
    "\n",
    "To sample from $g(x)=2x$, compute its CDF:\n",
    "$$\n",
    "G(x)=\\int_0^x 2t\\,dt = x^2,\\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "Use the inverse-CDF method:\n",
    "if $U\\sim \\text{Unif}(0,1)$, then\n",
    "$$\n",
    "X = G^{-1}(U).\n",
    "$$\n",
    "\n",
    "Solve $U=x^2$:\n",
    "$$\n",
    "x=\\sqrt{U}.\n",
    "$$\n",
    "\n",
    "So in code we propose with:\n",
    "- draw $U_1\\sim\\text{Unif}(0,1)$,\n",
    "- set $X=\\sqrt{U_1}$,\n",
    "which guarantees $X\\sim g(x)=2x$.\n",
    "\n",
    "Importantly, we also need an *independent* uniform $U_2$ to decide acceptance:\n",
    "$$\n",
    "\\text{accept if } U_2 \\le \\alpha(X).\n",
    "$$\n",
    "\n",
    "We do not put $U_1$ into $\\alpha(\\cdot)$ because $U_1$ was only used to generate the proposal $X$.\n",
    "Acceptance is a separate step that uses a new independent uniform $U_2$.\n",
    "\n",
    "---\n",
    "\n",
    "## Final rejection sampling algorithm (with $g(x)=2x$)\n",
    "\n",
    "1. Sample $U_1\\sim \\text{Unif}(0,1)$ and set $X=\\sqrt{U_1}$ (this gives $X\\sim g$).\n",
    "2. Sample $U_2\\sim \\text{Unif}(0,1)$.\n",
    "3. Accept $X$ if\n",
    "$$\n",
    "U_2 \\le \\alpha(X) = \\frac{e^{X^2}-1}{e-1}.\n",
    "$$\n",
    "Otherwise reject and repeat.\n",
    "\n",
    "This improves the acceptance rate to about $42\\%$ compared to the uniform proposal.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rejection sampling for target density:\n",
    "        f(x) = 2x (e^{x^2} - 1) / (e - 2),   0 < x < 1\n",
    "    using proposal:\n",
    "        g(x) = 2x,  0 < x < 1\n",
    "    with acceptance probability:\n",
    "        alpha(x) = (e^{x^2} - 1) / (e - 1)\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # 1) Propose X ~ g(x)=2x on (0,1) by inversion: G(x)=x^2 => X=sqrt(U1)\n",
    "        u1 = np.random.uniform(0.0, 1.0)\n",
    "        x = np.sqrt(u1)\n",
    "\n",
    "        # 2) Accept/reject with an independent uniform\n",
    "        u2 = np.random.uniform(0.0, 1.0)\n",
    "        alpha = (np.exp(x**2) - 1.0) / (np.e - 1.0)\n",
    "\n",
    "        if u2 <= alpha:\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb7b74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6feb3076",
   "metadata": {},
   "source": [
    "# This is the same problem as the one above, but with a better proposal g(x):\n",
    "\n",
    "## Problem 1 — Rejection sampling with an improved proposal\n",
    "\n",
    "We are given the CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x \\le 0,\\\\\n",
    "\\dfrac{e^{x^2}-x^2-1}{e-2}, & 0<x<1,\\\\\n",
    "1, & x \\ge 1.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We want to sample from this distribution using rejection sampling, but with a proposal $g$ that gives a high acceptance rate.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Differentiate the CDF to get the target PDF $f(x)$\n",
    "\n",
    "For $0<x<1$,\n",
    "$$\n",
    "f(x)=F'(x).\n",
    "$$\n",
    "\n",
    "Differentiate:\n",
    "$$\n",
    "F(x)=\\frac{e^{x^2}-x^2-1}{e-2}.\n",
    "$$\n",
    "\n",
    "Using $\\frac{d}{dx}e^{x^2}=2x e^{x^2}$ and $\\frac{d}{dx}(-x^2)=-2x$:\n",
    "$$\n",
    "f(x)=\\frac{2x e^{x^2}-2x}{e-2}\n",
    "=\\frac{2x\\big(e^{x^2}-1\\big)}{e-2}, \\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Choose a better proposal $g(x)$\n",
    "\n",
    "The target density has a dominant factor $2x e^{x^2}$.\n",
    "So choose a proposal proportional to that:\n",
    "$$\n",
    "g(x) \\propto 2x e^{x^2}, \\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "Normalize it. Compute the integral:\n",
    "$$\n",
    "\\int_0^1 2x e^{x^2}\\,dx.\n",
    "$$\n",
    "\n",
    "Let $u=x^2$ so $du=2x\\,dx$:\n",
    "$$\n",
    "\\int_0^1 2x e^{x^2}\\,dx = \\int_0^1 e^u\\,du = e-1.\n",
    "$$\n",
    "\n",
    "Therefore the normalized proposal is:\n",
    "$$\n",
    "g(x)=\\frac{2x e^{x^2}}{e-1},\\qquad 0<x<1.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Compute $M=\\sup \\frac{f(x)}{g(x)}$\n",
    "\n",
    "Compute the ratio:\n",
    "$$\n",
    "\\frac{f(x)}{g(x)}\n",
    "=\n",
    "\\frac{\\frac{2x(e^{x^2}-1)}{e-2}}{\\frac{2x e^{x^2}}{e-1}}\n",
    "=\n",
    "\\frac{e-1}{e-2}\\left(1-e^{-x^2}\\right).\n",
    "$$\n",
    "\n",
    "This is increasing in $x$ on $(0,1)$, so the maximum is at $x=1$:\n",
    "$$\n",
    "M=\\frac{e-1}{e-2}\\left(1-e^{-1}\\right).\n",
    "$$\n",
    "\n",
    "The acceptance rate is approximately $1/M \\approx 0.66$ (about $66\\%$), which is much better than using a uniform proposal.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: How to sample from $g(x)$ (proposal sampling)\n",
    "\n",
    "We need to generate $X\\sim g$.\n",
    "\n",
    "Compute the CDF of $g$:\n",
    "$$\n",
    "G(x)=\\int_0^x \\frac{2t e^{t^2}}{e-1}\\,dt\n",
    "=\\frac{e^{x^2}-1}{e-1}.\n",
    "$$\n",
    "\n",
    "Use inverse CDF sampling: if $U\\sim\\text{Unif}(0,1)$ then $X=G^{-1}(U)$.\n",
    "\n",
    "Solve for $x$:\n",
    "$$\n",
    "U=\\frac{e^{x^2}-1}{e-1}\n",
    "\\Rightarrow U(e-1)=e^{x^2}-1\n",
    "\\Rightarrow e^{x^2}=1+U(e-1)\n",
    "$$\n",
    "$$\n",
    "\\Rightarrow x^2=\\ln\\big(1+U(e-1)\\big)\n",
    "\\Rightarrow x=\\sqrt{\\ln\\big(1+U(e-1)\\big)}.\n",
    "$$\n",
    "\n",
    "So the proposal draw is:\n",
    "$$\n",
    "X=\\sqrt{\\ln\\big(1+U(e-1)\\big)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Acceptance probability\n",
    "\n",
    "In rejection sampling, accept the proposed $X$ with probability\n",
    "$$\n",
    "\\alpha(X)=\\frac{f(X)}{M g(X)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Final algorithm and code\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples):\n",
    "    samples = []\n",
    "    \n",
    "    # M = sup f(x)/g(x) on (0,1)\n",
    "    M = ((np.e - 1) / (np.e - 2)) * (1 - np.exp(-1))\n",
    "    \n",
    "    while len(samples) < n_samples:\n",
    "        # Propose X ~ g via inverse CDF\n",
    "        u1 = np.random.uniform(0, 1)\n",
    "        x = np.sqrt(np.log(u1 * (np.e - 1) + 1))\n",
    "        \n",
    "        # Accept/reject step\n",
    "        u2 = np.random.uniform(0, 1)\n",
    "        \n",
    "        fx = (2 * x * (np.exp(x**2) - 1)) / (np.e - 2)\n",
    "        gx = (2 * x * np.exp(x**2)) / (np.e - 1)\n",
    "        \n",
    "        alpha = fx / (M * gx)\n",
    "        \n",
    "        if u2 <= alpha:\n",
    "            samples.append(x)\n",
    "            \n",
    "    return np.array(samples)\n",
    "\n",
    "n_samples = 100000\n",
    "problem1_samples = problem1_rejection(n_samples)\n",
    "print(len(problem1_samples))\n",
    "```\n",
    "\n",
    "The code can also be simplified to:\n",
    "\n",
    "```python\n",
    "\n",
    "# Since alpha = (1 - e^{-x^2}) / (1 - e^{-1}) if you derive the expression. \n",
    "import numpy as np\n",
    "\n",
    "def problem1_rejection(n_samples):\n",
    "    samples = []\n",
    "    denom = 1.0 - np.exp(-1.0)  # 1 - e^{-1}\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # Propose X ~ g(x) = 2x e^{x^2} / (e-1) via inverse CDF\n",
    "        u1 = np.random.uniform(0.0, 1.0)\n",
    "        x = np.sqrt(np.log(1.0 + u1 * (np.e - 1.0)))\n",
    "\n",
    "        # Accept/reject\n",
    "        u2 = np.random.uniform(0.0, 1.0)\n",
    "        alpha = (1.0 - np.exp(-x**2)) / denom\n",
    "\n",
    "        if u2 <= alpha:\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples)\n",
    "\n",
    "n_samples = 100000\n",
    "problem1_samples = problem1_rejection(n_samples)\n",
    "print(len(problem1_samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40aa43f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c38a059",
   "metadata": {},
   "source": [
    "# Examples Assignment:\n",
    "\n",
    "\n",
    "1. [3p] Complete filling the function `cost` to compute the average cost of a prediction model under a certain prediction threshold. Plot the cost as a function of the threshold (using the validation data provided in the first code cell of this problem), between 0 and 1 with 0.01 increments.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# This is simply the code needed firstly in order to use the functions below, this is \n",
    "# just so we know what variable is what from the dataset:\n",
    "\n",
    "PROBLEM3_DF = pd.read_csv('data/fraud.csv')\n",
    "Y = PROBLEM3_DF['Class'].values\n",
    "X = PROBLEM3_DF[['V%d' % i for i in range(1,5)]+['Amount']].values\n",
    "\n",
    "# We will split the data into training, testing and validation sets\n",
    "from Utils import train_test_validation\n",
    "PROBLEM3_X_train, PROBLEM3_X_test, PROBLEM3_X_val, PROBLEM3_y_train, PROBLEM3_y_test, PROBLEM3_y_val = train_test_validation(X,Y,shuffle=True,random_state=1)\n",
    "\n",
    "# From this we will train a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(PROBLEM3_X_train,PROBLEM3_y_train)\n",
    "\n",
    "# THE FOLLOWING CODE WILL PRODUCE THE ARRAYS YOU NEED FOR THE PROBLEM\n",
    "\n",
    "PROBLEM3_y_pred_proba_val = lr.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "PROBLEM3_y_true_val = PROBLEM3_y_val\n",
    "\n",
    "# Below starts the implementation of the function for the task: \n",
    "\n",
    "# Part 1: 3 points\n",
    "\n",
    "# Implement the following function that calculates the cost of a binary classifier\n",
    "# according to the specification in the problem statement.\n",
    "# The function evaluates how expensive a given threshold choice is.\n",
    "def cost(y_true, y_predict_proba, threshold):\n",
    "    # y_true: numpy array of shape (n_samples,)\n",
    "    #         Contains the true binary labels (0 = legitimate, 1 = fraud)\n",
    "    #\n",
    "    # y_predict_proba: numpy array of shape (n_samples,)\n",
    "    #                  Contains predicted probabilities of fraud\n",
    "    #\n",
    "    # threshold: float in [0, 1]\n",
    "    #            Probabilities >= threshold are classified as fraud (1),\n",
    "    #            otherwise as legitimate (0)\n",
    "\n",
    "    # Convert predicted probabilities into binary predictions\n",
    "    # Fraud (1) if probability >= threshold, otherwise legitimate (0)\n",
    "    y_pred = (y_predict_proba >= threshold).astype(int)\n",
    "    \n",
    "    \n",
    "    # Compute confusion matrix components by counting outcomes\n",
    "    # True Positives: fraud correctly detected\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    \n",
    "    # True Negatives: legitimate transactions correctly allowed\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    \n",
    "    # False Positives: legitimate transactions incorrectly blocked\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    \n",
    "    # False Negatives: fraud that was missed\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    # Compute the total cost according to the problem specification\n",
    "    # TP  -> cost 100\n",
    "    # TN  -> cost 0\n",
    "    # FP  -> cost 120\n",
    "    # FN  -> cost 600\n",
    "    total_cost = (\n",
    "        100 * TP +     # Cost for detecting fraud (manual review, etc.)\n",
    "        0   * TN +     # No cost for correct legitimate transactions\n",
    "        120 * FP +     # Cost for wrongly blocking legitimate users\n",
    "        600 * FN       # High cost for missed fraud\n",
    "    )\n",
    "    \n",
    "    # Compute the average cost per sample\n",
    "    # This makes the cost comparable across datasets of different sizes\n",
    "    avg_cost = total_cost / len(y_true)\n",
    "    \n",
    "    return avg_cost\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Plot the cost as a function of the threshold\n",
    "# using validation data\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Create thresholds from 0 to 1 (inclusive) with step size 0.01\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "# Compute the average cost for each threshold value\n",
    "# using the validation labels and predicted probabilities\n",
    "costs = [\n",
    "    cost(PROBLEM3_y_true_val, PROBLEM3_y_pred_proba_val, t)\n",
    "    for t in thresholds\n",
    "]\n",
    "\n",
    "# Plot cost vs threshold\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, costs)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Average cost\")\n",
    "plt.title(\"Cost as a function of threshold (validation data)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a1f0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ba9fda",
   "metadata": {},
   "source": [
    "### Cost and Bayes Classifier with threshold:\n",
    "\n",
    "```python\n",
    "y_proba = model.predict_proba(X)[:, 1]  # = P(Y = 1 ∣ X), X = X_test\n",
    "y_pred = (y_proba >= threshold).astype(int) \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
