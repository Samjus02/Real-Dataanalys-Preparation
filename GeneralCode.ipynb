{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b5f837",
   "metadata": {},
   "source": [
    "## Here is an example of how to check reverisibility of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d89146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# P = matrix\n",
    "# pi = linear equation system we have \n",
    "\n",
    "# Transition matrix\n",
    "P = np.array([\n",
    "    [0.3, 0.7, 0.0, 0.0],  # Downtown\n",
    "    [0.2, 0.5, 0.3, 0.0],  # Suburbs\n",
    "    [0.0, 0.0, 0.5, 0.5],  # Countryside\n",
    "    [0.0, 0.0, 0.0, 1.0]   # Workshop\n",
    "])\n",
    "\n",
    "\n",
    "stationary_dist = stationary_distribution(P)\n",
    "\n",
    "\n",
    "# Code for checking reversibility for ANY matrix!:\n",
    "\n",
    "def is_reversible(P, stationary_dist, tol=1e-12):\n",
    "    n = P.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            left = stationary_dist[i] * P[i,j]\n",
    "            right = stationary_dist[j] * P[j,i]\n",
    "            if not np.isclose(left, right, atol=tol):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(\"Reversible:\", is_reversible(P, stationary_dist))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eb88f",
   "metadata": {},
   "source": [
    "## This is how you can always find the Stationary distribution of any matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64913b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution(P, tol=1e-12, verify=True):\n",
    "    \"\"\"\n",
    "    Compute the stationary distribution of a finite-state Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix. Rows should sum to 1 (row-stochastic).\n",
    "    tol : float, optional\n",
    "        Tolerance used for cleaning small numerical noise.\n",
    "    verify : bool, optional\n",
    "        If True, checks that the result is approximately stationary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : np.ndarray, shape (n,)\n",
    "        Stationary distribution vector (non-negative, sums to 1).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system:\n",
    "\n",
    "        (P^T - I) * pi = 0,  with  sum(pi) = 1\n",
    "\n",
    "    by replacing one of the equations with the normalization condition.\n",
    "    It assumes that a (unique) stationary distribution exists.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    if P.shape[0] != P.shape[1]:\n",
    "        raise ValueError(\"P must be a square matrix.\")\n",
    "\n",
    "    # Build A * pi = b\n",
    "    A = P.T - np.eye(n)\n",
    "    b = np.zeros(n)\n",
    "\n",
    "    # Replace last row with normalization condition: sum_i pi_i = 1\n",
    "    A[-1, :] = 1.0\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    # Clean tiny numerical noise\n",
    "    pi[np.abs(pi) < tol] = 0.0\n",
    "\n",
    "    # If there are small negative values, clamp them to 0 and renormalize\n",
    "    if np.any(pi < -tol):\n",
    "        # Serious negativity -> indicate potential problem\n",
    "        raise RuntimeError(\n",
    "            \"Computed stationary distribution has significantly negative entries. \"\n",
    "            \"Check that P is a valid transition matrix with a unique stationary distribution.\"\n",
    "        )\n",
    "\n",
    "    # Clamp small negatives and renormalize\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        raise RuntimeError(\"Failed to compute a valid stationary distribution (sum <= 0).\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify:\n",
    "        # Check stationarity: pi P ≈ pi\n",
    "        if not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "            raise RuntimeError(\"Result does not satisfy pi P ≈ pi. Check the input matrix P.\")\n",
    "\n",
    "    return pi\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f0aa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# This stationary distribution fiunction could also work, just not always the same and correct way:\n",
    "def stationary_distribution(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution of a Markov chain\n",
    "    by finding the eigenvector corresponding to eigenvalue 1.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(P.T)\n",
    "    \n",
    "    # Find the eigenvector associated with eigenvalue 1\n",
    "    idx = np.argmin(np.abs(eigenvalues - 1))\n",
    "    vec = np.real(eigenvectors[:, idx])\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    stationary = vec / np.sum(vec)\n",
    "    return stationary\n",
    "\n",
    "P = np.array([[0.3,0.7,0,0], [0.2,0.5,0.3,0], [0,0,0.5,0.5], [0,0,0,1]])\n",
    "print(\"This is stationary distribution: \", stationary_distribution(P))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085bfc2",
   "metadata": {},
   "source": [
    "### Always use this stationary distribution function below to check, it WILL always work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2878ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is stationary distribution:  [ 0. -0. -0.  1.]\n"
     ]
    }
   ],
   "source": [
    "def stationary_distribution_always_works(P):\n",
    "    \"\"\"\n",
    "    Computes the stationary distribution by solving\n",
    "    (P^T - I) * pi = 0  with  sum(pi) = 1.\n",
    "    This method ALWAYS works for any Markov chain with a stationary distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    n = P.shape[0]\n",
    "\n",
    "    # Build system: (P^T - I) * pi = 0\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Replace last equation with the normalization condition sum(pi)=1\n",
    "    A[-1] = np.ones(n)\n",
    "\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1.0\n",
    "\n",
    "    # Solve the linear system\n",
    "    pi = np.linalg.solve(A, b)\n",
    "\n",
    "    return pi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4df2f",
   "metadata": {},
   "source": [
    "### This is another stationary distribution function which the bot said will actually always for for any chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb10f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stationary_distribution_any_markov(P, tol=1e-12, cleanup_tol=1e-15, verify=True):\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "    if P.ndim != 2 or n != P.shape[1]:\n",
    "        raise ValueError(\"P must be square.\")\n",
    "\n",
    "    # Validate Markov matrix\n",
    "    if np.any(P < -cleanup_tol):\n",
    "        raise ValueError(\"P has negative entries.\")\n",
    "    if not np.allclose(P.sum(axis=1), 1.0, atol=1e-12):\n",
    "        raise ValueError(\"Rows of P must sum to 1.\")\n",
    "\n",
    "    A = P.T - np.eye(n)\n",
    "\n",
    "    # Add normalization as an extra equation (least squares)\n",
    "    A_aug = np.vstack([A, np.ones((1, n))])\n",
    "    b_aug = np.zeros(n + 1)\n",
    "    b_aug[-1] = 1.0\n",
    "\n",
    "    # Least-squares solution (works even if A is singular)\n",
    "    pi, *_ = np.linalg.lstsq(A_aug, b_aug, rcond=None)\n",
    "\n",
    "    # Cleanup / project to simplex\n",
    "    pi[np.abs(pi) < cleanup_tol] = 0.0\n",
    "    pi = np.maximum(pi, 0.0)\n",
    "    s = pi.sum()\n",
    "    if s <= 0 or not np.isfinite(s):\n",
    "        raise RuntimeError(\"Could not recover a valid distribution.\")\n",
    "    pi /= s\n",
    "\n",
    "    if verify and not np.allclose(pi @ P, pi, atol=1e-8):\n",
    "        # If periodic/reducible, this should still pass; if not, input likely invalid/ill-conditioned\n",
    "        raise RuntimeError(\"Result does not satisfy pi P ≈ pi (within tolerance).\")\n",
    "\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d7b41",
   "metadata": {},
   "source": [
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6437185",
   "metadata": {},
   "source": [
    "### Expected hitting time function\n",
    "\n",
    "This function computes expected hitting times to a given **target set of states** in a finite Markov chain with transition matrix \\( P \\).\n",
    "\n",
    "We consider a Markov chain with state space \\( \\{0, 1, \\dots, n-1\\} \\) and transition matrix\n",
    "\n",
    "$$\n",
    "P = (P_{ij})_{i,j=0}^{n-1},\n",
    "$$\n",
    "\n",
    "where \\( P_{ij} = \\mathbb{P}(X_{t+1} = j \\mid X_t = i) \\).\n",
    "\n",
    "Given a set of **target states** \\( T \\subset \\{0, \\dots, n-1\\} \\), the *hitting time* of \\( T \\) is\n",
    "\n",
    "$$\n",
    "T_{\\text{hit}} = \\min\\{ t \\ge 0 : X_t \\in T \\}.\n",
    "$$\n",
    "\n",
    "For each state \\( i \\), we define the expected hitting time\n",
    "\n",
    "$$\n",
    "h(i) = \\mathbb{E}[T_{\\text{hit}} \\mid X_0 = i].\n",
    "$$\n",
    "\n",
    "These satisfy\n",
    "\n",
    "$$\n",
    "h(i) = 0 \\quad \\text{for } i \\in T,\n",
    "$$\n",
    "\n",
    "and for \\( i \\notin T \\),\n",
    "\n",
    "$$\n",
    "h(i) = 1 + \\sum_{j=0}^{n-1} P_{ij} h(j).\n",
    "$$\n",
    "\n",
    "If we collect the non-target states into a set \\( S = \\{0, \\dots, n-1\\} \\setminus T \\), and form the submatrix \\( Q \\) of \\( P \\) with rows and columns indexed by \\( S \\), then the vector \\( h_S = (h(i))_{i \\in S} \\) solves\n",
    "\n",
    "$$\n",
    "(I - Q) h_S = \\mathbf{1},\n",
    "$$\n",
    "\n",
    "where \\( \\mathbf{1} \\) is a vector of ones.\n",
    "\n",
    "The function `expected_hitting_time` implements this:\n",
    "\n",
    "- **Parameters**\n",
    "  - `P`: `np.ndarray` of shape `(n, n)`  \n",
    "    Transition matrix of the Markov chain.\n",
    "  - `target_states`: iterable of integers  \n",
    "    Indices of the target states \\( T \\).\n",
    "  - `start_state` (optional): integer  \n",
    "    If provided, the function returns \\( h(\\text{start\\_state}) \\).\n",
    "  - `start_dist` (optional): 1D array-like of length `n`  \n",
    "    Initial distribution \\( \\alpha \\). If provided, the function returns\n",
    "    $$\n",
    "    \\mathbb{E}[T_{\\text{hit}}] = \\sum_{i=0}^{n-1} \\alpha_i h(i).\n",
    "    $$\n",
    "\n",
    "- **Return value**\n",
    "  - If `start_state` is given: a single float, \\( h(\\text{start\\_state}) \\).\n",
    "  - If `start_dist` is given: a single float, the expected hitting time under that initial distribution.\n",
    "  - If neither is given: a length-`n` NumPy array, containing \\( h(i) \\) for all states `i` (targets get value `0`).\n",
    "\n",
    "- **Usage example (this exam problem)**\n",
    "\n",
    "For the three-region chain\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix}\n",
    "0.3 & 0.4 & 0.3 \\\\\\\\\n",
    "0.2 & 0.5 & 0.3 \\\\\\\\\n",
    "0.4 & 0.3 & 0.3\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "with downtown = state 0 and suburbs = state 1:\n",
    "\n",
    "```python\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # should be 50/13 ≈ 3.8461538\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9228617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def expected_hitting_time(P, target_states, start_state=None, start_dist=None):\n",
    "    \"\"\"\n",
    "    Compute expected hitting times to a given set of target states in a finite Markov chain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : np.ndarray, shape (n, n)\n",
    "        Transition matrix of the Markov chain.\n",
    "    target_states : iterable of int\n",
    "        Indices of the target states.\n",
    "    start_state : int, optional\n",
    "        If provided, return the expected hitting time starting from this state.\n",
    "    start_dist : array-like, shape (n,), optional\n",
    "        If provided, return the expected hitting time under this initial distribution.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or np.ndarray\n",
    "        - If start_state is given: expected hitting time from that state.\n",
    "        - If start_dist is given: expected hitting time under that distribution.\n",
    "        - If neither is given: array h of length n with expected hitting times\n",
    "          from all states (targets have value 0).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This solves the linear system\n",
    "\n",
    "        (I - Q) h_S = 1\n",
    "\n",
    "    where Q is the submatrix of P restricted to non-target states,\n",
    "    and 1 is a vector of ones. Assumes that the target set is hit\n",
    "    with probability 1 from the relevant starting states.\n",
    "    \"\"\"\n",
    "    P = np.asarray(P, dtype=float)\n",
    "    n = P.shape[0]\n",
    "\n",
    "    target_states = np.array(sorted(set(target_states)), dtype=int)\n",
    "    all_states = np.arange(n, dtype=int)\n",
    "\n",
    "    # Non-target states S\n",
    "    non_target_states = np.array([s for s in all_states if s not in target_states], dtype=int)\n",
    "\n",
    "    # If all states are targets, hitting time is identically zero\n",
    "    if non_target_states.size == 0:\n",
    "        h = np.zeros(n, dtype=float)\n",
    "        if start_state is not None:\n",
    "            return float(h[start_state])\n",
    "        if start_dist is not None:\n",
    "            start_dist = np.asarray(start_dist, dtype=float)\n",
    "            return float(start_dist @ h)\n",
    "        return h\n",
    "\n",
    "    # Build Q and solve (I - Q) h_S = 1\n",
    "    Q = P[np.ix_(non_target_states, non_target_states)]\n",
    "    I = np.eye(Q.shape[0])\n",
    "    ones = np.ones(Q.shape[0])\n",
    "\n",
    "    # Solve for h_S\n",
    "    h_S = np.linalg.solve(I - Q, ones)\n",
    "\n",
    "    # Put back into full vector h of length n\n",
    "    h = np.zeros(n, dtype=float)\n",
    "    h[target_states] = 0.0\n",
    "    for idx, s in enumerate(non_target_states):\n",
    "        h[s] = h_S[idx]\n",
    "\n",
    "    # Return according to user request\n",
    "    if (start_state is not None) and (start_dist is not None):\n",
    "        raise ValueError(\"Provide either start_state or start_dist, not both.\")\n",
    "\n",
    "    if start_state is not None:\n",
    "        return float(h[start_state])\n",
    "\n",
    "    if start_dist is not None:\n",
    "        start_dist = np.asarray(start_dist, dtype=float)\n",
    "        if start_dist.shape[0] != n:\n",
    "            raise ValueError(\"start_dist must have length equal to number of states.\")\n",
    "        return float(start_dist @ h)\n",
    "\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03d2d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.846153846153846\n"
     ]
    }
   ],
   "source": [
    "# Example from above code: \n",
    "\n",
    "P = np.array([\n",
    "    [0.3, 0.4, 0.3],  # Downtown\n",
    "    [0.2, 0.5, 0.3],  # Suburbs\n",
    "    [0.4, 0.3, 0.3],  # Countryside\n",
    "])\n",
    "\n",
    "# Expected steps until first time in Downtown (state 0) starting from Suburbs (state 1)\n",
    "ET_suburbs_to_downtown = expected_hitting_time(P, target_states=[0], start_state=1)\n",
    "print(ET_suburbs_to_downtown)  # ~3.846153846153846 (50/13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dde39",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is how you can always find M in a finite interval for the Reject-Accept sampling algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7d8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_x(x):\n",
    "    return np.exp(x)          # target pdf\n",
    "\n",
    "def g_x(x):\n",
    "    return 1/np.log(2)        # uniform(0, ln 2) pdf\n",
    "\n",
    "# Remember to change the interval to your interval you have\n",
    "xs = np.linspace(0, np.log(2), 1000)\n",
    "\n",
    "ratio = f_x(xs) / g_x(xs)\n",
    "\n",
    "# This works since M is always maximum of f / g\n",
    "M_num = ratio.max()\n",
    "\n",
    "print(\"Answer: \", 2*np.log(2))\n",
    "\n",
    "print(\"Numeric M ≈\", M_num)   # should be close to 2*np.log(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24c8a7",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is the different Hoeffding intervals from the Lecture notes:\n",
    "\n",
    "## Summary of Concentration Inequalities from the Lecture Notes\n",
    "\n",
    "This cell summarizes the main inequalities used in Monte Carlo estimation and empirical distribution analysis:  \n",
    "- Hoeffding’s inequality for Monte Carlo means  \n",
    "- The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality  \n",
    "- Alternative concentration inequalities listed in the notes  \n",
    "For each formula we also show how to solve for $$\\varepsilon$$ when forming a confidence interval.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hoeffding’s Inequality for Monte Carlo Estimation\n",
    "\n",
    "Assume we estimate a mean using  \n",
    "$$\n",
    "\\overline{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i,\n",
    "$$  \n",
    "where the samples satisfy $Y_i \\in [a,b]$.\n",
    "\n",
    "The **two-sided Hoeffding bound** is:\n",
    "$$\n",
    "\\mathbb{P}\\left( \\left| \\overline{Y} - \\mathbb{E}[Y] \\right| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "$$\n",
    "2 \\exp\\left( \\frac{-2 n \\varepsilon^2}{(b-a)^2} \\right) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Thus a $(1-\\delta)100\\%$ confidence interval is:\n",
    "$$\n",
    "\\left[\\, \\overline{Y} - \\varepsilon,\\; \\overline{Y} + \\varepsilon \\,\\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dvoretzky–Kiefer–Wolfowitz (DKW) Inequality\n",
    "\n",
    "For empirical CDF $$F_n(x)$$ based on i.i.d. samples with true CDF $$F(x)$$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( \\sup_x |F_n(x) - F(x)| \\ge \\varepsilon \\right)\n",
    "\\;\\le\\; 2 \\exp(-2n\\varepsilon^2).\n",
    "$$\n",
    "\n",
    "### Solving for $$\\varepsilon$$\n",
    "\n",
    "Set the right-hand side equal to $\\delta$:\n",
    "\n",
    "$$\n",
    "2 \\exp(-2n\\varepsilon^2) = \\delta.\n",
    "$$\n",
    "\n",
    "Solving gives:\n",
    "$$\n",
    "\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}.\n",
    "$$\n",
    "\n",
    "Useful for constructing confidence bands:\n",
    "$$\n",
    "F_n(x) - \\varepsilon \\le F(x) \\le F_n(x) + \\varepsilon.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Other Alternatives Mentioned in the Lecture Notes\n",
    "\n",
    "### (a) Chebyshev’s Inequality\n",
    "Assuming finite variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le \\frac{\\sigma^2}{n\\varepsilon^2}.\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ by setting RHS = $\\delta$:\n",
    "$$\n",
    "\\varepsilon = \\sigma \\sqrt{\\frac{1}{n\\delta}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Central Limit Theorem (CLT) Approximation\n",
    "For large $n$:\n",
    "\n",
    "$$\n",
    "\\overline{Y} \\approx \\mathcal{N}\\!\\left(\\mathbb{E}[Y],\\, \\frac{\\sigma^2}{n}\\right).\n",
    "$$\n",
    "\n",
    "A $(1-\\delta)$ interval is:\n",
    "$$\n",
    "\\overline{Y} \\;\\pm\\; z_{1-\\delta/2}\\,\\frac{\\sigma}{\\sqrt{n}},\n",
    "$$\n",
    "where $z_{1-\\delta/2}$ is the standard normal quantile.\n",
    "\n",
    "---\n",
    "\n",
    "### (c) Bernstein (or Chernoff–Hoeffding) Inequality  \n",
    "Sometimes given in extended form when variance is known. In bounded case (same assumptions as Hoeffding):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left( |\\overline{Y} - \\mathbb{E}[Y]| \\ge \\varepsilon \\right)\n",
    "\\le 2 \\exp\\!\\left( \n",
    "\\frac{-n\\varepsilon^2}{2\\sigma^2 + \\frac{2}{3}(b-a)\\varepsilon}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Solving for $\\varepsilon$ requires numerical methods; not algebraic in closed form.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table of $$\\varepsilon$$ Solutions\n",
    "\n",
    "| Inequality | Bound | Solution for $$\\varepsilon$$ |\n",
    "|-----------|-------|-------------------------------|\n",
    "| Hoeffding | $2\\exp\\!\\left(-\\frac{2n\\varepsilon^2}{(b-a)^2}\\right) \\le \\delta$ | $\\varepsilon = (b-a)\\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| DKW | $2\\exp(-2n\\varepsilon^2) \\le \\delta$ | $\\varepsilon = \\sqrt{\\frac{\\ln(2/\\delta)}{2n}}$ |\n",
    "| Chebyshev | $\\frac{\\sigma^2}{n\\varepsilon^2} \\le \\delta$ | $\\varepsilon = \\sigma\\sqrt{\\frac{1}{n\\delta}}$ |\n",
    "| CLT | approx | $$\\varepsilon = z_{1-\\delta/2}\\,\\sigma/\\sqrt{n}$$ |\n",
    "| Bernstein | not closed form | requires numerical solution |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03c909",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### General information that could be important:\n",
    "\n",
    "* .values and .to_numpy() both convert pandas DataFrames or Series into NumPy arrays; \n",
    "* .to_numpy() is the recommended modern approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003d142",
   "metadata": {},
   "source": [
    "-----\n",
    "### Permutation Importance\n",
    "\n",
    "Permutation importance is a model-agnostic method for measuring feature importance. It works by randomly permuting the values of a single feature in the test set and then measuring how much the model’s predictive performance decreases. If permuting a feature leads to a large drop in performance, the model relied heavily on that feature, and it is considered important.\n",
    "\n",
    "This method measures the **impact of each feature on the model’s predictive performance**, rather than relying on model-specific parameters. Because it only requires the ability to make predictions and evaluate them with a chosen metric, permutation importance is applicable to **any type of predictive model**, including linear models, tree-based models, and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa904c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GENERAL TEMPLATE: Permutation Importance (works for ANY model)\n",
    "# ============================================================\n",
    "# What you MUST change depending on your setup:\n",
    "#   1) estimator      -> set this to your TRAINED model (or Pipeline)\n",
    "#   2) X_test, y_test -> set these to your TEST split\n",
    "#   3) feature_names  -> set these to your column names (list of strings)\n",
    "#   4) scoring        -> choose a metric appropriate for your task\n",
    "#\n",
    "# Notes:\n",
    "# - This works for any model as long as it has predict() (or predict_proba() for some scorers)\n",
    "# - If you used preprocessing (scaling, one-hot encoding, etc.), it's best to wrap it in a Pipeline\n",
    "#   and pass the Pipeline as the estimator to avoid mismatches.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --------------------------\n",
    "# 1) CHOOSE YOUR TRAINED MODEL\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If you trained a Pipeline (recommended): estimator = my_pipeline\n",
    "# - If you trained a plain model:           estimator = my_model\n",
    "#\n",
    "# Examples:\n",
    "# estimator = problem3_model                   # Pipeline: scaler + logistic regression\n",
    "# estimator = trained_random_forest_model      # e.g., RandomForestClassifier already fit\n",
    "# estimator = trained_svm_model                # e.g., SVC already fit\n",
    "estimator = problem3_model  # <-- CHANGE to your trained model / pipeline\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) PROVIDE TEST DATA\n",
    "# --------------------------\n",
    "# CHANGE THESE:\n",
    "# - X_test should be the test features (NumPy array or pandas DataFrame)\n",
    "# - y_test should be the test labels\n",
    "#\n",
    "# Examples:\n",
    "# X_test = problem3_X_test\n",
    "# y_test = problem3_y_test\n",
    "X_test = problem3_X_test   # <-- CHANGE if your variables are named differently\n",
    "y_test = problem3_y_test   # <-- CHANGE if your variables are named differently\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) PROVIDE FEATURE NAMES\n",
    "# --------------------------\n",
    "# CHANGE THIS:\n",
    "# - If X_test is a pandas DataFrame, you can do: feature_names = X_test.columns\n",
    "# - If X_test is a NumPy array, you must supply a list yourself (same order as columns in X_test)\n",
    "#\n",
    "# Examples:\n",
    "# feature_names = problem3_features\n",
    "# feature_names = list(X_test.columns)\n",
    "feature_names = problem3_features  # <-- CHANGE to your feature name list (correct order!)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) CHOOSE A SCORING METRIC\n",
    "# --------------------------\n",
    "# CHANGE THIS depending on your task:\n",
    "# Classification examples:\n",
    "#   scoring = \"accuracy\"            (simple, common)\n",
    "#   scoring = \"balanced_accuracy\"   (good if classes are imbalanced)\n",
    "#   scoring = \"f1\"                  (if you care about positive class quality)\n",
    "#   scoring = \"roc_auc\"             (needs probability or decision scores; many models support it)\n",
    "#\n",
    "# Regression examples:\n",
    "#   scoring = \"r2\"\n",
    "#   scoring = \"neg_mean_squared_error\"\n",
    "#   scoring = \"neg_mean_absolute_error\"\n",
    "#\n",
    "# Tip:\n",
    "# - If \"roc_auc\" fails, your estimator may not provide predict_proba/decision_function.\n",
    "scoring = \"accuracy\"  # <-- CHANGE if needed\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) RUN PERMUTATION IMPORTANCE\n",
    "# --------------------------\n",
    "perm = permutation_importance(\n",
    "    estimator=estimator,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    n_repeats=30,        # increase for more stable estimates (slower)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring=scoring\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 6) FORMAT RESULTS\n",
    "# --------------------------\n",
    "perm_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance Mean\": perm.importances_mean,\n",
    "    \"Importance Std\": perm.importances_std\n",
    "}).sort_values(by=\"Importance Mean\", ascending=False)\n",
    "\n",
    "print(\"Top features by permutation importance:\")\n",
    "print(perm_df.head(15))\n",
    "\n",
    "# --------------------------\n",
    "# 7) OPTIONAL: GET MOST IMPORTANT FEATURE\n",
    "# --------------------------\n",
    "most_important_feature = perm_df.iloc[0][\"Feature\"]\n",
    "print(\"\\nMost important feature (by permutation importance):\", most_important_feature)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 8) OPTIONAL: RESTRICT TO A SUBSET OF FEATURES\n",
    "# --------------------------\n",
    "# Example: if you want \"most important one-hot encoded feature\"\n",
    "# CHANGE the selection rule to match your one-hot naming scheme.\n",
    "# For your diabetes case (features starting with smoking_ or sex_):\n",
    "subset = [f for f in feature_names if str(f).startswith(\"smoking_\") or str(f).startswith(\"sex_\")]\n",
    "\n",
    "if len(subset) > 0:\n",
    "    perm_subset = perm_df[perm_df[\"Feature\"].isin(subset)].sort_values(by=\"Importance Mean\", ascending=False)\n",
    "    print(\"\\nPermutation importance for subset features:\")\n",
    "    print(perm_subset)\n",
    "\n",
    "    most_important_in_subset = perm_subset.iloc[0][\"Feature\"]\n",
    "    print(\"\\nMost important feature in subset:\", most_important_in_subset)\n",
    "else:\n",
    "    print(\"\\nSubset list is empty. Adjust the subset selection rule to match your feature names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f29983",
   "metadata": {},
   "source": [
    "### Remember:\n",
    "If the dataset does not have a header, then you need to use header=None in the pd.read_csv(\"data\", header=None). Otherwise we will miss one column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5124bb2",
   "metadata": {},
   "source": [
    "-----\n",
    "### Using Utils.py file\n",
    "\n",
    "If the exam question requires the file Utils.py, then simply copy paste the file into the current folder I am in and then the code will be able to find the file Utils.py. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5066841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f693a4e",
   "metadata": {},
   "source": [
    "-----\n",
    "# This is general information when to use which data set in the exam:\n",
    "\n",
    "## When to use each dataset and variable\n",
    "\n",
    "The data in this assignment is split into **training**, **validation**, and **test** sets. Each set has a specific role, and using them correctly is essential to avoid data leakage and to obtain an unbiased evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Training set: fit the model\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_train`\n",
    "- `PROBLEM3_y_train`\n",
    "\n",
    "**When to use**\n",
    "Use the training set **only to learn the model parameters**.\n",
    "\n",
    "**Typical operations**\n",
    "- Fit a model:\n",
    "\n",
    "```python\n",
    "    model.fit(PROBLEM3_X_train, PROBLEM3_y_train)\n",
    "```\n",
    "\n",
    "\n",
    "- Do **not** compute performance metrics or choose thresholds using training data.\n",
    "\n",
    "**Purpose**\n",
    "The training set teaches the model the relationship between features and labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Validation set: model selection and threshold choice\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_val`\n",
    "- `PROBLEM3_y_val` (or `PROBLEM3_y_true_val`)\n",
    "- `PROBLEM3_y_pred_proba_val`\n",
    "\n",
    "**When to use**\n",
    "Use the validation set to **make decisions about the model**, such as:\n",
    "- choosing a classification threshold,\n",
    "- comparing different loss functions,\n",
    "- computing cost, precision, recall, and 0–1 loss.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_val = model.predict_proba(PROBLEM3_X_val)[:,1]\n",
    "```\n",
    "\n",
    "- Convert probabilities to predictions:\n",
    "\n",
    "```python\n",
    "    y_pred_val = (y_pred_proba_val >= threshold).astype(int)\n",
    "```\n",
    "\n",
    "- Compute metrics:\n",
    "- cost\n",
    "- precision\n",
    "- recall\n",
    "- 0–1 loss\n",
    "\n",
    "**Purpose**\n",
    "The validation set is used to **tune decisions** without biasing the final evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Test set: final evaluation only\n",
    "\n",
    "**Variables**\n",
    "- `PROBLEM3_X_test`\n",
    "- `PROBLEM3_y_test` (or `PROBLEM3_y_true_test`)\n",
    "- `PROBLEM3_y_pred_proba_test`\n",
    "\n",
    "**When to use**\n",
    "Use the test set **only after**:\n",
    "- the model has been trained,\n",
    "- the threshold has been chosen using validation data.\n",
    "\n",
    "**Typical operations**\n",
    "- Predict probabilities:\n",
    "\n",
    "```python\n",
    "    y_pred_proba_test = model.predict_proba(PROBLEM3_X_test)[:,1]\n",
    "```\n",
    "\n",
    "- Evaluate final performance:\n",
    "- compute final cost,\n",
    "- build a confidence interval,\n",
    "- report final metrics.\n",
    "\n",
    "**Purpose**\n",
    "The test set provides an **unbiased estimate of real-world performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary table (conceptual)\n",
    "\n",
    "- Training set → **fit the model**\n",
    "- Validation set → **choose thresholds and compare decision rules**\n",
    "- Test set → **final evaluation and confidence intervals**\n",
    "\n",
    "---\n",
    "\n",
    "## Important rules to remember\n",
    "\n",
    "- Never choose thresholds using the test set.\n",
    "- Never report final performance using the validation set.\n",
    "- The test set must only be used **once**, at the very end.\n",
    "- Predicted probabilities (`predict_proba`) are used for **threshold-based decisions**.\n",
    "- Binary predictions (`>= threshold`) are used for **cost, precision, recall, and loss**.\n",
    "\n",
    "Following these rules ensures a correct and exam-safe machine learning workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cea85c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1e8f93c",
   "metadata": {},
   "source": [
    "# General Guide: Which Dataset to Use, When, and Why (Logistic Regression & Classification)\n",
    "\n",
    "THIS RESPONSE IS **INTENTIONALLY AND EXCLUSIVELY** A SINGLE MARKDOWN TEXT CELL.  \n",
    "There is **NO TEXT OUTSIDE THIS BLOCK**.  \n",
    "You can copy **once** and paste directly into a Jupyter Notebook **Markdown cell**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Defining Features and Target\n",
    "\n",
    "### Features (`X`)\n",
    "- Features are the **input variables** used by the model to make predictions.\n",
    "- These typically include:\n",
    "  - Numerical variables (e.g. age, BMI, blood glucose)\n",
    "  - One-Hot encoded categorical variables (e.g. `sex_Male`, `smoking_former`)\n",
    "- **Rule**: Features must represent information that is available **before** a prediction is made.\n",
    "- **Rule**: Never include the target variable inside the feature set.\n",
    "\n",
    "### Target (`y`)\n",
    "- The target is what the model is trying to predict.\n",
    "- For classification:\n",
    "  - Binary variable (e.g. diabetes = 0 or 1)\n",
    "- **Rule**: The target must NEVER be included among the features.\n",
    "\n",
    "```python\n",
    "problem3_X = problem3_df[feature_columns].values\n",
    "problem3_y = problem3_df[target_column].values\n",
    "```\n",
    "\n",
    "## 2. Train–Test Split (Why and How)\n",
    "\n",
    "Training Dataset\n",
    "- Used to train (fit) the model.\n",
    "- The model learns patterns from this data.\n",
    "\n",
    "Test Dataset\n",
    "- Used to evaluate final performance.\n",
    "- Simulates unseen, real-world data.\n",
    "- Must NEVER be used during training.\n",
    "\n",
    "Standard Split\n",
    "- 80% training\n",
    "- 20% testing\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    problem3_X,\n",
    "    problem3_y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## 3. Training the Model (.fit())\n",
    "Which Dataset Goes Into .fit()?\n",
    "- ONLY the training dataset.\n",
    "\n",
    "```python\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  model = LogisticRegression(C=1.0, max_iter=1000)\n",
    "  model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Notes\n",
    "- C controls regularization:\n",
    "  - Smaller C → stronger penalization\n",
    "  - Important when many One-Hot encoded features exist\n",
    "- max_iter is increased to avoid convergence warnings\n",
    "\n",
    "\n",
    "## 4. Making Predictions (.predict())\n",
    "Which Dataset Goes Into .predict()?\n",
    "\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "```python\n",
    "  y_pred = model.predict(X_test)\n",
    "```\n",
    "Rule: Never evaluate performance using predictions from X_train.\n",
    "\n",
    "\n",
    "## 5. Probability Predictions (.predict_proba())\n",
    "Used when:\n",
    "- You want class probabilities instead of labels\n",
    "- You want threshold-based decisions\n",
    "- You want confidence-aware analysis\n",
    "\n",
    "```python\n",
    "  y_prob = model.predict_proba(X_test)\n",
    "```\n",
    "* Output shape: (n_samples, 2)\n",
    "  * Column 0 → probability of class 0\n",
    "  * Column 1 → probability of class 1\n",
    "\n",
    "\n",
    "## 6. Evaluation Metrics: Precision & Recall\n",
    "Precision\n",
    "* Of all predicted positives, how many are correct?\n",
    "* Interpretation:\n",
    "  * “When the model predicts diabetes, how often is it right?”\n",
    "\n",
    "Recall\n",
    "* Of all actual positives, how many were found?\n",
    "* Interpretation:\n",
    "  * “How many diabetes cases did the model detect?”\n",
    "\n",
    "```python\n",
    "  from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "  precision_1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "  recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "  precision_0 = precision_score(y_test, y_pred, pos_label=0)\n",
    "  recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
    "```\n",
    "\n",
    "## 7. Feature Importance (Logistic Regression)\n",
    "\n",
    "How Feature Importance Is Defined\n",
    "* Logistic Regression uses coefficients\n",
    "* Larger absolute coefficient ⇒ stronger influence\n",
    "\n",
    "```python\n",
    "  coefficients = model.coef_[0]\n",
    "```\n",
    "\n",
    "One-Hot Encoded Features\n",
    "* Compare absolute values of coefficients\n",
    "* The most important One-Hot encoded feature is the one with the largest absolute coefficient\n",
    "\n",
    "```python\n",
    "  important_idx = np.argmax(np.abs(coefficients))\n",
    "  important_feature = feature_columns[important_idx]\n",
    "```\n",
    "\n",
    "## 8. What Dataset to Use for Each Task (Summary Table)\n",
    "| Purpose | Dataset |\n",
    "|--------|---------|\n",
    "| Train the model | `X_train`, `y_train` |\n",
    "| Final model evaluation | `X_test`, `y_test` |\n",
    "| Generate predicted class labels | `X_test` |\n",
    "| Generate predicted probabilities | `X_test` |\n",
    "| Predict on completely new / unseen data | New data with the **same feature structure** as `X_train` |\n",
    "| Debugging or sanity checks only | `X_train` (NOT for evaluation) |\n",
    "| Compute precision / recall | Compare `y_test` with predictions from `X_test` |\n",
    "| Compute confidence intervals | Predictions made on `X_test` |\n",
    "| Extract feature importance | Trained model (`model.coef_`) |\n",
    "\n",
    "\n",
    "## 10. Exam-Safe Golden Rules (MEMORIZE)\n",
    "* Never train on test data\n",
    "* Never evaluate on training data\n",
    "* .fit() → training data ONLY\n",
    "* .predict() → test or unseen data ONLY\n",
    "* Metrics → always computed using y_test\n",
    "* One-Hot feature importance → coefficient magnitude\n",
    "* If unsure: ask which dataset is allowed before using it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f1027",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ab3341a",
   "metadata": {},
   "source": [
    "# Relevant metrics to look at the LinearRegression model: \n",
    "\n",
    "Below are four common regression metrics. Each compares the **true target values** to the model’s **predicted target values**.\n",
    "\n",
    "### Notation\n",
    "- $y_i$: true target value for sample $i$\n",
    "- $\\hat{y}_i$: predicted target value for sample $i$\n",
    "- $\\bar{y}$: mean of true targets in the evaluated set (typically the test set)\n",
    "- $n$: number of samples in the evaluated set\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Mean Squared Error (MSE)\n",
    "**What it does (short):** Measures the average squared prediction error. Large errors are penalized more because of squaring.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test` (true values for the test set)\n",
    "- $\\hat{y}$ = `y_pred` (predictions for the test set)\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Root Mean Squared Error (RMSE)\n",
    "**What it does (short):** Square root of MSE, giving an error measure in the **same unit** as the target variable.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Mean Absolute Error (MAE)\n",
    "**What it does (short):** Measures the average absolute prediction error. Less sensitive to outliers than MSE/RMSE.\n",
    "\n",
    "**Formula:**\n",
    "$ \\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## 4) $R^2$ Score (Coefficient of Determination)\n",
    "**What it does (short):** Measures how much of the variance in $y$ is explained by the model. \n",
    "- $R^2 = 1$ is perfect fit\n",
    "- $R^2 = 0$ is no better than predicting the mean\n",
    "- $R^2 < 0$ is worse than predicting the mean\n",
    "\n",
    "**Formula:**\n",
    "$ R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} $\n",
    "\n",
    "**Datasets used:**\n",
    "- $y$ = `y_test`\n",
    "- $\\hat{y}$ = `y_pred`\n",
    "- $\\bar{y}$ is the mean of `y_test`\n",
    "\n",
    "---\n",
    "\n",
    "# Python code to compute all metrics (scikit-learn)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assume you already have a train/test split:\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# 1) Train the model on the training data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2) Create predictions on the test data\n",
    "# y_pred is \"y_hat\" (predicted y). It comes from calling model.predict(...) on X_test.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 3) Compute metrics comparing y_test (true) vs y_pred (predicted)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE  = {mse:.6f}\")\n",
    "print(f\"RMSE = {rmse:.6f}\")\n",
    "print(f\"MAE  = {mae:.6f}\")\n",
    "print(f\"R^2  = {r2:.6f}\")\n",
    "```\n",
    "\n",
    "Summary of which variables are used\n",
    "* **y_test**: the true target values for the test set (ground truth).\n",
    "* **y_pred**: the predicted target values for the test set, computed as:\n",
    "* **y_pred** = model.predict(X_test)\n",
    "* All four metrics above should typically be reported on the test set:\n",
    "    * Compare y_test vs y_pred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e60e4",
   "metadata": {},
   "source": [
    "## Interpreting regression metric values and how to compute them in practice\n",
    "\n",
    "### What does “closer to the true value” mean?\n",
    "In regression, each data point has:\n",
    "- a **true value** $y_i$ (the actual observed target from the dataset), and\n",
    "- a **predicted value** $\\hat{y}_i$ (the value predicted by the model).\n",
    "\n",
    "The difference between them is called the **residual**:\n",
    "$ \\text{residual}_i = y_i - \\hat{y}_i $\n",
    "\n",
    "A model is considered better when these residuals are small, meaning the predictions are numerically close to the true values.\n",
    "\n",
    "---\n",
    "\n",
    "## How do you get the actual (true) values?\n",
    "The **true values** come directly from your dataset.\n",
    "\n",
    "After splitting the data:\n",
    "- `y_train`: true target values used to train the model\n",
    "- `y_test`: true target values used to evaluate the model\n",
    "\n",
    "All regression metrics should be computed using **`y_test`**, because these values were not seen during training.\n",
    "\n",
    "---\n",
    "\n",
    "## How do you get the predicted values?\n",
    "Predicted values are produced by the trained model:\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "- `X_test`: input features for the test set\n",
    "- `y_pred`: predicted target values ($\\hat{y}$)\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Squared Error (MSE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Average squared difference between true and predicted values\n",
    "- **Interpretation:** Strongly penalizes large errors\n",
    "\n",
    "$ \\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` (true values) vs `y_pred` (predicted values)\n",
    "\n",
    "---\n",
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Square root of MSE, in the same unit as the target\n",
    "- **Interpretation:** Typical size of prediction error\n",
    "\n",
    "$ \\mathrm{RMSE} = \\sqrt{\\mathrm{MSE}} $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    import numpy as np\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "- **Goal:** Lower is better\n",
    "- **What it measures:** Average absolute difference between true and predicted values\n",
    "- **Interpretation:** Less sensitive to outliers than MSE/RMSE\n",
    "\n",
    "$ \\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`\n",
    "\n",
    "---\n",
    "\n",
    "## $R^2$ Score (Coefficient of Determination)\n",
    "- **Goal:** Higher is better\n",
    "- **What it measures:** Fraction of variance in the true values explained by the model\n",
    "- **Interpretation:**\n",
    "  - $R^2 = 1$: perfect predictions\n",
    "  - $R^2 = 0$: no better than predicting the mean\n",
    "  - $R^2 < 0$: worse than predicting the mean\n",
    "\n",
    "$ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} $\n",
    "\n",
    "Python code:\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "**Comparison used:**  \n",
    "`y_test` vs `y_pred`  \n",
    "$\\bar{y}$ is the mean of `y_test`\n",
    "\n",
    "---\n",
    "\n",
    "## Complete minimal example (context)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Train the model on true training values\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict target values for unseen data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **True (actual) values:** `y_test` (ground truth from the dataset)\n",
    "- **Predicted values:** `y_pred`, obtained using `model.predict(X_test)`\n",
    "- **Each metric compares:** `y_test` vs `y_pred`\n",
    "- **Lower is better:** MSE, RMSE, MAE\n",
    "- **Higher is better:** $R^2$\n",
    "- **Key idea:** Better models produce predictions $\\hat{y}$ that are numerically closer to the true values $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b50549",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b1493",
   "metadata": {},
   "source": [
    "-----\n",
    "## How to think when choosing a proposal distribution \\( g(x) \\) in rejection sampling\n",
    "\n",
    "When facing a difficult rejection sampling problem, do **not** start by guessing formulas. Instead, follow this reasoning process.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Locate where the probability mass is\n",
    "Ask: *Where does the distribution actually concentrate its mass?*\n",
    "\n",
    "Look for terms like:\n",
    "- $( e^{-1/x} )$, $( e^{x^2} )$, $( x^\\alpha )$\n",
    "- behavior near boundaries (0, infinity, endpoints)\n",
    "\n",
    "**Rule:**  \n",
    "Your proposal must put mass where the target puts mass.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Identify the dominant term\n",
    "Ignore constants and lower-order factors at first.\n",
    "\n",
    "Ask: *Which part of the density controls the shape?*\n",
    "\n",
    "Examples:\n",
    "- $( e^{-x} )$ → exponential\n",
    "- $( e^{-x^2} )$ → Gaussian-like\n",
    "- $( e^{-1/x} )$ → strong boundary concentration\n",
    "\n",
    "**Rule:**  \n",
    "Match the dominant term first; fix the rest using rejection.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Consider a change of variables\n",
    "If the density contains:\n",
    "- $( 1/x )$, $( \\log x )$, or sharp boundary behavior\n",
    "\n",
    "Ask: *Would this look simpler in another variable?*\n",
    "\n",
    "Common transformations:\n",
    "- $( Y = 1/X )$ for $( e^{-1/x} )$\n",
    "- $( Y = \\log X )$ for multiplicative scales\n",
    "\n",
    "**Rule:**  \n",
    "If the density is ugly in $( x )$, change coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Choose a proposal that is easy to sample from\n",
    "Good proposals:\n",
    "- Uniform (only if the target is fairly flat)\n",
    "- Exponential or shifted exponential\n",
    "- Gaussian\n",
    "\n",
    "Bad proposals:\n",
    "- hard-to-invert CDFs\n",
    "- complicated expressions\n",
    "\n",
    "**Rule:**  \n",
    "If sampling from $( g(x) )$ is hard, you chose the wrong proposal.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) Immediately check the ratio $( f(x)/g(x) )$\n",
    "Before coding, compute:\n",
    "$$\n",
    "\\frac{f(x)}{g(x)}\n",
    "$$\n",
    "\n",
    "Ask:\n",
    "- Is it bounded?\n",
    "- Does it simplify?\n",
    "- Do exponentials cancel?\n",
    "\n",
    "**Rule:**  \n",
    "If exponentials cancel and the ratio is simple, the proposal is good.\n",
    "\n",
    "---\n",
    "\n",
    "### 6) Estimate the rejection constant $( M )$\n",
    "Check where the maximum of $( f(x)/g(x) )$ occurs:\n",
    "- often at boundaries\n",
    "- sometimes at symmetry points\n",
    "\n",
    "Good signs:\n",
    "- $( M \\approx 1 )$: very efficient\n",
    "- $( M \\gg 10 )$: rethink your proposal\n",
    "\n",
    "---\n",
    "\n",
    "### 7) Key mindset\n",
    "Rejection sampling is not mechanical algebra — it is **distribution engineering**:\n",
    "- understand the shape\n",
    "- match it intelligently\n",
    "- use rejection only to correct small differences\n",
    "\n",
    "---\n",
    "\n",
    "### One-line checklist (exam-ready)\n",
    "1. Where is the mass?  \n",
    "2. What term dominates?  \n",
    "3. Should I change variables?  \n",
    "4. Can I sample from $( g )$ easily?  \n",
    "5. Is $( f/g )$ bounded?  \n",
    "6. Is $( M )$ small?\n",
    "\n",
    "If all answers are yes, your choice of $( g(x) )$ is good.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a18dd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a636f",
   "metadata": {},
   "source": [
    "-----\n",
    "# Below is General functions that can be used in this course, with their description of what they actually do and when they are to be used:\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb36415",
   "metadata": {},
   "source": [
    "## This is the General Info from the 2025 January Exam: \n",
    "\n",
    "# SVD & Anomaly Detection — Exam-Oriented Summary (Part 1)\n",
    "\n",
    "This section summarizes the **key functions and concepts** used in Part 1 of the SVD exercise.  \n",
    "The goal is to understand **what each concept does, when it is used, and why it is used**, without focusing on mathematical derivations.\n",
    "\n",
    "---\n",
    "\n",
    "## Loading the data\n",
    "\n",
    "### `pd.read_csv(\"data/SVD.csv\", header=None)`\n",
    "\n",
    "**What it does**  \n",
    "Loads the CSV file and treats **every row as data**, not as column names.\n",
    "\n",
    "**When it is used**  \n",
    "Used when the dataset contains **only numerical values**, which is common in linear algebra and machine learning problems.\n",
    "\n",
    "**Why it is used**  \n",
    "SVD requires a **pure numeric matrix**. If a header were assumed, the first data row could be dropped or interpreted incorrectly.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`header=None` means *“this CSV has no column names; everything is data.”*\n",
    "\n",
    "---\n",
    "\n",
    "## Singular Value Decomposition (SVD)\n",
    "\n",
    "### `np.linalg.svd(X, full_matrices=False)`\n",
    "\n",
    "**What it does**  \n",
    "Decomposes the data matrix $X$ into three components:\n",
    "\\[\n",
    "X \\approx U D V^T\n",
    "\\]\n",
    "\n",
    "**When it is used**  \n",
    "- Dimensionality reduction  \n",
    "- Anomaly detection  \n",
    "- Noise reduction  \n",
    "- Data compression  \n",
    "- Feature extraction  \n",
    "\n",
    "**Why it is used**  \n",
    "It separates the most important patterns in the data from less important ones, making it easier to approximate and analyze the dataset.\n",
    "\n",
    "**Exam takeaway**  \n",
    "SVD breaks a matrix into **patterns** and **how important those patterns are**.\n",
    "\n",
    "---\n",
    "\n",
    "## Meaning of $U$, $D$, and $V$\n",
    "\n",
    "### $U$ — Left singular vectors  \n",
    "- A matrix whose **columns** describe patterns across **samples (rows)**.  \n",
    "- Shape (with `full_matrices=False`):\n",
    "\\[\n",
    "(n\\_samples, r)\n",
    "\\]\n",
    "- Each column shows how strongly each sample is associated with a component.\n",
    "\n",
    "**Think of $U$ as:**  \n",
    "“How each data point uses the components.”\n",
    "\n",
    "---\n",
    "\n",
    "### $D$ — Singular values (diagonal matrix)  \n",
    "- A **diagonal matrix** containing the singular values.  \n",
    "- Shape:\n",
    "\\[\n",
    "(r, r)\n",
    "\\]\n",
    "- Larger values indicate more important components.\n",
    "\n",
    "**Why diagonal**  \n",
    "Each component scales independently, without mixing with others.\n",
    "\n",
    "---\n",
    "\n",
    "### $V$ — Right singular vectors  \n",
    "- A matrix whose **columns** describe patterns across **features (dimensions)**.  \n",
    "- Shape:\n",
    "\\[\n",
    "(n\\_dimensions, r)\n",
    "\\]\n",
    "\n",
    "**Think of $V$ as:**  \n",
    "“What the components look like in feature space.”\n",
    "\n",
    "---\n",
    "\n",
    "## Why `np.diag(s)` is used\n",
    "\n",
    "### `np.diag(s)`\n",
    "\n",
    "**What it does**  \n",
    "Converts the 1D vector of singular values `s` into a **diagonal matrix** $D$.\n",
    "\n",
    "**When it is used**  \n",
    "When explicitly reconstructing the matrix using:\n",
    "\\[\n",
    "X \\approx U D V^T\n",
    "\\]\n",
    "\n",
    "**Why it is used**  \n",
    "NumPy returns singular values as a vector for efficiency, but the SVD definition requires $D$ to be a matrix.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`s` contains the values, `np.diag(s)` builds the proper $D$ matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## `full_matrices=False` (important)\n",
    "\n",
    "**What it controls**  \n",
    "The **size** of the matrices returned by SVD.\n",
    "\n",
    "**With `full_matrices=True`**  \n",
    "- Produces large square matrices.\n",
    "- Often unnecessary and inefficient.\n",
    "\n",
    "**With `full_matrices=False`**  \n",
    "- Produces the **compact (economy) SVD**.\n",
    "- Keeps only the components needed to reconstruct and approximate the data.\n",
    "- More memory-efficient and faster.\n",
    "\n",
    "**Why it is used here**  \n",
    "This is the standard choice in data science and machine learning because it keeps only the meaningful information.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`full_matrices=False` means *“give me the smallest useful SVD.”*\n",
    "\n",
    "---\n",
    "\n",
    "## NumPy slicing for singular vectors\n",
    "\n",
    "### `problem1_first_right_singular_vector = problem1_V[:, 0]`\n",
    "\n",
    "**What it does**  \n",
    "Extracts the **first column** of the matrix $V$ and returns it as a 1D array.\n",
    "\n",
    "**How slicing works**  \n",
    "In NumPy:\n",
    "* array[rows, columns]\n",
    "\n",
    "- `:` means all rows  \n",
    "- `0` means column index 0  \n",
    "\n",
    "So `[:, 0]` means *all rows from column 0*.\n",
    "\n",
    "**Why it is used**  \n",
    "- Columns of $V$ are **right singular vectors**.\n",
    "- Column 0 corresponds to the **most important component**.\n",
    "\n",
    "**Common mistake**  \n",
    "This does **not** select a row — it selects a column.\n",
    "\n",
    "**Exam takeaway**  \n",
    "`[:, 0]` extracts the **first (most important) singular vector**.\n",
    "\n",
    "---\n",
    "\n",
    "## One-line exam summary\n",
    "\n",
    "- `header=None` → CSV has no column names  \n",
    "- `svd(X)` → decomposes data into patterns and their importance  \n",
    "- $U$ → how samples relate to components  \n",
    "- $D$ → importance of each component (diagonal matrix)  \n",
    "- $V$ → what components look like in feature space  \n",
    "- `full_matrices=False` → compact, efficient SVD  \n",
    "- `np.diag(s)` → build the diagonal $D$ matrix  \n",
    "- `[:, 0]` → extract the first singular vector  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e980d48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11014eb",
   "metadata": {},
   "source": [
    "# SVD — Explained Variance (Part 2) Exam-Oriented Summary\n",
    "\n",
    "This section explains **explained variance**, how it is calculated in the context of SVD, and the roles of `cumsum()` and `argmax()`. The focus is on **what these concepts mean and why they are used**, not on mathematical derivations.\n",
    "\n",
    "---\n",
    "\n",
    "## What is explained variance?\n",
    "\n",
    "**Explained variance** measures **how much of the total information (variation)** in the original dataset is captured when keeping only the first $k$ singular components from the SVD.\n",
    "\n",
    "In plain terms:\n",
    "- It tells you **how well a reduced version of the data represents the original data**\n",
    "- Higher explained variance means **less information loss**\n",
    "- It helps decide **how many components are enough**\n",
    "\n",
    "For example:\n",
    "- 60% explained variance → much structure is lost  \n",
    "- 95% explained variance → almost all important structure is retained  \n",
    "\n",
    "**Exam intuition**  \n",
    "Explained variance answers the question:  \n",
    "> “If I keep only the first $k$ components, how much of the original data do I still explain?”\n",
    "\n",
    "---\n",
    "\n",
    "## How is explained variance calculated?\n",
    "\n",
    "From the SVD, each singular value $\\sigma_j$ represents how important a component is.\n",
    "\n",
    "The calculation follows this logic:\n",
    "\n",
    "1. **Square each singular value**  \n",
    "   - $\\sigma_j^2$ represents the variance contribution of component $j$\n",
    "\n",
    "2. **Sum all squared singular values**  \n",
    "   - This gives the **total variance** in the data\n",
    "\n",
    "3. **Compute the cumulative fraction**\n",
    "$$\n",
    "\\text{ExplainedVariance}(k) =\n",
    "\\frac{\\sum_{j=1}^{k} \\sigma_j^2}{\\sum_{j=1}^{r} \\sigma_j^2}\n",
    "$$\n",
    "\n",
    "This produces values between $0$ and $1$ (or $0\\%$ to $100\\%$).\n",
    "\n",
    "**Why this works conceptually**  \n",
    "- Larger singular values correspond to more important directions\n",
    "- By summing them in order, we see how information accumulates as we keep more components\n",
    "\n",
    "---\n",
    "\n",
    "## What does `np.cumsum()` do?\n",
    "\n",
    "### `np.cumsum(array)`\n",
    "\n",
    "**What it does**  \n",
    "Computes the **cumulative sum** of an array.\n",
    "\n",
    "Example:\n",
    "Input: [a, b, c, d]\n",
    "Output: [a, a+b, a+b+c, a+b+c+d]\n",
    "\n",
    "\n",
    "**Why it is used here**\n",
    "- Singular values are ordered from **most important to least important**\n",
    "- `cumsum()` allows us to track:\n",
    "  - variance explained by 1 component\n",
    "  - variance explained by 2 components\n",
    "  - variance explained by 3 components\n",
    "  - and so on\n",
    "\n",
    "**In this exercise**\n",
    "np.cumsum(singular_values_squared) / total_variance\n",
    "\n",
    "produces a vector where each entry tells:\n",
    "“How much of the total variance is explained if we keep components up to this point”\n",
    "\n",
    "**Exam takeaway**  \n",
    "`cumsum()` builds the running total so we can see how variance accumulates as components are added.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `argmax()` and what does it do?\n",
    "\n",
    "### `np.argmax(array)`\n",
    "\n",
    "**What it does**\n",
    "- Returns the **index of the first maximum value** in an array.\n",
    "\n",
    "When used on a boolean array:\n",
    "- `False` is treated as 0  \n",
    "- `True` is treated as 1  \n",
    "- The first `True` is the maximum\n",
    "\n",
    "---\n",
    "\n",
    "## Why `argmax()` is used here\n",
    "\n",
    "This line:\n",
    "```python\n",
    "np.argmax(problem1_explained_variance >= 0.95)\n",
    "```\n",
    "\n",
    "Works conceptually as follows:\n",
    "problem1_explained_variance >= 0.95 → creates a boolean array like:\n",
    "[False, False, False, True, True, ...]\n",
    "\n",
    "argmax() → returns the index of the first True value\n",
    "\n",
    "+ 1→ converts from zero-based indexing to a component count starting at 1\n",
    "\n",
    "What this gives\n",
    "* The smallest number of components needed to explain at least 95% of the variance\n",
    "\n",
    "Exam intuition\n",
    "argmax() is used to answer:\n",
    "* “When does this condition become true for the first time?”\n",
    "\n",
    "One-line exam summary\n",
    "* Explained variance → how much information is retained using $k$ components\n",
    "* Squared singular values → variance contributions\n",
    "* Total variance → sum of all squared singular values\n",
    "* cumsum() → builds cumulative explained variance\n",
    "* argmax() → finds the first index where a condition is satisfied\n",
    "* Combined → select the smallest number of components that reaches a target variance level (e.g. 95%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4242909",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "343c2ff2",
   "metadata": {},
   "source": [
    "# This is an example from the Exam 230815 of how the plotting of the empirical distribution function of the residual with confidence bands (i.e. using the DKW inequality and 95% confidence) works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dea0e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. [3p] Empirical CDF of residuals with DKW 95% confidence band\n",
    "\n",
    "We want to study the **distribution of the residuals on the test set** and add a **uniform confidence band** using the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Define residuals on the test set\n",
    "\n",
    "For each test point $(i = 1,\\dots,n)$, we have:\n",
    "\n",
    "- True salary $(y_i)$ (from $(\\text{problem2\\_y\\_test})$)\n",
    "- Predicted salary $(\\hat{y}_i)$ (from $(\\text{problem2\\_model.predict})$)\n",
    "\n",
    "The **residual** is\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i.\n",
    "$$\n",
    "\n",
    "Collect all test residuals in a vector\n",
    "\n",
    "$$\n",
    "e_1, e_2, \\dots, e_n.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Empirical distribution function (EDF) of the residuals\n",
    "\n",
    "The **empirical distribution function** (EDF) of the residuals is defined as\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(t)\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\{ e_i \\le t \\},\n",
    "$$\n",
    "\n",
    "where $(\\mathbf{1}\\{\\cdot\\})$ is the indicator function.\n",
    "\n",
    "In practice, we:\n",
    "\n",
    "1. Sort the residuals:\n",
    "\n",
    "   $$\n",
    "   e_{(1)} \\le e_{(2)} \\le \\dots \\le e_{(n)},\n",
    "   $$\n",
    "\n",
    "2. At each sorted residual $(e_{(k)})$, the EDF jumps to\n",
    "\n",
    "   $$ \n",
    "   \\hat{F}_n\\bigl(e_{(k)}\\bigr) = \\frac{k}{n}.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: DKW inequality and 95% confidence band\n",
    "\n",
    "Let $(F(t))$ be the **true CDF** of the residuals (unknown).  \n",
    "The DKW inequality states that for any $(\\varepsilon > 0)$,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\!\\left(\n",
    "\\sup_{t} \\left| \\hat{F}_n(t) - F(t) \\right| > \\varepsilon\n",
    "\\right)\n",
    "\\le\n",
    "2 e^{-2 n \\varepsilon^2}.\n",
    "$$\n",
    "\n",
    "To get a **\\(95\\%\\)** (i.e. $(1-\\alpha = 0.95)$) **uniform confidence band**, we set $(\\alpha = 0.05)$ and solve\n",
    "\n",
    "$$\n",
    "2 e^{- 2 n \\varepsilon^2 } = \\alpha.\n",
    "$$\n",
    "\n",
    "Taking logarithms:\n",
    "\n",
    "$$\n",
    "e^{- 2 n \\varepsilon^2 } = \\frac{\\alpha}{2}\n",
    "\\quad\\Rightarrow\\quad\n",
    "-2 n \\varepsilon^2 = \\log \\frac{\\alpha}{2}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\varepsilon^2 = -\\frac{1}{2n} \\log \\frac{\\alpha}{2}.\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\varepsilon_n\n",
    "=\n",
    "\\sqrt{\n",
    "-\\frac{1}{2n} \\log \\frac{\\alpha}{2}\n",
    "}\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log\\!\\left( \\frac{2}{\\alpha} \\right)\n",
    "}.\n",
    "$$\n",
    "\n",
    "For $(\\alpha = 0.05)$,\n",
    "\n",
    "$$\n",
    "\\varepsilon_n\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log\\!\\left( \\frac{2}{0.05} \\right)\n",
    "}\n",
    "=\n",
    "\\sqrt{\n",
    "\\frac{1}{2n} \\log(40)\n",
    "}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Constructing the confidence band\n",
    "\n",
    "For each $(t)$, the **95% confidence band** is\n",
    "\n",
    "$$\n",
    "\\hat{F}_n(t) - \\varepsilon_n\n",
    "\\le\n",
    "F(t)\n",
    "\\le\n",
    "\\hat{F}_n(t) + \\varepsilon_n,\n",
    "$$\n",
    "\n",
    "or in terms of lower and upper band functions:\n",
    "\n",
    "$$\n",
    "F^{-}(t) = \\max\\bigl( \\hat{F}_n(t) - \\varepsilon_n,\\ 0 \\bigr),\n",
    "\\qquad\n",
    "F^{+}(t) = \\min\\bigl( \\hat{F}_n(t) + \\varepsilon_n,\\ 1 \\bigr).\n",
    "$$\n",
    "\n",
    "In the plot, we:\n",
    "\n",
    "- Plot the EDF $(\\hat{F}_n(t))$ as a step curve,\n",
    "- Plot the lower band $(F^{-}(t))$,\n",
    "- Plot the upper band $(F^{+}(t))$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Interpretation — what does the band tell us?\n",
    "\n",
    "The DKW band has the property that, with probability at least $(95\\%)$,\n",
    "\n",
    "$$\n",
    "F(t) \\in [F^{-}(t), F^{+}(t)] \\quad \\text{for all } t.\n",
    "$$\n",
    "\n",
    "In words:\n",
    "\n",
    "- The band tells us how much **uncertainty** there is in the empirical CDF as an estimate of the true residual distribution.\n",
    "- It is **uniform in \\(t\\)**: the guarantee holds simultaneously for all thresholds $(t)$.\n",
    "\n",
    "**What can we use it for?**\n",
    "\n",
    "- To assess how precisely we have estimated the distribution of residuals from a finite test sample.\n",
    "- To check whether a **candidate theoretical distribution** for residuals (e.g. normal distribution) lies mostly within this band; if the theoretical CDF goes outside the band, this suggests a poor fit.\n",
    "- More generally, to quantify uncertainty in distributional features of the residuals (e.g. quantiles, tail behavior) in a nonparametric way.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba1d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code from the above:\n",
    "\n",
    "# Part 6\n",
    "# Put the code for part 6 below this line\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Recompute predictions and residuals on the test set (for clarity)\n",
    "y_test_true = problem2_y_test\n",
    "y_test_pred = problem2_model.predict(problem2_X_test)\n",
    "residuals_test = y_test_true - y_test_pred\n",
    "\n",
    "# Number of test samples\n",
    "n = len(residuals_test)\n",
    "\n",
    "# 2. Sort residuals and build empirical CDF values\n",
    "residuals_sorted = np.sort(residuals_test)\n",
    "ecdf_values = np.arange(1, n + 1) / n  # k/n for k = 1,...,n\n",
    "\n",
    "# 3. Compute epsilon using the DKW inequality for 95% confidence\n",
    "alpha = 0.05\n",
    "epsilon = np.sqrt((1.0 / (2.0 * n)) * np.log(2.0 / alpha))\n",
    "\n",
    "# 4. Compute lower and upper confidence bands (clipped to [0, 1])\n",
    "lower_band = np.clip(ecdf_values - epsilon, 0.0, 1.0)\n",
    "upper_band = np.clip(ecdf_values + epsilon, 0.0, 1.0)\n",
    "\n",
    "print(f\"Number of test samples n = {n}\")\n",
    "print(f\"DKW epsilon (95% band)  = {epsilon:.4f}\")\n",
    "\n",
    "# 5. Plot the empirical CDF and the confidence band\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Empirical CDF as a step function\n",
    "plt.step(residuals_sorted, ecdf_values, where=\"post\", label=\"Empirical CDF of residuals\")\n",
    "\n",
    "# Confidence band as two lines\n",
    "plt.step(residuals_sorted, lower_band, where=\"post\", linestyle=\"--\", label=\"Lower 95% band\")\n",
    "plt.step(residuals_sorted, upper_band, where=\"post\", linestyle=\"--\", label=\"Upper 95% band\")\n",
    "\n",
    "plt.xlabel(\"Residual (true - predicted salary)\")\n",
    "plt.ylabel(\"Empirical CDF\")\n",
    "plt.title(\"Empirical CDF of Test Residuals with DKW 95% Confidence Band\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\":\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489df672",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4678c3c8",
   "metadata": {},
   "source": [
    "# This Section describes the difference between Inversion sampling and using simple Reject Accept:\n",
    "\n",
    "\n",
    "## PROBLEM 3 — Random variable generation from a given CDF\n",
    "\n",
    "We are given the CDF\n",
    "$$\n",
    "F(x)=\n",
    "\\begin{cases}\n",
    "0, & x\\le 0\\\\\n",
    "e^x-1, & 0<x<\\ln(2)\\\\\n",
    "1, & x\\ge \\ln(2)\n",
    "\\end{cases}\n",
    "$$\n",
    "The support is the interval $(x\\in(0,\\ln 2))$.\n",
    "\n",
    "---\n",
    "\n",
    "# A) Step-by-step: Inversion sampling (construct 1000 samples)\n",
    "\n",
    "### Step 1: Identify the part of the CDF we can invert\n",
    "For $(0<x<\\ln 2)$,\n",
    "$$\n",
    "F(x)=e^x-1.\n",
    "$$\n",
    "Also note $(F(0)=0)$ and $(F(\\ln 2)=e^{\\ln 2}-1=2-1=1)$, so this maps exactly to $((0,1))$.\n",
    "\n",
    "### Step 2: Set $(U \\sim \\text{Unif}(0,1))$ and solve $(U = F(X))$\n",
    "Let $(U\\in(0,1))$. Set:\n",
    "$$\n",
    "U = e^X - 1.\n",
    "$$\n",
    "Solve for (X)$:\n",
    "$$\n",
    "U+1 = e^X \\quad\\Rightarrow\\quad X=\\ln(1+U).\n",
    "$$\n",
    "\n",
    "### Step 3: Sampling rule (the inverse CDF)\n",
    "$$\n",
    "X = F^{-1}(U) = \\ln(1+U), \\quad U\\sim \\text{Unif}(0,1).\n",
    "$$\n",
    "\n",
    "### Step 4: Generate 1000 samples\n",
    "1. Draw $(U_1,\\dots,U_{1000}\\stackrel{iid}{\\sim}\\text{Unif}(0,1))$.\n",
    "2. Compute $(X_i=\\ln(1+U_i))$.\n",
    "\n",
    "**Python (minimal):**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "U = np.random.rand(n)         # U_i ~ Uniform(0,1)\n",
    "X_inv = np.log(1 + U)         # X_i = ln(1+U_i)\n",
    "```\n",
    "\n",
    "## Step 5: Estimate mean and variance from the 1000 samples\n",
    "\n",
    "The sample mean is estimated as\n",
    "\n",
    "$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} X_i,$\n",
    "\n",
    "and the sample variance is estimated as\n",
    "\n",
    "$\\hat{\\sigma}^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\hat{\\mu})^2.$\n",
    "\n",
    "```python\n",
    "    mean_hat = X_inv.mean()\n",
    "    var_hat  = X_inv.var(ddof=1)  # sample variance (divide by n-1)\n",
    "```\n",
    "\n",
    "## B) Step-by-step: Accept–Reject sampling (construct 1000 samples)\n",
    "\n",
    "### Step 1: Compute the target density $f(x)$\n",
    "\n",
    "Differentiate the CDF on the continuous part:\n",
    "\n",
    "$$\n",
    "f(x) = F'(x) = e^x, \\qquad 0 < x < \\ln 2,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "f(x) = 0 \\quad \\text{otherwise}.\n",
    "$$\n",
    "\n",
    "Check normalization:\n",
    "\n",
    "$$\n",
    "\\int_{0}^{\\ln 2} e^x \\, dx\n",
    "= e^{\\ln 2} - 1\n",
    "= 2 - 1\n",
    "= 1,\n",
    "$$\n",
    "\n",
    "so $f$ is already a valid PDF.\n",
    "\n",
    "### Step 2: Choose an easy proposal density $g(x)$\n",
    "\n",
    "A simple choice is uniform on the same support:\n",
    "\n",
    "$$\n",
    "g(x) = \\text{Unif}(0, \\ln 2)\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "g(x) = \\frac{1}{\\ln 2}, \\quad 0 < x < \\ln 2.\n",
    "$$\n",
    "\n",
    "**Why this proposal?**\n",
    "\n",
    "- Very easy to sample from.\n",
    "- Same support as $f$.\n",
    "- Makes $f(x) / g(x)$ simple, giving a good acceptance rate.\n",
    "\n",
    "### Step 3: Find a constant $M$ so that $f(x) \\le M g(x)$\n",
    "\n",
    "Compute the ratio on $[0, \\ln 2]$:\n",
    "\n",
    "$$\n",
    "\\frac{f(x)}{g(x)} = \\frac{e^x}{1/\\ln 2} = e^x \\ln 2.\n",
    "$$\n",
    "\n",
    "This is maximized at $x = \\ln 2$, where $e^{\\ln 2} = 2$. Hence:\n",
    "\n",
    "$$\n",
    "\\max_{x \\in [0, \\ln 2]} \\frac{f(x)}{g(x)} = 2 \\ln 2.\n",
    "$$\n",
    "\n",
    "So we can choose\n",
    "\n",
    "$$\n",
    "M = 2 \\ln 2.\n",
    "$$\n",
    "\n",
    "### Step 4: Write the acceptance probability\n",
    "\n",
    "Accept–Reject accepts a proposal $Y \\sim g$ with probability\n",
    "\n",
    "$$\n",
    "\\frac{f(Y)}{M g(Y)}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\n",
    "\\frac{f(y)}{M g(y)}\n",
    "= \\frac{e^y}{(2 \\ln 2)\\cdot (1/\\ln 2)}\n",
    "= \\frac{e^y}{2}.\n",
    "$$\n",
    "\n",
    "So the acceptance test is:\n",
    "\n",
    "* Draw $U \\sim \\text{Unif}(0,1)$. Accept $Y$ if $U \\le e^Y/2$.\n",
    "\n",
    "### Step 5: Generate 1000 accepted samples and compute acceptance proportion\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Propose $Y \\sim \\text{Unif}(0, \\ln 2)$.\n",
    "2. Draw $U \\sim \\text{Unif}(0, 1)$.\n",
    "3. If $U \\le e^Y / 2$, accept $Y$ as a sample; otherwise reject and repeat.\n",
    "4. Keep going until you have 1000 accepted samples.\n",
    "5. Acceptance proportion = (number accepted) / (number proposed).\n",
    "\n",
    "**Expected acceptance proportion:**\n",
    "\n",
    "$$\n",
    "\\text{AccRate} = \\frac{1}{M} = \\frac{1}{2 \\ln 2} \\approx 0.721.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## This is the final and complete code for both of the different approaches:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# PROBLEM 3 — Random variable generation from a given CDF\n",
    "# Target CDF:\n",
    "#   F(x)=0                  for x<=0\n",
    "#   F(x)=e^x - 1            for 0<x<ln(2)\n",
    "#   F(x)=1                  for x>=ln(2)\n",
    "#\n",
    "# Therefore the PDF on (0, ln(2)) is:\n",
    "#   f(x) = d/dx (e^x - 1) = e^x,   for 0<x<ln(2)\n",
    "# ============================================================\n",
    "\n",
    "# -----------------------------\n",
    "# Settings\n",
    "# -----------------------------\n",
    "n = 1000\n",
    "# NOTE: Using np.random.* directly (no rng / default_rng).\n",
    "# If you want reproducibility, uncomment the next line:\n",
    "# np.random.seed(0)\n",
    "\n",
    "ln2 = np.log(2.0)\n",
    "\n",
    "# ============================================================\n",
    "# A) Inversion sampling\n",
    "#   U ~ Uniform(0,1)\n",
    "#   X = F^{-1}(U) = ln(1+U)\n",
    "#\n",
    "# Implemented as a loop (as in your example).\n",
    "# ============================================================\n",
    "\n",
    "samples_inv = []\n",
    "\n",
    "while len(samples_inv) < n:\n",
    "    U = np.random.uniform(0.0, 1.0)  # U ~ Uniform(0,1)\n",
    "    x = np.log(U + 1.0)              # X = ln(1+U)\n",
    "    samples_inv.append(x)\n",
    "\n",
    "# Convert to NumPy array for easy stats\n",
    "X_inv = np.array(samples_inv, dtype=float)\n",
    "\n",
    "# ---- Estimate mean and variance (sample variance uses ddof=1) ----\n",
    "mean_inv = X_inv.mean()\n",
    "var_inv  = X_inv.var(ddof=1)\n",
    "\n",
    "print(\"=== Inversion sampling ===\")\n",
    "print(\"n =\", n)\n",
    "print(\"Length of samples:\", len(samples_inv))\n",
    "print(\"sample mean =\", mean_inv)\n",
    "print(\"sample var  =\", var_inv)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# B) Accept–Reject sampling (written in the same style as your example)\n",
    "#\n",
    "# Target: f(x) = e^x on (0, ln2)\n",
    "#\n",
    "# Proposal: g(x) = Uniform(0, ln2)\n",
    "#   g(x) = 1/ln2 on (0, ln2)\n",
    "#\n",
    "# Bound:\n",
    "#   f(x)/g(x) = e^x ln2, maximized at x=ln2 -> 2 ln2\n",
    "#   so we can choose M = 2 ln2\n",
    "#\n",
    "# Acceptance probability:\n",
    "#   f(y)/(M g(y)) = e^y / 2\n",
    "# ============================================================\n",
    "\n",
    "def problem3_rejection(n_samples=1):\n",
    "    \"\"\"\n",
    "    Return a numpy array of length n_samples with samples from\n",
    "    f(x)=e^x on (0, ln(2)) using accept-reject with proposal Unif(0, ln(2)).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # 1) Sample from the proposal: Uniform(0, ln2)\n",
    "        y = np.random.uniform(0.0, ln2)\n",
    "\n",
    "        # 2) Compute acceptance probability w = f(y)/(M g(y)) = e^y / 2\n",
    "        w = np.exp(y) / 2.0\n",
    "\n",
    "        # 3) Sample U ~ Uniform(0,1) for acceptance\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "\n",
    "        # 4) Accept if u <= w\n",
    "        if u <= w:\n",
    "            samples.append(y)\n",
    "\n",
    "    return np.array(samples, dtype=float)\n",
    "\n",
    "# Generate n samples with accept-reject\n",
    "X_ar = problem3_rejection(n)\n",
    "\n",
    "# ---- Estimate mean and variance (sample variance uses ddof=1) ----\n",
    "mean_ar = X_ar.mean()\n",
    "var_ar  = X_ar.var(ddof=1)\n",
    "\n",
    "print(\"=== Accept–Reject sampling ===\")\n",
    "print(\"n =\", n)\n",
    "print(\"Length of samples:\", len(X_ar))\n",
    "print(\"sample mean =\", mean_ar)\n",
    "print(\"sample var  =\", var_ar)\n",
    "print()\n",
    "\n",
    "# ============================================================\n",
    "# (Optional) Quick sanity checks\n",
    "# ============================================================\n",
    "\n",
    "print(\"Sanity checks:\")\n",
    "print(\"Inversion: min/max =\", X_inv.min(), X_inv.max(), \" (should be within (0, ln2) )\")\n",
    "print(\"A-R      : min/max =\", X_ar.min(),  X_ar.max(),  \" (should be within (0, ln2) )\")\n",
    "\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d18f3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c1e304",
   "metadata": {},
   "source": [
    "# This is information about the Accept Reject problems:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b329221",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2163b51",
   "metadata": {},
   "source": [
    "# This explains how Recall and Precision works for both class 0 and 1:\n",
    "\n",
    "\n",
    "### Precision and Recall (per class)\n",
    "\n",
    "Let the confusion matrix be:\n",
    "\n",
    "- **tn**: true class 0 predicted as 0  \n",
    "- **fp**: true class 0 predicted as 1  \n",
    "- **fn**: true class 1 predicted as 0  \n",
    "- **tp**: true class 1 predicted as 1  \n",
    "\n",
    "---\n",
    "\n",
    "### Class 1 (treat class `1` as the positive class)\n",
    "\n",
    "**Precision (class 1)**  \n",
    "Fraction of samples predicted as class 1 that are truly class 1:\n",
    "$$\n",
    "\\text{Precision}_1 = \\frac{tp}{tp + fp}\n",
    "$$\n",
    "\n",
    "**Recall (class 1)**  \n",
    "Fraction of true class 1 samples that are correctly predicted:\n",
    "$$\n",
    "\\text{Recall}_1 = \\frac{tp}{tp + fn}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Class 0 (treat class `0` as the positive class, one-vs-rest)\n",
    "\n",
    "When evaluating class 0, we treat class `0` as the “positive” class.\n",
    "\n",
    "- True positives for class 0: **tn**\n",
    "- Predicted positives for class 0: **tn + fn**\n",
    "- Actual positives for class 0: **tn + fp**\n",
    "\n",
    "**Precision (class 0)**  \n",
    "Fraction of samples predicted as class 0 that are truly class 0:\n",
    "$$\n",
    "\\text{Precision}_0 = \\frac{tn}{tn + fn}\n",
    "$$\n",
    "\n",
    "**Recall (class 0)**  \n",
    "Fraction of true class 0 samples that are correctly predicted:\n",
    "$$\n",
    "\\text{Recall}_0 = \\frac{tn}{tn + fp}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Class | Precision | Recall |\n",
    "|------|-----------|--------|\n",
    "| 0 | $( \\frac{tn}{tn + fn} $) | $( \\frac{tn}{tn + fp} $) |\n",
    "| 1 | $( \\frac{tp}{tp + fp} $) | $( \\frac{tp}{tp + fn} $) |\n",
    "\n",
    "---\n",
    "\n",
    "**Key intuition:**  \n",
    "- **Precision** conditions on what the model *predicted*  \n",
    "- **Recall** conditions on what the class *actually was*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb525ad6",
   "metadata": {},
   "source": [
    "### Choice of $( n )$ when using Hoeffding’s inequality\n",
    "\n",
    "Hoeffding’s inequality bounds the deviation of an empirical mean from its true\n",
    "expectation. Therefore, $( n )$ must be the **number of independent Bernoulli trials**\n",
    "used to compute that empirical mean.\n",
    "\n",
    "For precision and recall, each metric is an average of indicator variables\n",
    "(correct or incorrect classification) over a specific subset of samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision\n",
    "\n",
    "Precision is defined as:\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{number of correct positive predictions}}{\\text{number of predicted positives}}\n",
    "$$\n",
    "\n",
    "Each predicted positive contributes one Bernoulli trial (correct or not). Therefore:\n",
    "$$\n",
    "n_{\\text{precision}} = \\text{number of predicted positives}\n",
    "$$\n",
    "\n",
    "- Class 1: $( n = tp + fp )$  \n",
    "- Class 0: $( n = tn + fn )$\n",
    "\n",
    "---\n",
    "\n",
    "### Recall\n",
    "\n",
    "Recall is defined as:\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{number of correctly classified positives}}{\\text{number of actual positives}}\n",
    "$$\n",
    "\n",
    "Each actual positive contributes one Bernoulli trial. Therefore:\n",
    "$$\n",
    "n_{\\text{recall}} = \\text{number of actual positives}\n",
    "$$\n",
    "\n",
    "- Class 1: $( n = tp + fn )$  \n",
    "- Class 0: $( n = tn + fp )$\n",
    "\n",
    "---\n",
    "\n",
    "### Key idea\n",
    "\n",
    "The denominator of precision or recall is exactly the number of samples over which\n",
    "the empirical average is computed. This is why it is the correct choice of $( n )$\n",
    "in Hoeffding’s inequality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecbc8c",
   "metadata": {},
   "source": [
    "### This is how you get the precision and recall using the confusion matrix:\n",
    "\n",
    "```python\n",
    "# === 2. Build the confusion matrix ===\n",
    "# We specify labels=[0, 1] to ensure the order is TN, FP, FN, TP when we ravel()\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n",
    "print(\"tn:\", tn)\n",
    "print(\"fp:\", fp)\n",
    "print(\"fn:\", fn)\n",
    "print(\"tp:\", tp)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
